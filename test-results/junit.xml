<?xml version="1.0" encoding="utf-8"?><testsuites name="pytest tests"><testsuite name="pytest" errors="0" failures="17" skipped="50" tests="235" time="21.266" timestamp="2025-12-03T15:53:37.654180-06:00" hostname="Rajs-MacBook-Pro.local"><testcase classname="tests.unit.test_renamed_stages.TestPhase1RenamedStages" name="test_stage_module_importable[scripts.03_glossary_load-03_glossary_load]" time="0.144" /><testcase classname="tests.unit.test_renamed_stages.TestPhase1RenamedStages" name="test_stage_module_importable[scripts.05_ner-05_ner]" time="0.001" /><testcase classname="tests.unit.test_renamed_stages.TestPhase1RenamedStages" name="test_stage_module_importable[scripts.06_lyrics_detection-06_lyrics_detection]" time="0.001" /><testcase classname="tests.unit.test_renamed_stages.TestPhase1RenamedStages" name="test_stage_module_importable[scripts.07_hallucination_removal-07_hallucination_removal]" time="0.001" /><testcase classname="tests.unit.test_renamed_stages.TestPhase1RenamedStages" name="test_stage_module_importable[scripts.09_subtitle_gen-09_subtitle_gen]" time="0.001" /><testcase classname="tests.unit.test_renamed_stages.TestPhase1RenamedStages" name="test_stage_has_run_stage_function[scripts.03_glossary_load-03_glossary_load]" time="0.000" /><testcase classname="tests.unit.test_renamed_stages.TestPhase1RenamedStages" name="test_stage_has_run_stage_function[scripts.05_ner-05_ner]" time="0.000" /><testcase classname="tests.unit.test_renamed_stages.TestPhase1RenamedStages" name="test_stage_has_run_stage_function[scripts.06_lyrics_detection-06_lyrics_detection]" time="0.000" /><testcase classname="tests.unit.test_renamed_stages.TestPhase1RenamedStages" name="test_stage_has_run_stage_function[scripts.07_hallucination_removal-07_hallucination_removal]" time="0.000" /><testcase classname="tests.unit.test_renamed_stages.TestPhase1RenamedStages" name="test_stage_has_run_stage_function[scripts.09_subtitle_gen-09_subtitle_gen]" time="0.000" /><testcase classname="tests.unit.test_renamed_stages.TestPhase1RenamedStages" name="test_run_stage_signature[scripts.03_glossary_load-03_glossary_load]" time="0.000" /><testcase classname="tests.unit.test_renamed_stages.TestPhase1RenamedStages" name="test_run_stage_signature[scripts.05_ner-05_ner]" time="0.000" /><testcase classname="tests.unit.test_renamed_stages.TestPhase1RenamedStages" name="test_run_stage_signature[scripts.06_lyrics_detection-06_lyrics_detection]" time="0.000" /><testcase classname="tests.unit.test_renamed_stages.TestPhase1RenamedStages" name="test_run_stage_signature[scripts.07_hallucination_removal-07_hallucination_removal]" time="0.000" /><testcase classname="tests.unit.test_renamed_stages.TestPhase1RenamedStages" name="test_run_stage_signature[scripts.09_subtitle_gen-09_subtitle_gen]" time="0.000" /><testcase classname="tests.unit.test_renamed_stages.TestGlossaryLearnerModule" name="test_glossary_learner_importable" time="0.001" /><testcase classname="tests.unit.test_renamed_stages.TestGlossaryLearnerModule" name="test_glossary_learner_has_class" time="0.000" /><testcase classname="tests.unit.test_renamed_stages.TestStageFileNaming" name="test_all_stage_files_in_scripts_dir" time="0.000" /><testcase classname="tests.unit.test_renamed_stages.TestStageFileNaming" name="test_no_stage_subdirectories_remain" time="0.000" /><testcase classname="tests.unit.test_renamed_stages.TestStageFileNaming" name="test_stage_files_follow_naming_pattern" time="0.001" /><testcase classname="tests.unit.test_renamed_stages.TestStageImportPatterns" name="test_run_pipeline_uses_importlib" time="0.001" /><testcase classname="tests.unit.test_renamed_stages.TestStageImportPatterns" name="test_run_pipeline_no_old_sys_path_insertions" time="0.000" /><testcase classname="tests.unit.test_renamed_stages.TestStageModuleMetadata" name="test_module_has_docstring[scripts.03_glossary_load]" time="0.000" /><testcase classname="tests.unit.test_renamed_stages.TestStageModuleMetadata" name="test_module_has_docstring[scripts.03_glossary_learner]" time="0.000" /><testcase classname="tests.unit.test_renamed_stages.TestStageModuleMetadata" name="test_module_has_docstring[scripts.05_ner]" time="0.000" /><testcase classname="tests.unit.test_renamed_stages.TestStageModuleMetadata" name="test_module_has_docstring[scripts.06_lyrics_detection]" time="0.000" /><testcase classname="tests.unit.test_renamed_stages.TestStageModuleMetadata" name="test_module_has_docstring[scripts.07_hallucination_removal]" time="0.000" /><testcase classname="tests.unit.test_renamed_stages.TestStageModuleMetadata" name="test_module_has_docstring[scripts.09_subtitle_gen]" time="0.000" /><testcase classname="tests.unit.test_renamed_stages.TestStageModuleMetadata" name="test_module_has_logger[scripts.03_glossary_load]" time="0.001" /><testcase classname="tests.unit.test_renamed_stages.TestStageModuleMetadata" name="test_module_has_logger[scripts.03_glossary_learner]" time="0.001" /><testcase classname="tests.unit.test_renamed_stages.TestStageModuleMetadata" name="test_module_has_logger[scripts.05_ner]" time="0.001" /><testcase classname="tests.unit.test_renamed_stages.TestStageModuleMetadata" name="test_module_has_logger[scripts.06_lyrics_detection]" time="0.001" /><testcase classname="tests.unit.test_renamed_stages.TestStageModuleMetadata" name="test_module_has_logger[scripts.07_hallucination_removal]" time="0.001" /><testcase classname="tests.unit.test_renamed_stages.TestStageModuleMetadata" name="test_module_has_logger[scripts.09_subtitle_gen]" time="0.001" /><testcase classname="tests.unit.test_shared_modules.TestConfigLoader" name="test_config_loader_module_importable" time="0.001" /><testcase classname="tests.unit.test_shared_modules.TestConfigLoader" name="test_shared_config_module_importable" time="0.000" /><testcase classname="tests.unit.test_shared_modules.TestConfigLoader" name="test_load_config_returns_dict" time="0.005"><failure message="AssertionError: load_config should return dict&#10;assert False&#10; +  where False = isinstance(PipelineConfig(job_id='', user_id=1, title=None, year=None, workflow_mode='subtitle-gen', source_language=None, target_language=None, media_processing_mode='full', media_start_time=None, media_end_time=None, docker_registry='rajiup', docker_tag='latest', in_root='', output_root='./out', log_root='./logs', temp_root='./temp', log_level='info', log_format='json', log_to_console=True, log_to_file=True, secrets_path='./config/secrets.json', tmdb_api_key='ea32848e3a843143f35705bf3503557b', hf_token='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', step_demux=True, step_tmdb_metadata=True, step_pre_asr_ner=True, step_vad_silero=True, step_silero_vad=True, step_vad_pyannote=True, step_pyannote_vad=True, step_diarization=True, step_whisperx=True, step_post_asr_ner=True, step_subtitle_gen=True, step_mux=True, audio_sample_rate=16000, audio_channels=1, audio_format='wav', audio_codec='pcm_s16le', tmdb_enabled=True, tmdb_language='en-US', tmdb_infer_from_filename=True, pre_ner_enabled=True, pre_ner_model='en_core_web_trf', pre_ner_confidence_threshold=0.0, pre_ner_entity_types='PERSON,ORG,GPE,LOC,FAC', bias_enabled=True, bias_window_seconds=45, bias_stride_seconds=15, bias_topk=10, bias_min_confidence=0.6, song_bias_enabled=True, song_bias_fuzzy_threshold=0.8, use_musicbrainz=True, cache_musicbrainz=True, whisper_model='large-v3', whisper_compute_type='int8', whisper_batch_size=16, whisper_language='hi', whisper_task='translate', whisper_temperature='0.0,0.2,0.4,0.6,0.8,1.0', whisper_beam_size=5, whisper_best_of=5, whisper_patience=1.0, whisper_length_penalty=1.0, whisper_no_speech_threshold=0.6, whisper_logprob_threshold=-1.0, whisper_compression_ratio_threshold=2.4, whisper_condition_on_previous_text=True, whisper_initial_prompt='', whisperx_device='auto', whisperx_backend='auto', whisperx_align_extend=2.0, whisperx_align_from_prev=True, src_lang='hi', tgt_lang='en', source_lang='hi', target_lang='en', silero_threshold=0.6, silero_min_speech_duration_ms=250, silero_min_silence_duration_ms=300, silero_merge_gap_sec=0.35, pyannote_onset=0.5, pyannote_offset=0.5, pyannote_min_duration_on=0.0, pyannote_min_duration_off=0.0, pyannote_device='auto', pyannote_window_pad=0.25, pyannote_merge_gap=0.2, diarization_min_speakers=1, diarization_max_speakers=10, diarization_model='pyannote/speaker-diarization-3.1', diarization_device='auto', diarization_method='pyannote', speaker_map='', subtitle_format='srt', subtitle_max_line_length=42, subtitle_max_lines=2, subtitle_include_speaker_labels=True, subtitle_speaker_format='[{speaker}]', subtitle_word_level_timestamps=False, subtitle_max_duration=7.0, subtitle_min_duration=1.0, subtitle_merge_short=True, glossary_enable=True, glossary_seed_sources='asr,tmdb,master', glossary_min_conf=0.55, glossary_master='glossary/hinglish_master.tsv', glossary_prompts_dir='glossary/prompts', glossary_cache_dir='glossary/cache', glossary_cache_enabled=True, glossary_cache_ttl_days=30, glossary_learning_enabled=False, glossary_auto_learn=True, glossary_min_occurrences=2, glossary_confidence_threshold=3, glossary_enabled=True, glossary_path='glossary/hinglish_master.tsv', glossary_strategy='adaptive', film_prompt_path='', frequency_data_path='glossary/learned/term_frequency.json', cps_target=15.0, cps_hard_cap=17.0, cps_enforcement=True, second_pass_enabled=True, second_pass_backend='nllb', lyric_detect_enabled=True, lyric_threshold=0.5, lyric_style='lyric', lyric_min_duration=30.0, post_ner_model='en_core_web_trf', post_ner_entity_correction=True, post_ner_tmdb_matching=True, post_ner_confidence_threshold=0.8, post_ner_device='cpu', mux_subtitle_codec='mov_text', mux_subtitle_language='eng', mux_subtitle_title='English', mux_copy_video=True, mux_copy_audio=True, mux_container_format='mp4', device='cpu', device_whisperx='cpu', device_diarization='cpu', device_vad='cpu', device_ner='cpu', docker_memory_limit='10g', docker_cpu_limit=4, enable_chunking=False, chunk_duration_minutes=30, cleanup_temp_files=True, keep_intermediate_files=False, max_retries=3), dict)">E   AssertionError: load_config should return dict
    assert False
     +  where False = isinstance(PipelineConfig(job_id='', user_id=1, title=None, year=None, workflow_mode='subtitle-gen', source_language=None, target_language=None, media_processing_mode='full', media_start_time=None, media_end_time=None, docker_registry='rajiup', docker_tag='latest', in_root='', output_root='./out', log_root='./logs', temp_root='./temp', log_level='info', log_format='json', log_to_console=True, log_to_file=True, secrets_path='./config/secrets.json', tmdb_api_key='ea32848e3a843143f35705bf3503557b', hf_token='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', step_demux=True, step_tmdb_metadata=True, step_pre_asr_ner=True, step_vad_silero=True, step_silero_vad=True, step_vad_pyannote=True, step_pyannote_vad=True, step_diarization=True, step_whisperx=True, step_post_asr_ner=True, step_subtitle_gen=True, step_mux=True, audio_sample_rate=16000, audio_channels=1, audio_format='wav', audio_codec='pcm_s16le', tmdb_enabled=True, tmdb_language='en-US', tmdb_infer_from_filename=True, pre_ner_enabled=True, pre_ner_model='en_core_web_trf', pre_ner_confidence_threshold=0.0, pre_ner_entity_types='PERSON,ORG,GPE,LOC,FAC', bias_enabled=True, bias_window_seconds=45, bias_stride_seconds=15, bias_topk=10, bias_min_confidence=0.6, song_bias_enabled=True, song_bias_fuzzy_threshold=0.8, use_musicbrainz=True, cache_musicbrainz=True, whisper_model='large-v3', whisper_compute_type='int8', whisper_batch_size=16, whisper_language='hi', whisper_task='translate', whisper_temperature='0.0,0.2,0.4,0.6,0.8,1.0', whisper_beam_size=5, whisper_best_of=5, whisper_patience=1.0, whisper_length_penalty=1.0, whisper_no_speech_threshold=0.6, whisper_logprob_threshold=-1.0, whisper_compression_ratio_threshold=2.4, whisper_condition_on_previous_text=True, whisper_initial_prompt='', whisperx_device='auto', whisperx_backend='auto', whisperx_align_extend=2.0, whisperx_align_from_prev=True, src_lang='hi', tgt_lang='en', source_lang='hi', target_lang='en', silero_threshold=0.6, silero_min_speech_duration_ms=250, silero_min_silence_duration_ms=300, silero_merge_gap_sec=0.35, pyannote_onset=0.5, pyannote_offset=0.5, pyannote_min_duration_on=0.0, pyannote_min_duration_off=0.0, pyannote_device='auto', pyannote_window_pad=0.25, pyannote_merge_gap=0.2, diarization_min_speakers=1, diarization_max_speakers=10, diarization_model='pyannote/speaker-diarization-3.1', diarization_device='auto', diarization_method='pyannote', speaker_map='', subtitle_format='srt', subtitle_max_line_length=42, subtitle_max_lines=2, subtitle_include_speaker_labels=True, subtitle_speaker_format='[{speaker}]', subtitle_word_level_timestamps=False, subtitle_max_duration=7.0, subtitle_min_duration=1.0, subtitle_merge_short=True, glossary_enable=True, glossary_seed_sources='asr,tmdb,master', glossary_min_conf=0.55, glossary_master='glossary/hinglish_master.tsv', glossary_prompts_dir='glossary/prompts', glossary_cache_dir='glossary/cache', glossary_cache_enabled=True, glossary_cache_ttl_days=30, glossary_learning_enabled=False, glossary_auto_learn=True, glossary_min_occurrences=2, glossary_confidence_threshold=3, glossary_enabled=True, glossary_path='glossary/hinglish_master.tsv', glossary_strategy='adaptive', film_prompt_path='', frequency_data_path='glossary/learned/term_frequency.json', cps_target=15.0, cps_hard_cap=17.0, cps_enforcement=True, second_pass_enabled=True, second_pass_backend='nllb', lyric_detect_enabled=True, lyric_threshold=0.5, lyric_style='lyric', lyric_min_duration=30.0, post_ner_model='en_core_web_trf', post_ner_entity_correction=True, post_ner_tmdb_matching=True, post_ner_confidence_threshold=0.8, post_ner_device='cpu', mux_subtitle_codec='mov_text', mux_subtitle_language='eng', mux_subtitle_title='English', mux_copy_video=True, mux_copy_audio=True, mux_container_format='mp4', device='cpu', device_whisperx='cpu', device_diarization='cpu', device_vad='cpu', device_ner='cpu', docker_memory_limit='10g', docker_cpu_limit=4, enable_chunking=False, chunk_duration_minutes=30, cleanup_temp_files=True, keep_intermediate_files=False, max_retries=3), dict)</failure></testcase><testcase classname="tests.unit.test_shared_modules.TestConfigLoader" name="test_config_handles_missing_file_gracefully" time="0.003"><failure message="AssertionError: assert False&#10; +  where False = isinstance(PipelineConfig(job_id='', user_id=1, title=None, year=None, workflow_mode='subtitle-gen', source_language=None, target_language=None, media_processing_mode='full', media_start_time=None, media_end_time=None, docker_registry='rajiup', docker_tag='latest', in_root='', output_root='./out', log_root='./logs', temp_root='./temp', log_level='info', log_format='json', log_to_console=True, log_to_file=True, secrets_path='./config/secrets.json', tmdb_api_key='ea32848e3a843143f35705bf3503557b', hf_token='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', step_demux=True, step_tmdb_metadata=True, step_pre_asr_ner=True, step_vad_silero=True, step_silero_vad=True, step_vad_pyannote=True, step_pyannote_vad=True, step_diarization=True, step_whisperx=True, step_post_asr_ner=True, step_subtitle_gen=True, step_mux=True, audio_sample_rate=16000, audio_channels=1, audio_format='wav', audio_codec='pcm_s16le', tmdb_enabled=True, tmdb_language='en-US', tmdb_infer_from_filename=True, pre_ner_enabled=True, pre_ner_model='en_core_web_trf', pre_ner_confidence_threshold=0.0, pre_ner_entity_types='PERSON,ORG,GPE,LOC,FAC', bias_enabled=True, bias_window_seconds=45, bias_stride_seconds=15, bias_topk=10, bias_min_confidence=0.6, song_bias_enabled=True, song_bias_fuzzy_threshold=0.8, use_musicbrainz=True, cache_musicbrainz=True, whisper_model='large-v3', whisper_compute_type='int8', whisper_batch_size=16, whisper_language='hi', whisper_task='translate', whisper_temperature='0.0,0.2,0.4,0.6,0.8,1.0', whisper_beam_size=5, whisper_best_of=5, whisper_patience=1.0, whisper_length_penalty=1.0, whisper_no_speech_threshold=0.6, whisper_logprob_threshold=-1.0, whisper_compression_ratio_threshold=2.4, whisper_condition_on_previous_text=True, whisper_initial_prompt='', whisperx_device='auto', whisperx_backend='auto', whisperx_align_extend=2.0, whisperx_align_from_prev=True, src_lang='hi', tgt_lang='en', source_lang='hi', target_lang='en', silero_threshold=0.6, silero_min_speech_duration_ms=250, silero_min_silence_duration_ms=300, silero_merge_gap_sec=0.35, pyannote_onset=0.5, pyannote_offset=0.5, pyannote_min_duration_on=0.0, pyannote_min_duration_off=0.0, pyannote_device='auto', pyannote_window_pad=0.25, pyannote_merge_gap=0.2, diarization_min_speakers=1, diarization_max_speakers=10, diarization_model='pyannote/speaker-diarization-3.1', diarization_device='auto', diarization_method='pyannote', speaker_map='', subtitle_format='srt', subtitle_max_line_length=42, subtitle_max_lines=2, subtitle_include_speaker_labels=True, subtitle_speaker_format='[{speaker}]', subtitle_word_level_timestamps=False, subtitle_max_duration=7.0, subtitle_min_duration=1.0, subtitle_merge_short=True, glossary_enable=True, glossary_seed_sources='asr,tmdb,master', glossary_min_conf=0.55, glossary_master='glossary/hinglish_master.tsv', glossary_prompts_dir='glossary/prompts', glossary_cache_dir='glossary/cache', glossary_cache_enabled=True, glossary_cache_ttl_days=30, glossary_learning_enabled=False, glossary_auto_learn=True, glossary_min_occurrences=2, glossary_confidence_threshold=3, glossary_enabled=True, glossary_path='glossary/hinglish_master.tsv', glossary_strategy='adaptive', film_prompt_path='', frequency_data_path='glossary/learned/term_frequency.json', cps_target=15.0, cps_hard_cap=17.0, cps_enforcement=True, second_pass_enabled=True, second_pass_backend='nllb', lyric_detect_enabled=True, lyric_threshold=0.5, lyric_style='lyric', lyric_min_duration=30.0, post_ner_model='en_core_web_trf', post_ner_entity_correction=True, post_ner_tmdb_matching=True, post_ner_confidence_threshold=0.8, post_ner_device='cpu', mux_subtitle_codec='mov_text', mux_subtitle_language='eng', mux_subtitle_title='English', mux_copy_video=True, mux_copy_audio=True, mux_container_format='mp4', device='cpu', device_whisperx='cpu', device_diarization='cpu', device_vad='cpu', device_ner='cpu', docker_memory_limit='10g', docker_cpu_limit=4, enable_chunking=False, chunk_duration_minutes=30, cleanup_temp_files=True, keep_intermediate_files=False, max_retries=3), dict)">E   AssertionError: assert False
     +  where False = isinstance(PipelineConfig(job_id='', user_id=1, title=None, year=None, workflow_mode='subtitle-gen', source_language=None, target_language=None, media_processing_mode='full', media_start_time=None, media_end_time=None, docker_registry='rajiup', docker_tag='latest', in_root='', output_root='./out', log_root='./logs', temp_root='./temp', log_level='info', log_format='json', log_to_console=True, log_to_file=True, secrets_path='./config/secrets.json', tmdb_api_key='ea32848e3a843143f35705bf3503557b', hf_token='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', step_demux=True, step_tmdb_metadata=True, step_pre_asr_ner=True, step_vad_silero=True, step_silero_vad=True, step_vad_pyannote=True, step_pyannote_vad=True, step_diarization=True, step_whisperx=True, step_post_asr_ner=True, step_subtitle_gen=True, step_mux=True, audio_sample_rate=16000, audio_channels=1, audio_format='wav', audio_codec='pcm_s16le', tmdb_enabled=True, tmdb_language='en-US', tmdb_infer_from_filename=True, pre_ner_enabled=True, pre_ner_model='en_core_web_trf', pre_ner_confidence_threshold=0.0, pre_ner_entity_types='PERSON,ORG,GPE,LOC,FAC', bias_enabled=True, bias_window_seconds=45, bias_stride_seconds=15, bias_topk=10, bias_min_confidence=0.6, song_bias_enabled=True, song_bias_fuzzy_threshold=0.8, use_musicbrainz=True, cache_musicbrainz=True, whisper_model='large-v3', whisper_compute_type='int8', whisper_batch_size=16, whisper_language='hi', whisper_task='translate', whisper_temperature='0.0,0.2,0.4,0.6,0.8,1.0', whisper_beam_size=5, whisper_best_of=5, whisper_patience=1.0, whisper_length_penalty=1.0, whisper_no_speech_threshold=0.6, whisper_logprob_threshold=-1.0, whisper_compression_ratio_threshold=2.4, whisper_condition_on_previous_text=True, whisper_initial_prompt='', whisperx_device='auto', whisperx_backend='auto', whisperx_align_extend=2.0, whisperx_align_from_prev=True, src_lang='hi', tgt_lang='en', source_lang='hi', target_lang='en', silero_threshold=0.6, silero_min_speech_duration_ms=250, silero_min_silence_duration_ms=300, silero_merge_gap_sec=0.35, pyannote_onset=0.5, pyannote_offset=0.5, pyannote_min_duration_on=0.0, pyannote_min_duration_off=0.0, pyannote_device='auto', pyannote_window_pad=0.25, pyannote_merge_gap=0.2, diarization_min_speakers=1, diarization_max_speakers=10, diarization_model='pyannote/speaker-diarization-3.1', diarization_device='auto', diarization_method='pyannote', speaker_map='', subtitle_format='srt', subtitle_max_line_length=42, subtitle_max_lines=2, subtitle_include_speaker_labels=True, subtitle_speaker_format='[{speaker}]', subtitle_word_level_timestamps=False, subtitle_max_duration=7.0, subtitle_min_duration=1.0, subtitle_merge_short=True, glossary_enable=True, glossary_seed_sources='asr,tmdb,master', glossary_min_conf=0.55, glossary_master='glossary/hinglish_master.tsv', glossary_prompts_dir='glossary/prompts', glossary_cache_dir='glossary/cache', glossary_cache_enabled=True, glossary_cache_ttl_days=30, glossary_learning_enabled=False, glossary_auto_learn=True, glossary_min_occurrences=2, glossary_confidence_threshold=3, glossary_enabled=True, glossary_path='glossary/hinglish_master.tsv', glossary_strategy='adaptive', film_prompt_path='', frequency_data_path='glossary/learned/term_frequency.json', cps_target=15.0, cps_hard_cap=17.0, cps_enforcement=True, second_pass_enabled=True, second_pass_backend='nllb', lyric_detect_enabled=True, lyric_threshold=0.5, lyric_style='lyric', lyric_min_duration=30.0, post_ner_model='en_core_web_trf', post_ner_entity_correction=True, post_ner_tmdb_matching=True, post_ner_confidence_threshold=0.8, post_ner_device='cpu', mux_subtitle_codec='mov_text', mux_subtitle_language='eng', mux_subtitle_title='English', mux_copy_video=True, mux_copy_audio=True, mux_container_format='mp4', device='cpu', device_whisperx='cpu', device_diarization='cpu', device_vad='cpu', device_ner='cpu', docker_memory_limit='10g', docker_cpu_limit=4, enable_chunking=False, chunk_duration_minutes=30, cleanup_temp_files=True, keep_intermediate_files=False, max_retries=3), dict)</failure></testcase><testcase classname="tests.unit.test_shared_modules.TestStageUtils" name="test_stage_utils_importable" time="0.000" /><testcase classname="tests.unit.test_shared_modules.TestStageUtils" name="test_stageio_class_exists" time="0.000" /><testcase classname="tests.unit.test_shared_modules.TestStageUtils" name="test_stageio_can_be_instantiated" time="0.002" /><testcase classname="tests.unit.test_shared_modules.TestStageUtils" name="test_stageio_creates_stage_directory" time="0.002"><failure message="AssertionError: StageIO should create stage directory&#10;assert False&#10; +  where False = exists()&#10; +    where exists = PosixPath('/var/folders/3s/1tzr9fgd00n236ct50ny49vh0000gn/T/tmp598evl0s/test_stage').exists">E   AssertionError: StageIO should create stage directory
    assert False
     +  where False = exists()
     +    where exists = PosixPath('/var/folders/3s/1tzr9fgd00n236ct50ny49vh0000gn/T/tmp598evl0s/test_stage').exists</failure></testcase><testcase classname="tests.unit.test_shared_modules.TestStageUtils" name="test_stageio_has_stage_logger_method" time="0.002" /><testcase classname="tests.unit.test_shared_modules.TestLogger" name="test_logger_module_importable" time="0.000" /><testcase classname="tests.unit.test_shared_modules.TestLogger" name="test_get_logger_returns_logger" time="0.000" /><testcase classname="tests.unit.test_shared_modules.TestLogger" name="test_get_logger_with_name" time="0.000" /><testcase classname="tests.unit.test_shared_modules.TestLogger" name="test_pipeline_logger_class_exists" time="0.000" /><testcase classname="tests.unit.test_shared_modules.TestManifest" name="test_manifest_module_importable" time="0.000" /><testcase classname="tests.unit.test_shared_modules.TestManifest" name="test_manifest_can_be_instantiated" time="0.001" /><testcase classname="tests.unit.test_shared_modules.TestManifest" name="test_manifest_has_add_input_method" time="0.001"><skipped type="pytest.skip" message="StageManifest API may differ - adjust test based on actual implementation">/Users/rpatel/Projects/Active/cp-whisperx-app/tests/unit/test_shared_modules.py:201: StageManifest API may differ - adjust test based on actual implementation</skipped></testcase><testcase classname="tests.unit.test_shared_modules.TestManifest" name="test_manifest_has_add_output_method" time="0.001" /><testcase classname="tests.unit.test_shared_modules.TestEnvironmentManager" name="test_environment_manager_importable" time="0.003" /><testcase classname="tests.unit.test_shared_modules.TestEnvironmentManager" name="test_environment_manager_can_be_instantiated" time="0.001" /><testcase classname="tests.unit.test_shared_modules.TestEnvironmentManager" name="test_environment_manager_detects_platform" time="0.000" /><testcase classname="tests.unit.test_shared_modules.TestStageDependencies" name="test_stage_dependencies_module_importable" time="0.001" /><testcase classname="tests.unit.test_shared_modules.TestStageDependencies" name="test_validate_stage_dependencies_function_exists" time="0.000" /><testcase classname="tests.unit.test_shared_modules.TestStageDependencies" name="test_get_workflow_stages_function_exists" time="0.000" /><testcase classname="tests.unit.test_shared_modules.TestStageDependencies" name="test_stage_dependencies_dict_exists" time="0.000" /><testcase classname="tests.unit.test_shared_modules.TestStageDependencies" name="test_workflow_presets_dict_exists" time="0.000" /><testcase classname="tests.unit.test_shared_modules.TestStageOrder" name="test_stage_order_module_importable" time="0.000" /><testcase classname="tests.unit.test_shared_modules.TestStageOrder" name="test_get_stage_dir_function_exists" time="0.000" /><testcase classname="tests.unit.test_shared_modules.TestStageOrder" name="test_get_stage_dir_returns_string" time="0.000" /><testcase classname="tests.unit.test_shared_modules.TestHardwareDetection" name="test_hardware_detection_importable" time="0.012" /><testcase classname="tests.unit.test_shared_modules.TestHardwareDetection" name="test_hardware_detection_has_functions" time="0.000" /><testcase classname="tests.unit.test_shared_modules.TestAllSharedModules" name="test_shared_module_importable[shared.config]" time="0.000" /><testcase classname="tests.unit.test_shared_modules.TestAllSharedModules" name="test_shared_module_importable[shared.logger]" time="0.000" /><testcase classname="tests.unit.test_shared_modules.TestAllSharedModules" name="test_shared_module_importable[shared.manifest]" time="0.000" /><testcase classname="tests.unit.test_shared_modules.TestAllSharedModules" name="test_shared_module_importable[shared.stage_utils]" time="0.000" /><testcase classname="tests.unit.test_shared_modules.TestAllSharedModules" name="test_shared_module_importable[shared.environment_manager]" time="0.000" /><testcase classname="tests.unit.test_shared_modules.TestAllSharedModules" name="test_shared_module_importable[shared.stage_dependencies]" time="0.000" /><testcase classname="tests.unit.test_shared_modules.TestAllSharedModules" name="test_shared_module_importable[shared.stage_order]" time="0.000" /><testcase classname="tests.unit.test_shared_modules.TestAllSharedModules" name="test_shared_module_importable[shared.hardware_detection]" time="0.000" /><testcase classname="tests.unit.test_shared_modules.TestAllSharedModules" name="test_shared_module_importable[shared.model_checker]" time="0.003" /><testcase classname="tests.unit.test_shared_modules.TestAllSharedModules" name="test_shared_module_has_docstring[shared.config]" time="0.000" /><testcase classname="tests.unit.test_shared_modules.TestAllSharedModules" name="test_shared_module_has_docstring[shared.logger]" time="0.000" /><testcase classname="tests.unit.test_shared_modules.TestAllSharedModules" name="test_shared_module_has_docstring[shared.manifest]" time="0.000" /><testcase classname="tests.unit.test_shared_modules.TestAllSharedModules" name="test_shared_module_has_docstring[shared.stage_utils]" time="0.000" /><testcase classname="tests.unit.test_shared_modules.TestAllSharedModules" name="test_shared_module_has_docstring[shared.environment_manager]" time="0.000" /><testcase classname="tests.unit.test_shared_modules.TestAllSharedModules" name="test_shared_module_has_docstring[shared.stage_dependencies]" time="0.000" /><testcase classname="tests.unit.test_shared_modules.TestAllSharedModules" name="test_shared_module_has_docstring[shared.stage_order]" time="0.000" /><testcase classname="tests.unit.test_shared_modules.TestAllSharedModules" name="test_shared_module_has_docstring[shared.hardware_detection]" time="0.000" /><testcase classname="tests.unit.test_shared_modules.TestAllSharedModules" name="test_shared_module_has_docstring[shared.model_checker]" time="0.000" /><testcase classname="tests.unit.test_shared_modules.TestAllSharedModules" name="test_shared_module_file_exists[shared.config]" time="0.000" /><testcase classname="tests.unit.test_shared_modules.TestAllSharedModules" name="test_shared_module_file_exists[shared.logger]" time="0.000" /><testcase classname="tests.unit.test_shared_modules.TestAllSharedModules" name="test_shared_module_file_exists[shared.manifest]" time="0.000" /><testcase classname="tests.unit.test_shared_modules.TestAllSharedModules" name="test_shared_module_file_exists[shared.stage_utils]" time="0.000" /><testcase classname="tests.unit.test_shared_modules.TestAllSharedModules" name="test_shared_module_file_exists[shared.environment_manager]" time="0.000" /><testcase classname="tests.unit.test_shared_modules.TestAllSharedModules" name="test_shared_module_file_exists[shared.stage_dependencies]" time="0.000" /><testcase classname="tests.unit.test_shared_modules.TestAllSharedModules" name="test_shared_module_file_exists[shared.stage_order]" time="0.000" /><testcase classname="tests.unit.test_shared_modules.TestAllSharedModules" name="test_shared_module_file_exists[shared.hardware_detection]" time="0.000" /><testcase classname="tests.unit.test_shared_modules.TestAllSharedModules" name="test_shared_module_file_exists[shared.model_checker]" time="0.000" /><testcase classname="tests.unit.stages.test_core_stages.TestDemuxStage" name="test_demux_script_exists" time="0.000" /><testcase classname="tests.unit.stages.test_core_stages.TestDemuxStage" name="test_demux_is_executable" time="0.001" /><testcase classname="tests.unit.stages.test_core_stages.TestDemuxStage" name="test_demux_imports_correctly" time="0.003" /><testcase classname="tests.unit.stages.test_core_stages.TestTMDBEnrichmentStage" name="test_tmdb_enrichment_module_importable" time="0.108" /><testcase classname="tests.unit.stages.test_core_stages.TestTMDBEnrichmentStage" name="test_tmdb_enrichment_has_run_stage" time="0.000" /><testcase classname="tests.unit.stages.test_core_stages.TestTMDBEnrichmentStage" name="test_tmdb_enrichment_has_docstring" time="0.000" /><testcase classname="tests.unit.stages.test_core_stages.TestSourceSeparationStage" name="test_source_separation_module_importable" time="0.003" /><testcase classname="tests.unit.stages.test_core_stages.TestSourceSeparationStage" name="test_source_separation_has_run_stage" time="0.000"><failure message="AssertionError: Missing run_stage() function&#10;assert False&#10; +  where False = hasattr(&lt;module 'scripts.04_source_separation' from '/Users/rpatel/Projects/Active/cp-whisperx-app/scripts/04_source_separation.py'&gt;, 'run_stage')">E   AssertionError: Missing run_stage() function
    assert False
     +  where False = hasattr(&lt;module 'scripts.04_source_separation' from '/Users/rpatel/Projects/Active/cp-whisperx-app/scripts/04_source_separation.py'&gt;, 'run_stage')</failure></testcase><testcase classname="tests.unit.stages.test_core_stages.TestSourceSeparationStage" name="test_source_separation_uses_logger" time="0.001" /><testcase classname="tests.unit.stages.test_core_stages.TestPyannoteVADStage" name="test_pyannote_vad_module_importable" time="0.001"><failure message="  File &quot;/Users/rpatel/Projects/Active/cp-whisperx-app/scripts/05_pyannote_vad.py&quot;, line 90&#10;    logger.error(f&quot;✗ File not found: {e}&quot;, exc_info=True, exc_info=True)&#10;                                                            ^^^^^^^^^^^^^&#10;SyntaxError: keyword argument repeated: exc_info">E     File "/Users/rpatel/Projects/Active/cp-whisperx-app/scripts/05_pyannote_vad.py", line 90
        logger.error(f"✗ File not found: {e}", exc_info=True, exc_info=True)
                                                                ^^^^^^^^^^^^^
    SyntaxError: keyword argument repeated: exc_info</failure></testcase><testcase classname="tests.unit.stages.test_core_stages.TestPyannoteVADStage" name="test_pyannote_vad_has_run_stage" time="0.001"><failure message="  File &quot;/Users/rpatel/Projects/Active/cp-whisperx-app/scripts/05_pyannote_vad.py&quot;, line 90&#10;    logger.error(f&quot;✗ File not found: {e}&quot;, exc_info=True, exc_info=True)&#10;                                                            ^^^^^^^^^^^^^&#10;SyntaxError: keyword argument repeated: exc_info">E     File "/Users/rpatel/Projects/Active/cp-whisperx-app/scripts/05_pyannote_vad.py", line 90
        logger.error(f"✗ File not found: {e}", exc_info=True, exc_info=True)
                                                                ^^^^^^^^^^^^^
    SyntaxError: keyword argument repeated: exc_info</failure></testcase><testcase classname="tests.unit.stages.test_core_stages.TestPyannoteVADStage" name="test_pyannote_vad_has_docstring" time="0.001"><failure message="  File &quot;/Users/rpatel/Projects/Active/cp-whisperx-app/scripts/05_pyannote_vad.py&quot;, line 90&#10;    logger.error(f&quot;✗ File not found: {e}&quot;, exc_info=True, exc_info=True)&#10;                                                            ^^^^^^^^^^^^^&#10;SyntaxError: keyword argument repeated: exc_info">E     File "/Users/rpatel/Projects/Active/cp-whisperx-app/scripts/05_pyannote_vad.py", line 90
        logger.error(f"✗ File not found: {e}", exc_info=True, exc_info=True)
                                                                ^^^^^^^^^^^^^
    SyntaxError: keyword argument repeated: exc_info</failure></testcase><testcase classname="tests.unit.stages.test_core_stages.TestWhisperXASRStage" name="test_whisperx_asr_module_importable" time="0.003"><failure message="Failed: Failed to import 06_whisperx_asr: No module named 'whisperx_integration'">E   ModuleNotFoundError: No module named 'whisperx_integration'

During handling of the above exception, another exception occurred:
E   Failed: Failed to import 06_whisperx_asr: No module named 'whisperx_integration'</failure></testcase><testcase classname="tests.unit.stages.test_core_stages.TestWhisperXASRStage" name="test_whisperx_asr_has_run_stage" time="0.001"><failure message="ModuleNotFoundError: No module named 'whisperx_integration'">E   ModuleNotFoundError: No module named 'whisperx_integration'</failure></testcase><testcase classname="tests.unit.stages.test_core_stages.TestWhisperXASRStage" name="test_whisperx_asr_uses_logger" time="0.000" /><testcase classname="tests.unit.stages.test_core_stages.TestAlignmentStage" name="test_alignment_module_importable" time="0.002"><failure message="  File &quot;/Users/rpatel/Projects/Active/cp-whisperx-app/scripts/07_alignment.py&quot;, line 260&#10;    logger: logging.Logger&#10;          ^&#10;SyntaxError: invalid syntax">E     File "/Users/rpatel/Projects/Active/cp-whisperx-app/scripts/07_alignment.py", line 260
        logger: logging.Logger
              ^
    SyntaxError: invalid syntax</failure></testcase><testcase classname="tests.unit.stages.test_core_stages.TestAlignmentStage" name="test_alignment_has_run_stage" time="0.002"><failure message="  File &quot;/Users/rpatel/Projects/Active/cp-whisperx-app/scripts/07_alignment.py&quot;, line 260&#10;    logger: logging.Logger&#10;          ^&#10;SyntaxError: invalid syntax">E     File "/Users/rpatel/Projects/Active/cp-whisperx-app/scripts/07_alignment.py", line 260
        logger: logging.Logger
              ^
    SyntaxError: invalid syntax</failure></testcase><testcase classname="tests.unit.stages.test_core_stages.TestAlignmentStage" name="test_alignment_has_docstring" time="0.002"><failure message="  File &quot;/Users/rpatel/Projects/Active/cp-whisperx-app/scripts/07_alignment.py&quot;, line 260&#10;    logger: logging.Logger&#10;          ^&#10;SyntaxError: invalid syntax">E     File "/Users/rpatel/Projects/Active/cp-whisperx-app/scripts/07_alignment.py", line 260
        logger: logging.Logger
              ^
    SyntaxError: invalid syntax</failure></testcase><testcase classname="tests.unit.stages.test_core_stages.TestTranslationStage" name="test_translation_module_importable" time="20.663" /><testcase classname="tests.unit.stages.test_core_stages.TestTranslationStage" name="test_translation_has_run_stage" time="0.000" /><testcase classname="tests.unit.stages.test_core_stages.TestTranslationStage" name="test_translation_uses_logger" time="0.001" /><testcase classname="tests.unit.stages.test_core_stages.TestTranslationStage" name="test_translation_has_docstring" time="0.000" /><testcase classname="tests.unit.stages.test_core_stages.TestMuxStage" name="test_mux_module_importable" time="0.007" /><testcase classname="tests.unit.stages.test_core_stages.TestMuxStage" name="test_mux_has_run_stage" time="0.000" /><testcase classname="tests.unit.stages.test_core_stages.TestMuxStage" name="test_mux_has_docstring" time="0.000" /><testcase classname="tests.unit.stages.test_core_stages.TestAllStagesCommon" name="test_stage_file_exists[scripts.01_demux]" time="0.000" /><testcase classname="tests.unit.stages.test_core_stages.TestAllStagesCommon" name="test_stage_file_exists[scripts.02_tmdb_enrichment]" time="0.000" /><testcase classname="tests.unit.stages.test_core_stages.TestAllStagesCommon" name="test_stage_file_exists[scripts.04_source_separation]" time="0.000" /><testcase classname="tests.unit.stages.test_core_stages.TestAllStagesCommon" name="test_stage_file_exists[scripts.05_pyannote_vad]" time="0.004" /><testcase classname="tests.unit.stages.test_core_stages.TestAllStagesCommon" name="test_stage_file_exists[scripts.06_whisperx_asr]" time="0.000" /><testcase classname="tests.unit.stages.test_core_stages.TestAllStagesCommon" name="test_stage_file_exists[scripts.07_alignment]" time="0.000" /><testcase classname="tests.unit.stages.test_core_stages.TestAllStagesCommon" name="test_stage_file_exists[scripts.08_translation]" time="0.000" /><testcase classname="tests.unit.stages.test_core_stages.TestAllStagesCommon" name="test_stage_file_exists[scripts.10_mux]" time="0.000" /><testcase classname="tests.unit.stages.test_core_stages.TestAllStagesCommon" name="test_stage_has_no_syntax_errors[scripts.01_demux]" time="0.010" /><testcase classname="tests.unit.stages.test_core_stages.TestAllStagesCommon" name="test_stage_has_no_syntax_errors[scripts.02_tmdb_enrichment]" time="0.013" /><testcase classname="tests.unit.stages.test_core_stages.TestAllStagesCommon" name="test_stage_has_no_syntax_errors[scripts.04_source_separation]" time="0.007" /><testcase classname="tests.unit.stages.test_core_stages.TestAllStagesCommon" name="test_stage_has_no_syntax_errors[scripts.05_pyannote_vad]" time="0.002"><failure message="Failed: 05_pyannote_vad.py has syntax errors:   File &quot;/Users/rpatel/Projects/Active/cp-whisperx-app/scripts/05_pyannote_vad.py&quot;, line 90&#10;    logger.error(f&quot;✗ File not found: {e}&quot;, exc_info=True, exc_info=True)&#10;                                                            ^^^^^^^^^^^^^&#10;SyntaxError: keyword argument repeated: exc_info">E     File "/Users/rpatel/Projects/Active/cp-whisperx-app/scripts/05_pyannote_vad.py", line 90
        logger.error(f"✗ File not found: {e}", exc_info=True, exc_info=True)
                                                                ^^^^^^^^^^^^^
    SyntaxError: keyword argument repeated: exc_info

During handling of the above exception, another exception occurred:
E   py_compile.PyCompileError:   File "/Users/rpatel/Projects/Active/cp-whisperx-app/scripts/05_pyannote_vad.py", line 90
        logger.error(f"✗ File not found: {e}", exc_info=True, exc_info=True)
                                                                ^^^^^^^^^^^^^
    SyntaxError: keyword argument repeated: exc_info

During handling of the above exception, another exception occurred:
E   Failed: 05_pyannote_vad.py has syntax errors:   File "/Users/rpatel/Projects/Active/cp-whisperx-app/scripts/05_pyannote_vad.py", line 90
        logger.error(f"✗ File not found: {e}", exc_info=True, exc_info=True)
                                                                ^^^^^^^^^^^^^
    SyntaxError: keyword argument repeated: exc_info</failure></testcase><testcase classname="tests.unit.stages.test_core_stages.TestAllStagesCommon" name="test_stage_has_no_syntax_errors[scripts.06_whisperx_asr]" time="0.002" /><testcase classname="tests.unit.stages.test_core_stages.TestAllStagesCommon" name="test_stage_has_no_syntax_errors[scripts.07_alignment]" time="0.003"><failure message="Failed: 07_alignment.py has syntax errors:   File &quot;/Users/rpatel/Projects/Active/cp-whisperx-app/scripts/07_alignment.py&quot;, line 260&#10;    logger: logging.Logger&#10;          ^&#10;SyntaxError: invalid syntax">E     File "/Users/rpatel/Projects/Active/cp-whisperx-app/scripts/07_alignment.py", line 260
        logger: logging.Logger
              ^
    SyntaxError: invalid syntax

During handling of the above exception, another exception occurred:
E   py_compile.PyCompileError:   File "/Users/rpatel/Projects/Active/cp-whisperx-app/scripts/07_alignment.py", line 260
        logger: logging.Logger
              ^
    SyntaxError: invalid syntax

During handling of the above exception, another exception occurred:
E   Failed: 07_alignment.py has syntax errors:   File "/Users/rpatel/Projects/Active/cp-whisperx-app/scripts/07_alignment.py", line 260
        logger: logging.Logger
              ^
    SyntaxError: invalid syntax</failure></testcase><testcase classname="tests.unit.stages.test_core_stages.TestAllStagesCommon" name="test_stage_has_no_syntax_errors[scripts.08_translation]" time="0.005" /><testcase classname="tests.unit.stages.test_core_stages.TestAllStagesCommon" name="test_stage_has_no_syntax_errors[scripts.10_mux]" time="0.003" /><testcase classname="tests.unit.stages.test_core_stages.TestAllStagesCommon" name="test_stage_follows_naming_pattern[scripts.01_demux]" time="0.001" /><testcase classname="tests.unit.stages.test_core_stages.TestAllStagesCommon" name="test_stage_follows_naming_pattern[scripts.02_tmdb_enrichment]" time="0.000" /><testcase classname="tests.unit.stages.test_core_stages.TestAllStagesCommon" name="test_stage_follows_naming_pattern[scripts.04_source_separation]" time="0.000" /><testcase classname="tests.unit.stages.test_core_stages.TestAllStagesCommon" name="test_stage_follows_naming_pattern[scripts.05_pyannote_vad]" time="0.000" /><testcase classname="tests.unit.stages.test_core_stages.TestAllStagesCommon" name="test_stage_follows_naming_pattern[scripts.06_whisperx_asr]" time="0.000" /><testcase classname="tests.unit.stages.test_core_stages.TestAllStagesCommon" name="test_stage_follows_naming_pattern[scripts.07_alignment]" time="0.000" /><testcase classname="tests.unit.stages.test_core_stages.TestAllStagesCommon" name="test_stage_follows_naming_pattern[scripts.08_translation]" time="0.000" /><testcase classname="tests.unit.stages.test_core_stages.TestAllStagesCommon" name="test_stage_follows_naming_pattern[scripts.10_mux]" time="0.000" /><testcase classname="tests.unit.stages.test_renamed_stage_entry_points.TestGlossaryLoadStage" name="test_glossary_load_entry_point_exists" time="0.000" /><testcase classname="tests.unit.stages.test_renamed_stage_entry_points.TestGlossaryLoadStage" name="test_glossary_load_creates_output_dir" time="0.000"><skipped type="pytest.skip" message="Phase 3 - Requires full stage implementation">/Users/rpatel/Projects/Active/cp-whisperx-app/tests/unit/stages/test_renamed_stage_entry_points.py:55: Phase 3 - Requires full stage implementation</skipped></testcase><testcase classname="tests.unit.stages.test_renamed_stage_entry_points.TestGlossaryLoadStage" name="test_glossary_load_returns_exit_code" time="0.000"><skipped type="pytest.skip" message="Phase 3 - Requires full stage implementation">/Users/rpatel/Projects/Active/cp-whisperx-app/tests/unit/stages/test_renamed_stage_entry_points.py:67: Phase 3 - Requires full stage implementation</skipped></testcase><testcase classname="tests.unit.stages.test_renamed_stage_entry_points.TestNERStage" name="test_ner_entry_point_exists" time="0.000" /><testcase classname="tests.unit.stages.test_renamed_stage_entry_points.TestNERStage" name="test_ner_creates_output_dir" time="0.000"><skipped type="pytest.skip" message="Phase 3 - Requires full stage implementation">/Users/rpatel/Projects/Active/cp-whisperx-app/tests/unit/stages/test_renamed_stage_entry_points.py:108: Phase 3 - Requires full stage implementation</skipped></testcase><testcase classname="tests.unit.stages.test_renamed_stage_entry_points.TestLyricsDetectionStage" name="test_lyrics_detection_entry_point_exists" time="0.000" /><testcase classname="tests.unit.stages.test_renamed_stage_entry_points.TestLyricsDetectionStage" name="test_lyrics_detection_creates_output_dir" time="0.000"><skipped type="pytest.skip" message="Phase 3 - Requires full stage implementation">/Users/rpatel/Projects/Active/cp-whisperx-app/tests/unit/stages/test_renamed_stage_entry_points.py:149: Phase 3 - Requires full stage implementation</skipped></testcase><testcase classname="tests.unit.stages.test_renamed_stage_entry_points.TestHallucinationRemovalStage" name="test_hallucination_removal_entry_point_exists" time="0.000" /><testcase classname="tests.unit.stages.test_renamed_stage_entry_points.TestHallucinationRemovalStage" name="test_hallucination_removal_creates_output_dir" time="0.000"><skipped type="pytest.skip" message="Phase 3 - Requires full stage implementation">/Users/rpatel/Projects/Active/cp-whisperx-app/tests/unit/stages/test_renamed_stage_entry_points.py:194: Phase 3 - Requires full stage implementation</skipped></testcase><testcase classname="tests.unit.stages.test_renamed_stage_entry_points.TestSubtitleGenStage" name="test_subtitle_gen_entry_point_exists" time="0.000" /><testcase classname="tests.unit.stages.test_renamed_stage_entry_points.TestSubtitleGenStage" name="test_subtitle_gen_creates_output_dir" time="0.000"><skipped type="pytest.skip" message="Phase 3 - Requires full stage implementation">/Users/rpatel/Projects/Active/cp-whisperx-app/tests/unit/stages/test_renamed_stage_entry_points.py:239: Phase 3 - Requires full stage implementation</skipped></testcase><testcase classname="tests.unit.stages.test_renamed_stage_entry_points.TestStageErrorHandling" name="test_stage_handles_missing_job_dir[scripts.03_glossary_load]" time="0.000"><skipped type="pytest.skip" message="Phase 3 - Requires full error handling implementation">/Users/rpatel/Projects/Active/cp-whisperx-app/tests/unit/stages/test_renamed_stage_entry_points.py:267: Phase 3 - Requires full error handling implementation</skipped></testcase><testcase classname="tests.unit.stages.test_renamed_stage_entry_points.TestStageErrorHandling" name="test_stage_handles_missing_job_dir[scripts.05_ner]" time="0.000"><skipped type="pytest.skip" message="Phase 3 - Requires full error handling implementation">/Users/rpatel/Projects/Active/cp-whisperx-app/tests/unit/stages/test_renamed_stage_entry_points.py:267: Phase 3 - Requires full error handling implementation</skipped></testcase><testcase classname="tests.unit.stages.test_renamed_stage_entry_points.TestStageErrorHandling" name="test_stage_handles_missing_job_dir[scripts.06_lyrics_detection]" time="0.000"><skipped type="pytest.skip" message="Phase 3 - Requires full error handling implementation">/Users/rpatel/Projects/Active/cp-whisperx-app/tests/unit/stages/test_renamed_stage_entry_points.py:267: Phase 3 - Requires full error handling implementation</skipped></testcase><testcase classname="tests.unit.stages.test_renamed_stage_entry_points.TestStageErrorHandling" name="test_stage_handles_missing_job_dir[scripts.07_hallucination_removal]" time="0.000"><skipped type="pytest.skip" message="Phase 3 - Requires full error handling implementation">/Users/rpatel/Projects/Active/cp-whisperx-app/tests/unit/stages/test_renamed_stage_entry_points.py:267: Phase 3 - Requires full error handling implementation</skipped></testcase><testcase classname="tests.unit.stages.test_renamed_stage_entry_points.TestStageErrorHandling" name="test_stage_handles_missing_job_dir[scripts.09_subtitle_gen]" time="0.000"><skipped type="pytest.skip" message="Phase 3 - Requires full error handling implementation">/Users/rpatel/Projects/Active/cp-whisperx-app/tests/unit/stages/test_renamed_stage_entry_points.py:267: Phase 3 - Requires full error handling implementation</skipped></testcase><testcase classname="tests.unit.stages.test_renamed_stage_entry_points.TestStageErrorHandling" name="test_stage_handles_missing_prerequisites[scripts.03_glossary_load]" time="0.000"><skipped type="pytest.skip" message="Phase 3 - Requires full error handling implementation">/Users/rpatel/Projects/Active/cp-whisperx-app/tests/unit/stages/test_renamed_stage_entry_points.py:282: Phase 3 - Requires full error handling implementation</skipped></testcase><testcase classname="tests.unit.stages.test_renamed_stage_entry_points.TestStageErrorHandling" name="test_stage_handles_missing_prerequisites[scripts.05_ner]" time="0.000"><skipped type="pytest.skip" message="Phase 3 - Requires full error handling implementation">/Users/rpatel/Projects/Active/cp-whisperx-app/tests/unit/stages/test_renamed_stage_entry_points.py:282: Phase 3 - Requires full error handling implementation</skipped></testcase><testcase classname="tests.unit.stages.test_renamed_stage_entry_points.TestStageErrorHandling" name="test_stage_handles_missing_prerequisites[scripts.06_lyrics_detection]" time="0.000"><skipped type="pytest.skip" message="Phase 3 - Requires full error handling implementation">/Users/rpatel/Projects/Active/cp-whisperx-app/tests/unit/stages/test_renamed_stage_entry_points.py:282: Phase 3 - Requires full error handling implementation</skipped></testcase><testcase classname="tests.unit.stages.test_renamed_stage_entry_points.TestStageErrorHandling" name="test_stage_handles_missing_prerequisites[scripts.07_hallucination_removal]" time="0.000"><skipped type="pytest.skip" message="Phase 3 - Requires full error handling implementation">/Users/rpatel/Projects/Active/cp-whisperx-app/tests/unit/stages/test_renamed_stage_entry_points.py:282: Phase 3 - Requires full error handling implementation</skipped></testcase><testcase classname="tests.unit.stages.test_renamed_stage_entry_points.TestStageErrorHandling" name="test_stage_handles_missing_prerequisites[scripts.09_subtitle_gen]" time="0.000"><skipped type="pytest.skip" message="Phase 3 - Requires full error handling implementation">/Users/rpatel/Projects/Active/cp-whisperx-app/tests/unit/stages/test_renamed_stage_entry_points.py:282: Phase 3 - Requires full error handling implementation</skipped></testcase><testcase classname="tests.unit.stages.test_renamed_stage_entry_points.TestStageManifests" name="test_stage_creates_manifest[scripts.03_glossary_load]" time="0.000"><skipped type="pytest.skip" message="Phase 3 - Requires full StageIO implementation">/Users/rpatel/Projects/Active/cp-whisperx-app/tests/unit/stages/test_renamed_stage_entry_points.py:316: Phase 3 - Requires full StageIO implementation</skipped></testcase><testcase classname="tests.unit.stages.test_renamed_stage_entry_points.TestStageManifests" name="test_stage_creates_manifest[scripts.05_ner]" time="0.000"><skipped type="pytest.skip" message="Phase 3 - Requires full StageIO implementation">/Users/rpatel/Projects/Active/cp-whisperx-app/tests/unit/stages/test_renamed_stage_entry_points.py:316: Phase 3 - Requires full StageIO implementation</skipped></testcase><testcase classname="tests.unit.stages.test_renamed_stage_entry_points.TestStageManifests" name="test_stage_creates_manifest[scripts.06_lyrics_detection]" time="0.000"><skipped type="pytest.skip" message="Phase 3 - Requires full StageIO implementation">/Users/rpatel/Projects/Active/cp-whisperx-app/tests/unit/stages/test_renamed_stage_entry_points.py:316: Phase 3 - Requires full StageIO implementation</skipped></testcase><testcase classname="tests.unit.stages.test_renamed_stage_entry_points.TestStageManifests" name="test_stage_creates_manifest[scripts.07_hallucination_removal]" time="0.000"><skipped type="pytest.skip" message="Phase 3 - Requires full StageIO implementation">/Users/rpatel/Projects/Active/cp-whisperx-app/tests/unit/stages/test_renamed_stage_entry_points.py:316: Phase 3 - Requires full StageIO implementation</skipped></testcase><testcase classname="tests.unit.stages.test_renamed_stage_entry_points.TestStageManifests" name="test_stage_creates_manifest[scripts.09_subtitle_gen]" time="0.000"><skipped type="pytest.skip" message="Phase 3 - Requires full StageIO implementation">/Users/rpatel/Projects/Active/cp-whisperx-app/tests/unit/stages/test_renamed_stage_entry_points.py:316: Phase 3 - Requires full StageIO implementation</skipped></testcase><testcase classname="tests.integration.test_standard_media.TestStandardMediaAvailability" name="test_sample1_exists" time="0.000" /><testcase classname="tests.integration.test_standard_media.TestStandardMediaAvailability" name="test_sample2_exists" time="0.000" /><testcase classname="tests.integration.test_standard_media.TestStandardMediaAvailability" name="test_test_media_index_exists" time="0.001" /><testcase classname="tests.integration.test_standard_media.TestSample1Transcribe" name="test_sample1_transcribe_workflow" time="0.000"><skipped type="pytest.skip" message="Phase 3 - Requires full pipeline integration">/Users/rpatel/Projects/Active/cp-whisperx-app/tests/integration/test_standard_media.py:162: Phase 3 - Requires full pipeline integration</skipped></testcase><testcase classname="tests.integration.test_standard_media.TestSample1Transcribe" name="test_sample1_handles_technical_terminology" time="0.000"><skipped type="pytest.skip" message="Phase 3 - Requires full pipeline integration">/Users/rpatel/Projects/Active/cp-whisperx-app/tests/integration/test_standard_media.py:185: Phase 3 - Requires full pipeline integration</skipped></testcase><testcase classname="tests.integration.test_standard_media.TestSample1Translate" name="test_sample1_english_to_hindi_translation" time="0.000"><skipped type="pytest.skip" message="Phase 3 - Requires full pipeline integration">/Users/rpatel/Projects/Active/cp-whisperx-app/tests/integration/test_standard_media.py:201: Phase 3 - Requires full pipeline integration</skipped></testcase><testcase classname="tests.integration.test_standard_media.TestSample2Subtitle" name="test_sample2_subtitle_workflow" time="0.000"><skipped type="pytest.skip" message="Phase 3 - Requires full pipeline integration">/Users/rpatel/Projects/Active/cp-whisperx-app/tests/integration/test_standard_media.py:227: Phase 3 - Requires full pipeline integration</skipped></testcase><testcase classname="tests.integration.test_standard_media.TestSample2Subtitle" name="test_sample2_character_name_preservation" time="0.000"><skipped type="pytest.skip" message="Phase 3 - Requires full pipeline integration">/Users/rpatel/Projects/Active/cp-whisperx-app/tests/integration/test_standard_media.py:257: Phase 3 - Requires full pipeline integration</skipped></testcase><testcase classname="tests.integration.test_standard_media.TestSample2Subtitle" name="test_sample2_hinglish_code_mixing_handled" time="0.000"><skipped type="pytest.skip" message="Phase 3 - Requires full pipeline integration">/Users/rpatel/Projects/Active/cp-whisperx-app/tests/integration/test_standard_media.py:266: Phase 3 - Requires full pipeline integration</skipped></testcase><testcase classname="tests.integration.test_standard_media.TestSample2Transcribe" name="test_sample2_transcribe_workflow" time="0.000"><skipped type="pytest.skip" message="Phase 3 - Requires full pipeline integration">/Users/rpatel/Projects/Active/cp-whisperx-app/tests/integration/test_standard_media.py:282: Phase 3 - Requires full pipeline integration</skipped></testcase><testcase classname="tests.integration.test_standard_media.TestSample2Translate" name="test_sample2_hindi_to_english_translation" time="0.000"><skipped type="pytest.skip" message="Phase 3 - Requires full pipeline integration">/Users/rpatel/Projects/Active/cp-whisperx-app/tests/integration/test_standard_media.py:304: Phase 3 - Requires full pipeline integration</skipped></testcase><testcase classname="tests.integration.test_standard_media.TestQualityBaselines" name="test_sample1_meets_asr_baseline" time="0.000"><skipped type="pytest.skip" message="Phase 3 - Requires baseline measurements">/Users/rpatel/Projects/Active/cp-whisperx-app/tests/integration/test_standard_media.py:329: Phase 3 - Requires baseline measurements</skipped></testcase><testcase classname="tests.integration.test_standard_media.TestQualityBaselines" name="test_sample1_meets_translation_baseline" time="0.000"><skipped type="pytest.skip" message="Phase 3 - Requires baseline measurements">/Users/rpatel/Projects/Active/cp-whisperx-app/tests/integration/test_standard_media.py:337: Phase 3 - Requires baseline measurements</skipped></testcase><testcase classname="tests.integration.test_standard_media.TestQualityBaselines" name="test_sample2_meets_asr_baseline" time="0.000"><skipped type="pytest.skip" message="Phase 3 - Requires baseline measurements">/Users/rpatel/Projects/Active/cp-whisperx-app/tests/integration/test_standard_media.py:345: Phase 3 - Requires baseline measurements</skipped></testcase><testcase classname="tests.integration.test_standard_media.TestQualityBaselines" name="test_sample2_meets_subtitle_baseline" time="0.000"><skipped type="pytest.skip" message="Phase 3 - Requires baseline measurements">/Users/rpatel/Projects/Active/cp-whisperx-app/tests/integration/test_standard_media.py:353: Phase 3 - Requires baseline measurements</skipped></testcase><testcase classname="tests.integration.test_standard_media.TestStandardMediaSmoke" name="test_sample1_is_video_file" time="0.001" /><testcase classname="tests.integration.test_standard_media.TestStandardMediaSmoke" name="test_sample2_is_video_file" time="0.001" /><testcase classname="tests.integration.test_workflow_integration.TestPrepareJobIntegration" name="test_prepare_job_script_exists" time="0.000" /><testcase classname="tests.integration.test_workflow_integration.TestPrepareJobIntegration" name="test_prepare_job_is_executable" time="0.000" /><testcase classname="tests.integration.test_workflow_integration.TestPrepareJobIntegration" name="test_prepare_job_creates_job_directory" time="0.000"><skipped type="pytest.skip" message="Requires actual execution - slow test">/Users/rpatel/Projects/Active/cp-whisperx-app/tests/integration/test_workflow_integration.py:118: Requires actual execution - slow test</skipped></testcase><testcase classname="tests.integration.test_workflow_integration.TestPrepareJobIntegration" name="test_prepare_job_creates_config" time="0.000"><skipped type="pytest.skip" message="Requires actual execution - slow test">/Users/rpatel/Projects/Active/cp-whisperx-app/tests/integration/test_workflow_integration.py:137: Requires actual execution - slow test</skipped></testcase><testcase classname="tests.integration.test_workflow_integration.TestWorkflowStructure" name="test_transcribe_workflow_stages_defined" time="0.000" /><testcase classname="tests.integration.test_workflow_integration.TestWorkflowStructure" name="test_translate_workflow_stages_defined" time="0.000" /><testcase classname="tests.integration.test_workflow_integration.TestWorkflowStructure" name="test_subtitle_workflow_stages_defined" time="0.000"><failure message="AssertionError: Subtitle workflow should have at least 8 stages (full pipeline)&#10;assert 5 &gt;= 8&#10; +  where 5 = len(['01_demux', '04_asr', '08_translation', '09_subtitle_gen', '10_mux'])">E   AssertionError: Subtitle workflow should have at least 8 stages (full pipeline)
    assert 5 &gt;= 8
     +  where 5 = len(['01_demux', '04_asr', '08_translation', '09_subtitle_gen', '10_mux'])</failure></testcase><testcase classname="tests.integration.test_workflow_integration.TestWorkflowStructure" name="test_workflow_dependencies_valid" time="0.000"><failure message="TypeError: object of type 'NoneType' has no len()">E   TypeError: object of type 'NoneType' has no len()</failure></testcase><testcase classname="tests.integration.test_workflow_integration.TestStageScriptsAvailable" name="test_stage_script_exists[01_demux.py]" time="0.000" /><testcase classname="tests.integration.test_workflow_integration.TestStageScriptsAvailable" name="test_stage_script_exists[02_tmdb_enrichment.py]" time="0.000" /><testcase classname="tests.integration.test_workflow_integration.TestStageScriptsAvailable" name="test_stage_script_exists[03_glossary_load.py]" time="0.000" /><testcase classname="tests.integration.test_workflow_integration.TestStageScriptsAvailable" name="test_stage_script_exists[04_source_separation.py]" time="0.000" /><testcase classname="tests.integration.test_workflow_integration.TestStageScriptsAvailable" name="test_stage_script_exists[05_pyannote_vad.py]" time="0.000" /><testcase classname="tests.integration.test_workflow_integration.TestStageScriptsAvailable" name="test_stage_script_exists[06_whisperx_asr.py]" time="0.000" /><testcase classname="tests.integration.test_workflow_integration.TestStageScriptsAvailable" name="test_stage_script_exists[07_alignment.py]" time="0.000" /><testcase classname="tests.integration.test_workflow_integration.TestStageScriptsAvailable" name="test_stage_script_exists[08_translation.py]" time="0.000" /><testcase classname="tests.integration.test_workflow_integration.TestStageScriptsAvailable" name="test_stage_script_exists[09_subtitle_generation.py]" time="0.000" /><testcase classname="tests.integration.test_workflow_integration.TestStageScriptsAvailable" name="test_stage_script_exists[10_mux.py]" time="0.000" /><testcase classname="tests.integration.test_workflow_integration.TestStageScriptsAvailable" name="test_stage_script_is_python[01_demux.py]" time="0.000" /><testcase classname="tests.integration.test_workflow_integration.TestStageScriptsAvailable" name="test_stage_script_is_python[02_tmdb_enrichment.py]" time="0.000" /><testcase classname="tests.integration.test_workflow_integration.TestStageScriptsAvailable" name="test_stage_script_is_python[03_glossary_load.py]" time="0.001" /><testcase classname="tests.integration.test_workflow_integration.TestStageScriptsAvailable" name="test_stage_script_is_python[04_source_separation.py]" time="0.000" /><testcase classname="tests.integration.test_workflow_integration.TestStageScriptsAvailable" name="test_stage_script_is_python[05_pyannote_vad.py]" time="0.000" /><testcase classname="tests.integration.test_workflow_integration.TestStageScriptsAvailable" name="test_stage_script_is_python[06_whisperx_asr.py]" time="0.000" /><testcase classname="tests.integration.test_workflow_integration.TestStageScriptsAvailable" name="test_stage_script_is_python[07_alignment.py]" time="0.000" /><testcase classname="tests.integration.test_workflow_integration.TestStageScriptsAvailable" name="test_stage_script_is_python[08_translation.py]" time="0.000"><failure message="AssertionError: Stage 08_translation.py should have Python shebang&#10;assert (False)&#10; +  where False = &lt;built-in method startswith of str object at 0x117257830&gt;('#!')&#10; +    where &lt;built-in method startswith of str object at 0x117257830&gt; = '&quot;&quot;&quot;\n'.startswith">E   AssertionError: Stage 08_translation.py should have Python shebang
    assert (False)
     +  where False = &lt;built-in method startswith of str object at 0x117257830&gt;('#!')
     +    where &lt;built-in method startswith of str object at 0x117257830&gt; = '"""\n'.startswith</failure></testcase><testcase classname="tests.integration.test_workflow_integration.TestStageScriptsAvailable" name="test_stage_script_is_python[09_subtitle_generation.py]" time="0.001" /><testcase classname="tests.integration.test_workflow_integration.TestStageScriptsAvailable" name="test_stage_script_is_python[10_mux.py]" time="0.000" /><testcase classname="tests.integration.test_workflow_integration.TestJobDirectoryStructure" name="test_output_directory_exists" time="0.000" /><testcase classname="tests.integration.test_workflow_integration.TestJobDirectoryStructure" name="test_logs_directory_exists" time="0.000" /><testcase classname="tests.integration.test_workflow_integration.TestJobDirectoryStructure" name="test_expected_stage_directories_structure" time="0.003" /><testcase classname="tests.integration.test_workflow_integration.TestPipelineRunner" name="test_run_pipeline_script_exists" time="0.000" /><testcase classname="tests.integration.test_workflow_integration.TestPipelineRunner" name="test_run_pipeline_py_exists" time="0.000" /><testcase classname="tests.integration.test_workflow_integration.TestPipelineRunner" name="test_run_pipeline_has_main_function" time="0.004"><skipped type="pytest.skip" message="Could not verify run-pipeline.py: run-pipeline.py should have main execution guard&#10;assert &quot;if __name__ == '__main__'&quot; in '#!/usr/bin/env python3\n&quot;&quot;&quot;\nIndicTrans2 Pipeline Orchestrator\n\nSimplified pipeline execution for IndicTrans2 workflows:\n- Transcribe workflow: demux \u2192 asr \u2192 alignment\n- Translate workflow: load_transcript \u2192 indictrans2_translation \u2192 subtitle_generation\n\nReuses existing infrastructure:\n- shared/logger.py for logging\n- shared/manifest.py for tracking\n- Existing stage implementations where possible\n&quot;&quot;&quot;\n\n# Standard library\nimport sys\nimport os\nimport json\nimport argparse\nimport subprocess\nimport traceback\nimport logging\nfrom pathlib import Path\nfrom datetime import datetime\nfrom typing import Optional, Dict, List, Any\n\n# Add paths for imports\nSCRIPT_DIR = Path(__file__).parent\nPROJECT_ROOT = SCRIPT_DIR.parent\nsys.path.insert(0, str(PROJECT_ROOT))\n\nfrom shared.logger import PipelineLogger, get_logger\nfrom shared.environment_manager import EnvironmentManager\nfrom scripts.config_loader import Config\nfrom shared.stage_order import get_stage_dir\nfrom shared.stage_dependencies import (\n    validate_stage_dependencies,\n    get_workflow_stages,\n    get_execution_order\n)\n\n# Initialize logger\nlogger = get_logger(__name__)\n\n\ndef format_timestamp_srt(seconds: float) -&gt; str:\n    &quot;&quot;&quot;Format seconds as SRT timestamp (HH:MM:SS,mmm)&quot;&quot;&quot;\n    hours = int(seconds // 3600)\n    minutes = int((seconds % 3600) // 60)\n    secs = int(seconds % 60)\n    millis = int((seconds % 1) * 1000)\n    return f&quot;{hours:02d}:{minutes:02d}:{secs:02d},{millis:03d}&quot;\n\n\ndef normalize_segments_data(data: Dict[str, Any]) -&gt; Any:\n    &quot;&quot;&quot;\n    Normalize segments data to consistent dict format.\n    Handles both list [...] and dict {&quot;segments&quot;: [...]} formats.\n    \n    Args:\n        data: Either a list of segments or a dict containing segments\n        \n    Returns:\n        Tuple of (data_dict, segments_list)\n    &quot;&quot;&quot;\n    if isinstance(data, list):\n        segments = data\n        data = {&quot;segments&quot;: segments}\n    elif isinstance(data, dict):\n        segments = data.get(&quot;segments&quot;, [])\n    else:\n        segments = []\n    \n    return data, segments\n\n\ndef generate_srt_from_segments(segments: List[Dict], output_path: Path) -&gt; bool:\n    &quot;&quot;&quot;Generate SRT subtitle file from segments&quot;&quot;&quot;\n    try:\n        with open(output_path, \'w\', encoding=\'utf-8\') as f:\n            for i, segment in enumerate(segments, 1):\n                # Segment number\n                f.write(f&quot;{i}\\n&quot;)\n                \n                # Timestamps\n                start = format_timestamp_srt(segment.get(\'start\', 0))\n                end = format_timestamp_srt(segment.get(\'end\', 0))\n                f.write(f&quot;{start} --&gt; {end}\\n&quot;)\n                \n                # Text\n                text = segment.get(\'text\', \'\').strip()\n                f.write(f&quot;{text}\\n&quot;)\n                \n                # Blank line between segments\n                f.write(&quot;\\n&quot;)\n        \n        return True\n    except Exception as e:\n        return False\n\n\n\nclass IndicTrans2Pipeline:\n    &quot;&quot;&quot;Pipeline orchestrator for IndicTrans2 workflows&quot;&quot;&quot;\n    \n    def __init__(self, job_dir: Path, resume: bool = False):\n        &quot;&quot;&quot;  Init  .&quot;&quot;&quot;\n        self.job_dir = job_dir\n        self.resume = resume\n        \n        # Set scripts directory\n        self.scripts_dir = PROJECT_ROOT / &quot;scripts&quot;\n        \n        # Load main configuration for fallback defaults\n        self.main_config = Config(PROJECT_ROOT)\n        \n        # Load job configuration\n        self.job_config = self._load_config(&quot;job.json&quot;)\n        self.manifest = self._load_config(&quot;manifest.json&quot;)\n        \n        # Initialize environment manager\n        self.env_manager = EnvironmentManager(PROJECT_ROOT)\n        \n        # Load job-specific environment configuration\n        self.env_config = self._load_env_config()\n        \n        # Get debug mode from job config\n        self.debug = self.job_config.get(&quot;debug&quot;, False)\n        log_level = &quot;DEBUG&quot; if self.debug else &quot;INFO&quot;\n        \n        # Setup logging - DUAL logging architecture:\n        # 1. Main pipeline log: High-level orchestration\n        # 2. Stage logs: Detailed logs in each stage subdirectory\n        log_dir = job_dir / &quot;logs&quot;\n        log_dir.mkdir(exist_ok=True)\n        \n        # Create main pipeline log file (99_pipeline_*.log for clarity)\n        timestamp = datetime.now().strftime(&quot;%Y%m%d_%H%M%S&quot;)\n        log_file = log_dir / f&quot;99_pipeline_{timestamp}.log&quot;\n        \n        self.logger = PipelineLogger(\n            module_name=&quot;pipeline&quot;,\n            log_file=log_file,\n            log_level=log_level\n        )\n        \n        self.logger.info(&quot;=&quot; * 80)\n        self.logger.info(&quot;PIPELINE LOGGING ARCHITECTURE&quot;)\n        self.logger.info(&quot;=&quot; * 80)\n        self.logger.info(f&quot;\U0001f4cb Main pipeline log: {log_file.relative_to(job_dir)}&quot;)\n        self.logger.info(f&quot;\U0001f4cb Stage logs: Each stage writes to its own subdirectory&quot;)\n        self.logger.info(f&quot;\U0001f4cb Stage manifests: Track inputs/outputs/intermediate files&quot;)\n        self.logger.info(&quot;&quot;)\n        \n        if self.debug:\n            self.logger.info(&quot;\U0001f41b DEBUG MODE ENABLED - Verbose logging active&quot;)\n        \n        self.workflow = self.job_config[&quot;workflow&quot;]\n        \n        # Initialize glossary manager (will be loaded in stage)\n        self.glossary_manager = None\n        \n        # Log cache configuration\n        cache_config = self.env_manager.hardware_cache.get(&quot;cache&quot;, {})\n        if cache_config:\n            self.logger.info(&quot;\U0001f4e6 Model cache configuration:&quot;)\n            if &quot;hf_home&quot; in cache_config:\n                hf_cache = PROJECT_ROOT / cache_config[&quot;hf_home&quot;]\n                if hf_cache.exists():\n                    # Count cached models\n                    hub_dir = hf_cache / &quot;hub&quot;\n                    if hub_dir.exists():\n                        model_count = len([d for d in hub_dir.iterdir() if d.is_dir() and d.name.startswith(&quot;models--&quot;)])\n                        self.logger.info(f&quot;  HuggingFace cache: {cache_config[\'hf_home\']} ({model_count} models cached)&quot;)\n                    else:\n                        self.logger.info(f&quot;  HuggingFace cache: {cache_config[\'hf_home\']} (empty)&quot;)\n                else:\n                    self.logger.warning(f&quot;  HuggingFace cache not found: {cache_config[\'hf_home\']}&quot;)\n            if &quot;torch_home&quot; in cache_config:\n                self.logger.info(f&quot;  PyTorch cache: {cache_config[\'torch_home\']}&quot;)\n            if &quot;mlx_home&quot; in cache_config:\n                self.logger.info(f&quot;  MLX cache: {cache_config[\'mlx_home\']}&quot;)\n        \n        # Log environment information\n        envs = self.job_config.get(&quot;environments&quot;, {})\n        if envs:\n            self.logger.info(f&quot;Multi-environment mode: {len(envs)} environment(s) configured&quot;)\n            for env_name, env_path in envs.items():\n                installed = &quot;\u2713&quot; if self.env_manager.is_environment_installed(env_name) else &quot;\u2717&quot;\n                self.logger.info(f&quot;  {installed} {env_name}: {env_path}&quot;)\n    \n    def _stage_path(self, stage_name: str) -&gt; Path:\n        &quot;&quot;&quot;\n        Get the path to a stage directory using centralized stage ordering.\n        \n        Args:\n            stage_name: Name of the stage\n            \n        Returns:\n            Path to stage directory\n        &quot;&quot;&quot;\n        return self.job_dir / get_stage_dir(stage_name)\n    \n    def _load_env_config(self) -&gt; Dict[str, str]:\n        &quot;&quot;&quot;Load job-specific .env file created by prepare-job&quot;&quot;&quot;\n        job_id = self.job_config[&quot;job_id&quot;]\n        env_file = self.job_dir / f&quot;.{job_id}.env&quot;\n        \n        if not env_file.exists():\n            self.logger.warning(f&quot;Job .env file not found: {env_file}&quot;)\n            return {}\n        \n        config = {}\n        with open(env_file) as f:\n            for line in f:\n                line = line.strip()\n                if line and not line.startswith(\'#\') and \'=\' in line:\n                    key, value = line.split(\'=\', 1)\n                    config[key] = value\n        \n        return config\n    \n    def _is_indic_language(self, lang_code: str) -&gt; bool:\n        &quot;&quot;&quot;Check if a language code is an Indic language supported by IndicTrans2&quot;&quot;&quot;\n        indic_languages = {\n            &quot;hi&quot;, &quot;as&quot;, &quot;bn&quot;, &quot;gu&quot;, &quot;kn&quot;, &quot;ml&quot;, &quot;mr&quot;, &quot;or&quot;, &quot;pa&quot;, &quot;ta&quot;, &quot;te&quot;, &quot;ur&quot;,\n            &quot;ne&quot;, &quot;sd&quot;, &quot;si&quot;, &quot;sa&quot;, &quot;ks&quot;, &quot;doi&quot;, &quot;mni&quot;, &quot;kok&quot;, &quot;mai&quot;, &quot;sat&quot;\n        }\n        return lang_code in indic_languages\n        \n    def _load_config(self, filename: str) -&gt; Dict:\n        &quot;&quot;&quot;Load JSON configuration file&quot;&quot;&quot;\n        config_file = self.job_dir / filename\n        if not config_file.exists():\n            raise FileNotFoundError(f&quot;Configuration not found: {config_file}&quot;)\n        \n        with open(config_file) as f:\n            return json.load(f)\n    \n    def _save_manifest(self) -&gt; None:\n        &quot;&quot;&quot;Save manifest to file&quot;&quot;&quot;\n        manifest_file = self.job_dir / &quot;manifest.json&quot;\n        self.manifest[&quot;updated_at&quot;] = datetime.now().isoformat()\n        \n        with open(manifest_file, \'w\') as f:\n            json.dump(self.manifest, f, indent=2)\n    \n    def _update_stage_status(self, stage_name: str, status: str, \n                            duration: Optional[float] = None):\n        &quot;&quot;&quot;Update stage status in manifest&quot;&quot;&quot;\n        for stage in self.manifest[&quot;stages&quot;]:\n            if stage[&quot;name&quot;] == stage_name:\n                stage[&quot;status&quot;] = status\n                if status == &quot;running&quot;:\n                    stage[&quot;start_time&quot;] = datetime.now().isoformat()\n                elif status in [&quot;completed&quot;, &quot;failed&quot;]:\n                    stage[&quot;end_time&quot;] = datetime.now().isoformat()\n                    if duration:\n                        stage[&quot;duration_seconds&quot;] = duration\n                break\n        \n        self._save_manifest()\n    \n    def _get_stage_environment(self, stage_name: str) -&gt; Optional[str]:\n        &quot;&quot;&quot;Get the required environment for a stage&quot;&quot;&quot;\n        stage_envs = self.job_config.get(&quot;stage_environments&quot;, {})\n        \n        # Try exact match first\n        if stage_name in stage_envs:\n            return stage_envs[stage_name]\n        \n        # Handle dynamic translation stage names (e.g., indictrans2_translation_en)\n        if stage_name.startswith(&quot;indictrans2_translation_&quot;):\n            # Look for generic &quot;translation&quot; mapping\n            return stage_envs.get(&quot;translation&quot;) or &quot;indictrans2&quot;\n        \n        # Handle dynamic NLLB translation stage names\n        if stage_name.startswith(&quot;nllb_translation_&quot;):\n            return stage_envs.get(&quot;nllb_translation&quot;) or &quot;nllb&quot;\n        \n        # Handle dynamic subtitle generation stage names\n        if stage_name.startswith(&quot;subtitle_generation_&quot;):\n            return stage_envs.get(&quot;subtitle_gen&quot;) or stage_envs.get(&quot;subtitle_generation&quot;) or &quot;common&quot;\n        \n        # Default environment mappings for stages without explicit config\n        default_envs = {\n            &quot;source_separation&quot;: &quot;common&quot;,\n            &quot;pyannote_vad&quot;: &quot;pyannote&quot;\n        }\n        \n        if stage_name in default_envs:\n            return default_envs[stage_name]\n        \n        return None\n    \n    def _get_target_language(self) -&gt; Optional[str]:\n        &quot;&quot;&quot;\n        Get target language from job config, handling both singular and plural forms.\n        \n        Returns:\n            Target language code, or None if not set\n        &quot;&quot;&quot;\n        # Try singular form first (legacy)\n        target_lang = self.job_config.get(&quot;target_language&quot;)\n        if target_lang:\n            return target_lang\n        \n        # Try plural form (new format)\n        target_langs = self.job_config.get(&quot;target_languages&quot;, [])\n        if target_langs:\n            return target_langs[0]\n        \n        return None\n    \n    def _run_in_environment(self, stage_name: str, command: List[str], **kwargs) -&gt; subprocess.CompletedProcess:\n        &quot;&quot;&quot;\n        Run a command in the appropriate environment for a stage\n        \n        Args:\n            stage_name: Name of the stage\n            command: Command to run\n            **kwargs: Additional arguments for subprocess.run\n            \n        Returns:\n            CompletedProcess instance\n        &quot;&quot;&quot;\n        env_name = self._get_stage_environment(stage_name)\n        \n        if env_name:\n            self.logger.info(f&quot;Running stage \'{stage_name}\' in environment \'{env_name}\'&quot;)\n            python_exe = self.env_manager.get_python_executable(env_name)\n            \n            # Start with provided env if any, otherwise use os.environ\n            env = kwargs.get(\'env\', os.environ).copy()\n            \n            # Set up environment variables for the virtual environment\n            env[&quot;VIRTUAL_ENV&quot;] = str(self.env_manager.get_environment_path(env_name))\n            env_bin = self.env_manager.get_environment_path(env_name) / &quot;bin&quot;\n            env[&quot;PATH&quot;] = f&quot;{env_bin}:{env[\'PATH\']}&quot;\n            env[&quot;DEBUG_MODE&quot;] = \'true\' if self.debug else \'false\'\n            env[&quot;LOG_LEVEL&quot;] = \'DEBUG\' if self.debug else \'INFO\'\n            \n            # Replace python in command with environment-specific python\n            if command[0] == &quot;python&quot; or command[0] == &quot;python3&quot;:\n                command[0] = str(python_exe)\n            \n            kwargs[\'env\'] = env\n        else:\n            self.logger.warning(f&quot;No environment specified for stage \'{stage_name}\', using current environment&quot;)\n            \n            # Set debug environment variables\n            if \'env\' not in kwargs:\n                kwargs[\'env\'] = os.environ.copy()\n            kwargs[\'env\'][\'DEBUG_MODE\'] = \'true\' if self.debug else \'false\'\n            kwargs[\'env\'][\'LOG_LEVEL\'] = \'DEBUG\' if self.debug else \'INFO\'\n        \n        return subprocess.run(command, **kwargs)\n    \n    def _check_indictrans2_available(self) -&gt; bool:\n        &quot;&quot;&quot;Check if IndicTrans2 environment is available&quot;&quot;&quot;\n        try:\n            # Check if indictrans2 environment exists and is valid\n            return self.env_manager.is_environment_installed(&quot;indictrans2&quot;)\n        except Exception as e:\n            self.logger.debug(f&quot;IndicTrans2 check failed: {e}&quot;)\n            return False\n    \n    def run_transcribe_workflow(self) -&gt; bool:\n        &quot;&quot;&quot;\n        Execute transcribe workflow stages:\n        1. Demux - Extract audio\n        1.5. Source Separation - Extract vocals (if enabled)\n        2. PyAnnote VAD - Detect speech segments (highest quality)\n        3. ASR - Transcribe using WhisperX\n        4. Alignment - Word-level timestamps\n        5. Export - Generate plain text transcript\n        &quot;&quot;&quot;\n        self.logger.info(&quot;=&quot; * 80)\n        self.logger.info(&quot;TRANSCRIBE WORKFLOW&quot;)\n        self.logger.info(&quot;=&quot; * 80)\n        \n        # Build stages list\n        stages = [(&quot;demux&quot;, self._stage_demux)]\n        \n        # Add TMDB enrichment if enabled (BEFORE ASR for metadata context)\n        if self.job_config.get(&quot;tmdb_enrichment&quot;, {}).get(&quot;enabled&quot;, False):\n            stages.append((&quot;tmdb&quot;, self._stage_tmdb_enrichment))\n            # Add glossary load after TMDB (to use enrichment data)\n            stages.append((&quot;glossary_load&quot;, self._stage_glossary_load))\n        \n        # Add source separation if enabled\n        sep_config = self.job_config.get(&quot;source_separation&quot;, {})\n        if sep_config.get(&quot;enabled&quot;, False):\n            stages.append((&quot;source_separation&quot;, self._stage_source_separation))\n        \n        # Add core ASR stages\n        stages.extend([\n            (&quot;pyannote_vad&quot;, self._stage_pyannote_vad),\n            (&quot;asr&quot;, self._stage_asr),\n            (&quot;hallucination_removal&quot;, self._stage_hallucination_removal),\n            (&quot;alignment&quot;, self._stage_alignment),\n        ])\n        \n        # Add lyrics detection AFTER ASR (optional)\n        lyrics_enabled = self.env_config.get(&quot;LYRICS_DETECTION_ENABLED&quot;, &quot;true&quot;).lower() == &quot;true&quot;\n        if lyrics_enabled:\n            stages.append((&quot;lyrics_detection&quot;, self._stage_lyrics_detection))\n        \n        # Final export stage\n        stages.append((&quot;export_transcript&quot;, self._stage_export_transcript))\n        \n        return self._execute_stages(stages)\n    \n    def run_translate_workflow(self) -&gt; bool:\n        &quot;&quot;&quot;\n        Execute translate workflow stages:\n        Auto-executes transcribe workflow if transcript doesn\'t exist\n        1. Demux - Extract audio (if needed)\n        2. ASR - Transcribe (if needed)\n        3. Alignment - Word timestamps (if needed)\n        4. Load Transcript - Load segments.json\n        5. IndicTrans2 Translation - Translate text\n        6. Subtitle Generation - Create SRT in target language\n        &quot;&quot;&quot;\n        self.logger.info(&quot;=&quot; * 80)\n        self.logger.info(&quot;TRANSLATE WORKFLOW&quot;)\n        self.logger.info(&quot;=&quot; * 80)\n        \n        # Get target language\n        target_lang = self._get_target_language()\n        \n        # Check if transcript exists, if not run transcribe stages first\n        segments_file = self.job_dir / &quot;transcripts&quot; / &quot;segments.json&quot;\n        \n        if not segments_file.exists():\n            self.logger.info(&quot;\U0001f4dd Transcript not found - auto-executing transcribe workflow first&quot;)\n            self.logger.info(&quot;&quot;)\n            \n            # Run transcribe stages\n            transcribe_stages = [(&quot;demux&quot;, self._stage_demux)]\n            \n            # Add TMDB enrichment if enabled (BEFORE ASR for metadata context)\n            if self.job_config.get(&quot;tmdb_enrichment&quot;, {}).get(&quot;enabled&quot;, False):\n                transcribe_stages.append((&quot;tmdb&quot;, self._stage_tmdb_enrichment))\n                # Add glossary load after TMDB (to use enrichment data)\n                transcribe_stages.append((&quot;glossary_load&quot;, self._stage_glossary_load))\n            \n            # Add source separation if enabled\n            sep_config = self.job_config.get(&quot;source_separation&quot;, {})\n            if sep_config.get(&quot;enabled&quot;, False):\n                transcribe_stages.append((&quot;source_separation&quot;, self._stage_source_separation))\n            \n            # Add core ASR stages\n            transcribe_stages.extend([\n                (&quot;pyannote_vad&quot;, self._stage_pyannote_vad),\n                (&quot;asr&quot;, self._stage_asr),\n                (&quot;hallucination_removal&quot;, self._stage_hallucination_removal),\n                (&quot;alignment&quot;, self._stage_alignment),\n            ])\n            \n            # Add lyrics detection AFTER ASR (optional)\n            lyrics_enabled = self.env_config.get(&quot;LYRICS_DETECTION_ENABLED&quot;, &quot;true&quot;).lower() == &quot;true&quot;\n            if lyrics_enabled:\n                transcribe_stages.append((&quot;lyrics_detection&quot;, self._stage_lyrics_detection))\n            \n            # Final export stage\n            transcribe_stages.append((&quot;export_transcript&quot;, self._stage_export_transcript))\n            \n            if not self._execute_stages(transcribe_stages):\n                self.logger.error(&quot;Transcribe workflow failed - cannot proceed with translation&quot;)\n                return False\n            \n            self.logger.info(&quot;&quot;)\n            self.logger.info(&quot;\u2705 Transcribe workflow completed successfully&quot;)\n            self.logger.info(&quot;=&quot; * 80)\n            self.logger.info(&quot;CONTINUING WITH TRANSLATION&quot;)\n            self.logger.info(&quot;=&quot; * 80)\n        else:\n            self.logger.info(&quot;\u2713 Transcript found - skipping transcribe stages&quot;)\n        \n        # Route to appropriate translator based on target language\n        translate_stages = [(&quot;load_transcript&quot;, self._stage_load_transcript)]\n        \n        # Check if hybrid translation is enabled\n        use_hybrid = self.env_config.get(&quot;USE_HYBRID_TRANSLATION&quot;, &quot;true&quot;).lower() == &quot;true&quot;\n        \n        if use_hybrid:\n            # Use hybrid translation (IndicTrans2 + LLM for songs)\n            self.logger.info(f&quot;Using hybrid translation for {target_lang}&quot;)\n            translate_stages.append((&quot;hybrid_translation&quot;, self._stage_hybrid_translation))\n        elif self._is_indic_language(target_lang):\n            # Use IndicTrans2 for Indic languages\n            self.logger.info(f&quot;Using IndicTrans2 for Indic language: {target_lang}&quot;)\n            translate_stages.append((&quot;indictrans2_translation&quot;, self._stage_indictrans2_translation))\n        else:\n            # Use NLLB for non-Indic languages\n            self.logger.info(f&quot;Using NLLB for non-Indic language: {target_lang}&quot;)\n            translate_stages.append((&quot;nllb_translation&quot;, self._stage_nllb_translation))\n        \n        translate_stages.append((&quot;subtitle_generation&quot;, self._stage_subtitle_generation))\n        \n        return self._execute_stages(translate_stages)\n    \n    def run_subtitle_workflow(self) -&gt; bool:\n        &quot;&quot;&quot;\n        Execute subtitle workflow stages:\n        Auto-executes transcribe + translate workflows if needed\n        Generates subtitles in source and multiple target languages (up to 5)\n        1. Demux - Extract audio (if needed)\n        2. ASR - Transcribe (if needed)\n        3. Alignment - Word timestamps (if needed)\n        4. Load Transcript - Load segments.json\n        5. IndicTrans2 Translation - Translate text (for each target language)\n        6. Subtitle Generation (Target) - Create SRT for each target language\n        7. Subtitle Generation (Source) - Create SRT in source language\n        8. Mux - Embed all subtitle tracks in video\n        &quot;&quot;&quot;\n        self.logger.info(&quot;=&quot; * 80)\n        self.logger.info(&quot;SUBTITLE WORKFLOW&quot;)\n        self.logger.info(&quot;=&quot; * 80)\n        \n        # Get target languages from config\n        target_languages = self.job_config.get(&quot;target_languages&quot;, [])\n        if not target_languages:\n            self.logger.error(&quot;No target languages configured!&quot;)\n            return False\n        \n        self.logger.info(f&quot;Target languages: {\', \'.join(target_languages)}&quot;)\n        \n        # Check IndicTrans2 availability\n        if not self._check_indictrans2_available():\n            self.logger.error(&quot;IndicTrans2 not available!&quot;)\n            self.logger.error(&quot;Please install: ./install-indictrans2.sh&quot;)\n            return False\n        \n        # Check if transcript exists, if not run transcribe stages first\n        segments_file = self.job_dir / &quot;transcripts&quot; / &quot;segments.json&quot;\n        \n        if not segments_file.exists():\n            self.logger.info(&quot;\U0001f4dd Transcript not found - auto-executing transcribe workflow first&quot;)\n            self.logger.info(&quot;&quot;)\n            \n            # Run transcribe stages\n            transcribe_stages = [(&quot;demux&quot;, self._stage_demux)]\n            \n            # Add TMDB enrichment if enabled (BEFORE ASR for metadata context)\n            if self.job_config.get(&quot;tmdb_enrichment&quot;, {}).get(&quot;enabled&quot;, False):\n                transcribe_stages.append((&quot;tmdb&quot;, self._stage_tmdb_enrichment))\n                # Add glossary load after TMDB (to use enrichment data)\n                transcribe_stages.append((&quot;glossary_load&quot;, self._stage_glossary_load))\n            \n            # Add source separation if enabled\n            sep_config = self.job_config.get(&quot;source_separation&quot;, {})\n            if sep_config.get(&quot;enabled&quot;, False):\n                transcribe_stages.append((&quot;source_separation&quot;, self._stage_source_separation))\n            \n            # Add core ASR stages\n            transcribe_stages.extend([\n                (&quot;pyannote_vad&quot;, self._stage_pyannote_vad),\n                (&quot;asr&quot;, self._stage_asr),\n                (&quot;hallucination_removal&quot;, self._stage_hallucination_removal),\n                (&quot;alignment&quot;, self._stage_alignment),\n            ])\n            \n            # Add lyrics detection AFTER ASR (optional)\n            lyrics_enabled = self.env_config.get(&quot;LYRICS_DETECTION_ENABLED&quot;, &quot;true&quot;).lower() == &quot;true&quot;\n            if lyrics_enabled:\n                transcribe_stages.append((&quot;lyrics_detection&quot;, self._stage_lyrics_detection))\n            \n            # Final stage\n            transcribe_stages.append((&quot;export_transcript&quot;, self._stage_export_transcript))\n            \n            if not self._execute_stages(transcribe_stages):\n                self.logger.error(&quot;Transcribe workflow failed - cannot proceed with subtitle generation&quot;)\n                return False\n            \n            self.logger.info(&quot;&quot;)\n            self.logger.info(&quot;\u2705 Transcribe workflow completed successfully&quot;)\n            self.logger.info(&quot;=&quot; * 80)\n            self.logger.info(&quot;CONTINUING WITH TRANSLATION AND SUBTITLE GENERATION&quot;)\n            self.logger.info(&quot;=&quot; * 80)\n        else:\n            self.logger.info(&quot;\u2713 Transcript found - skipping transcribe stages&quot;)\n        \n        # Build subtitle stages dynamically\n        subtitle_stages = [(&quot;load_transcript&quot;, self._stage_load_transcript)]\n        \n        # Check if hybrid translation is enabled\n        use_hybrid = self.env_config.get(&quot;USE_HYBRID_TRANSLATION&quot;, &quot;true&quot;).lower() == &quot;true&quot;\n        \n        # Add translation and subtitle generation for each target language\n        for target_lang in target_languages:\n            # Route to appropriate translator based on language and hybrid setting\n            if use_hybrid:\n                # Use hybrid translation (IndicTrans2 + LLM for songs)\n                subtitle_stages.append((\n                    f&quot;hybrid_translation_{target_lang}&quot;,\n                    lambda tl=target_lang: self._stage_hybrid_translation_multi(tl)\n                ))\n            elif self._is_indic_language(target_lang):\n                # Use IndicTrans2 for Indic languages\n                subtitle_stages.append((\n                    f&quot;indictrans2_translation_{target_lang}&quot;,\n                    lambda tl=target_lang: self._stage_indictrans2_translation_multi(tl)\n                ))\n            else:\n                # Use NLLB for non-Indic languages\n                subtitle_stages.append((\n                    f&quot;nllb_translation_{target_lang}&quot;,\n                    lambda tl=target_lang: self._stage_nllb_translation_multi(tl)\n                ))\n            \n            subtitle_stages.append((\n                f&quot;subtitle_generation_{target_lang}&quot;,\n                lambda tl=target_lang: self._stage_subtitle_generation_target_multi(tl)\n            ))\n        \n        # Add source subtitle generation\n        subtitle_stages.append((&quot;subtitle_generation_source&quot;, self._stage_subtitle_generation_source))\n        \n        # Add Hinglish detection for source subtitles if source language is Hindi/Indic\n        source_lang = self.job_config.get(&quot;source_language&quot;, &quot;&quot;)\n        hinglish_detection = self.job_config.get(&quot;hinglish_detection&quot;, {})\n        if hinglish_detection.get(&quot;enabled&quot;, True) and source_lang in [&quot;hi&quot;, &quot;hin&quot;, &quot;hin_Deva&quot;]:\n            subtitle_stages.append((&quot;hinglish_detection&quot;, self._stage_hinglish_detection))\n        \n        # Add mux stage\n        subtitle_stages.append((&quot;mux&quot;, self._stage_mux))\n        \n        return self._execute_stages(subtitle_stages)\n    \n    def _execute_stages(self, stages: List[tuple]) -&gt; bool:\n        &quot;&quot;&quot;Execute list of stages&quot;&quot;&quot;\n        for stage_name, stage_func in stages:\n            # Check if resuming and stage already completed\n            if self.resume:\n                stage_status = self._get_stage_status(stage_name)\n                if stage_status == &quot;completed&quot;:\n                    self.logger.info(f&quot;\u23ed  Stage {stage_name}: SKIPPED (already completed)&quot;)\n                    continue\n            \n            # Execute stage\n            self.logger.info(f&quot;\u25b6\ufe0f  Stage {stage_name}: STARTING&quot;)\n            self._update_stage_status(stage_name, &quot;running&quot;)\n            \n            start_time = datetime.now()\n            \n            try:\n                success = stage_func()\n                \n                duration = (datetime.now() - start_time).total_seconds()\n                \n                if success:\n                    self.logger.info(f&quot;\u2705 Stage {stage_name}: COMPLETED ({duration:.1f}s)&quot;)\n                    self._update_stage_status(stage_name, &quot;completed&quot;, duration)\n                else:\n                    self.logger.error(f&quot;\u274c Stage {stage_name}: FAILED&quot;)\n                    self._update_stage_status(stage_name, &quot;failed&quot;, duration)\n                    return False\n                    \n            except Exception as e:\n                duration = (datetime.now() - start_time).total_seconds()\n                self.logger.error(f&quot;\u274c Stage {stage_name}: EXCEPTION: {e}&quot;, exc_info=True)\n                if self.debug:\n                    self.logger.error(f&quot;Traceback: {traceback.format_exc()}&quot;, exc_info=True)\n                self._update_stage_status(stage_name, &quot;failed&quot;, duration)\n                return False\n        \n        return True\n    \n    def _get_stage_status(self, stage_name: str) -&gt; Optional[str]:\n        &quot;&quot;&quot;Get current status of a stage&quot;&quot;&quot;\n        for stage in self.manifest[&quot;stages&quot;]:\n            if stage[&quot;name&quot;] == stage_name:\n                return stage[&quot;status&quot;]\n        return None\n    \n    # ========================================================================\n    # Stage Implementations\n    # ========================================================================\n    \n    def _stage_demux(self) -&gt; bool:\n        &quot;&quot;&quot;Stage 1: Extract audio from video (full or clipped)&quot;&quot;&quot;\n        \n        # Initialize stage I/O and manifest\n        from shared.stage_utils import StageIO\n        stage_io = StageIO(&quot;demux&quot;, self.job_dir, enable_manifest=True)\n        stage_logger = stage_io.get_stage_logger(&quot;DEBUG&quot; if self.debug else &quot;INFO&quot;)\n        \n        # Input/output setup\n        input_media = Path(self.job_config[&quot;input_media&quot;])\n        stage_dir = stage_io.stage_dir\n        audio_output = stage_io.get_output_path(&quot;audio.wav&quot;)\n        \n        # Track input in manifest\n        stage_io.track_input(input_media, &quot;video&quot;, format=input_media.suffix[1:])\n        \n        # Log input/output (to both stage log and pipeline log)\n        self.logger.info(f&quot;\U0001f4e5 Input: {input_media.relative_to(PROJECT_ROOT) if input_media.is_relative_to(PROJECT_ROOT) else input_media}&quot;)\n        self.logger.info(f&quot;\U0001f4e4 Output: {audio_output.relative_to(self.job_dir)}&quot;)\n        stage_logger.info(f&quot;Input media: {input_media}&quot;)\n        stage_logger.info(f&quot;Output directory: {stage_dir}&quot;)\n        \n        # Get media processing configuration\n        media_config = self.job_config.get(&quot;media_processing&quot;, {})\n        processing_mode = media_config.get(&quot;mode&quot;, &quot;full&quot;)\n        start_time = media_config.get(&quot;start_time&quot;, &quot;&quot;)\n        end_time = media_config.get(&quot;end_time&quot;, &quot;&quot;)\n        \n        # Add config to manifest\n        stage_io.set_config({\n            &quot;processing_mode&quot;: processing_mode,\n            &quot;start_time&quot;: start_time,\n            &quot;end_time&quot;: end_time,\n            &quot;sample_rate&quot;: &quot;16000&quot;,\n            &quot;channels&quot;: &quot;1&quot;\n        })\n        \n        if processing_mode == &quot;clip&quot;:\n            self.logger.info(f&quot;Extracting audio clip (from {start_time} to {end_time})...&quot;)\n            stage_logger.info(f&quot;Clip mode: {start_time} to {end_time}&quot;)\n        else:\n            self.logger.info(&quot;Extracting audio from full media...&quot;)\n            stage_logger.info(&quot;Full media extraction&quot;)\n        \n        # Build ffmpeg command\n        cmd = [&quot;ffmpeg&quot;, &quot;-y&quot;]\n        \n        # Add log level based on debug mode\n        if not self.debug:\n            cmd.extend([&quot;-loglevel&quot;, &quot;error&quot;])\n        \n        # Add start time if clipping\n        if processing_mode == &quot;clip&quot; and start_time:\n            cmd.extend([&quot;-ss&quot;, start_time])\n        \n        # Add input file\n        cmd.extend([&quot;-i&quot;, str(input_media)])\n        \n        # Add end time if clipping\n        if processing_mode == &quot;clip&quot; and end_time:\n            cmd.extend([&quot;-to&quot;, end_time])\n        \n        # Add output options\n        cmd.extend([\n            &quot;-vn&quot;,  # No video\n            &quot;-acodec&quot;, &quot;pcm_s16le&quot;,\n            &quot;-ar&quot;, &quot;16000&quot;,  # 16kHz for Whisper\n            &quot;-ac&quot;, &quot;1&quot;,  # Mono\n            str(audio_output)\n        ])\n        \n        try:\n            # Set up environment with debug flag\n            env = os.environ.copy()\n            env[\'DEBUG_MODE\'] = \'true\' if self.debug else \'false\'\n            env[\'LOG_LEVEL\'] = \'DEBUG\' if self.debug else \'INFO\'\n            \n            result = subprocess.run(\n                cmd,\n                capture_output=True,\n                text=True,\n                check=True,\n                env=env\n            )\n            \n            if self.debug and result.stderr:\n                self.logger.debug(f&quot;FFmpeg output: {result.stderr}&quot;)\n                stage_logger.debug(f&quot;FFmpeg stderr:\\n{result.stderr}&quot;)\n            \n            if audio_output.exists():\n                size_mb = audio_output.stat().st_size / (1024 * 1024)\n                mode_str = f&quot;clip ({start_time} to {end_time})&quot; if processing_mode == &quot;clip&quot; else &quot;full media&quot;\n                \n                # Track output in manifest\n                stage_io.track_output(audio_output, &quot;audio&quot;, \n                                     format=&quot;wav&quot;, \n                                     sample_rate=16000,\n                                     channels=1,\n                                     size_mb=round(size_mb, 2))\n                \n                # Finalize manifest with success\n                stage_io.finalize(status=&quot;success&quot;, \n                                 output_size_mb=round(size_mb, 2),\n                                 processing_mode=processing_mode)\n                \n                self.logger.info(f&quot;\u2713 Audio extracted from {mode_str}: {audio_output.name} ({size_mb:.1f} MB)&quot;)\n                stage_logger.info(f&quot;Successfully extracted audio: {size_mb:.1f} MB&quot;)\n                stage_logger.info(f&quot;Stage log: {stage_io.stage_log.relative_to(self.job_dir)}&quot;)\n                stage_logger.info(f&quot;Stage manifest: {stage_io.manifest_path.relative_to(self.job_dir)}&quot;)\n                return True\n            else:\n                stage_io.add_error(&quot;Audio extraction failed - no output file&quot;)\n                stage_io.finalize(status=&quot;failed&quot;)\n                self.logger.error(&quot;Audio extraction failed&quot;)\n                stage_logger.error(&quot;Audio extraction failed - no output file&quot;)\n                return False\n                \n        except subprocess.CalledProcessError as e:\n            self.logger.error(f&quot;FFmpeg failed: {e}&quot;, exc_info=True)\n            stage_logger.error(f&quot;FFmpeg command failed: {e.stderr if e.stderr else str(e, exc_info=True)}&quot;, exc_info=True)\n            stage_io.add_error(f&quot;FFmpeg command failed: {e}&quot;)\n            stage_io.finalize(status=&quot;failed&quot;, error=&quot;Demux failed&quot;)\n            return False\n        \n        except FileNotFoundError as e:\n            self.logger.error(f&quot;Input file not found: {e}&quot;, exc_info=True)\n            stage_logger.error(f&quot;Input file not found: {e}&quot;, exc_info=True)\n            stage_io.add_error(f&quot;Input file not found: {e}&quot;)\n            stage_io.finalize(status=&quot;failed&quot;, error=str(e))\n            return False\n        \n        except IOError as e:\n            self.logger.error(f&quot;I/O error: {e}&quot;, exc_info=True)\n            stage_logger.error(f&quot;I/O error during demux: {e}&quot;, exc_info=True)\n            stage_io.add_error(f&quot;I/O error: {e}&quot;)\n            stage_io.finalize(status=&quot;failed&quot;, error=str(e))\n            return False\n        \n        except Exception as e:\n            self.logger.error(f&quot;Unexpected error: {e}&quot;, exc_info=True)\n            stage_logger.error(f&quot;Unexpected error during demux: {e}&quot;, exc_info=True)\n            stage_io.add_error(f&quot;Unexpected error: {e}&quot;)\n            stage_io.finalize(status=&quot;failed&quot;, error=str(e))\n            return False\n    \n    def _stage_tmdb_enrichment(self) -&gt; bool:\n        &quot;&quot;&quot;Stage 3: TMDB enrichment - Fetch movie metadata and generate glossaries&quot;&quot;&quot;\n        \n        # Check if TMDB enrichment is enabled\n        tmdb_config = self.job_config.get(&quot;tmdb_enrichment&quot;, {})\n        enabled = tmdb_config.get(&quot;enabled&quot;, False)\n        \n        if not enabled:\n            self.logger.info(&quot;TMDB enrichment is disabled (skipping)&quot;)\n            return True\n        \n        title = tmdb_config.get(&quot;title&quot;) or self.job_config.get(&quot;title&quot;)\n        year = tmdb_config.get(&quot;year&quot;) or self.job_config.get(&quot;year&quot;)\n        \n        if not title:\n            self.logger.warning(&quot;No movie title provided - skipping TMDB enrichment&quot;)\n            return True\n        \n        # Input/output setup\n        output_dir = self._stage_path(&quot;tmdb&quot;)\n        output_dir.mkdir(parents=True, exist_ok=True)\n        \n        # Log input/output\n        self.logger.info(f&quot;\U0001f4e5 Input: Title=\'{title}\', Year={year or \'N/A\'}&quot;)\n        self.logger.info(f&quot;\U0001f4e4 Output: {output_dir.relative_to(self.job_dir)}/&quot;)\n        self.logger.info(f&quot;Fetching TMDB metadata for: {title}&quot; + (f&quot; ({year})&quot; if year else &quot;&quot;))\n        \n        # Run the TMDB enrichment script\n        script_path = self.scripts_dir / &quot;tmdb_enrichment_stage.py&quot;\n        \n        try:\n            # Set up environment\n            env = os.environ.copy()\n            env[\'OUTPUT_DIR\'] = str(self.job_dir)\n            env[\'DEBUG_MODE\'] = \'true\' if self.debug else \'false\'\n            env[\'LOG_LEVEL\'] = \'DEBUG\' if self.debug else \'INFO\'\n            \n            # Build command arguments\n            cmd = [sys.executable, str(script_path), &quot;--job-dir&quot;, str(self.job_dir)]\n            if title:\n                cmd.extend([&quot;--title&quot;, title])\n            if year:\n                cmd.extend([&quot;--year&quot;, str(year)])\n            \n            # Run in common environment (TMDB/NER tools)\n            result = self._run_in_environment(\n                &quot;tmdb&quot;,  # Stage name for environment lookup\n                cmd,\n                capture_output=True,\n                text=True,\n                check=True,\n                env=env\n            )\n            \n            if self.debug:\n                self.logger.debug(f&quot;TMDB enrichment output: {result.stdout}&quot;)\n            \n            # Check if enrichment file was created\n            enrichment_file = self._stage_path(&quot;tmdb&quot;) / &quot;enrichment.json&quot;\n            if enrichment_file.exists():\n                with open(enrichment_file, \'r\', encoding=\'utf-8\') as f:\n                    data = json.load(f)\n                    \n                # Log success with details\n                cast_count = len(data.get(\'cast\', []))\n                crew_count = len(data.get(\'crew\', []))\n                soundtrack_count = len(data.get(\'soundtrack\', []))\n                \n                self.logger.info(f&quot;\u2713 TMDB metadata fetched successfully&quot;)\n                if cast_count &gt; 0:\n                    self.logger.info(f&quot;  Cast: {cast_count} actors&quot;)\n                if crew_count &gt; 0:\n                    self.logger.info(f&quot;  Crew: {crew_count} members&quot;)\n                if soundtrack_count &gt; 0:\n                    self.logger.info(f&quot;  Soundtrack: {soundtrack_count} songs&quot;)\n            else:\n                self.logger.warning(&quot;TMDB enrichment completed but no output file found&quot;)\n            \n            return True\n            \n        except subprocess.CalledProcessError as e:\n            self.logger.error(f&quot;TMDB enrichment failed: {e.stderr if e.stderr else str(e, exc_info=True)}&quot;)\n            self.logger.warning(&quot;Continuing without TMDB metadata&quot;)\n            return True  # Non-blocking failure\n    \n    def _stage_glossary_load(self) -&gt; bool:\n        &quot;&quot;&quot;Stage 3: Load glossary system using new modular stage&quot;&quot;&quot;\n        \n        try:\n            # Check if glossary is enabled\n            glossary_enabled = self.env_config.get(&quot;STAGE_03_GLOSSARY_ENABLED&quot;, &quot;true&quot;).lower() == &quot;true&quot;\n            \n            if not glossary_enabled:\n                self.logger.info(&quot;Glossary system is disabled (skipping)&quot;)\n                return True\n            \n            # Import glossary load stage module (module name starts with number, use importlib)\n            import importlib\n            glossary_load = importlib.import_module(&quot;scripts.03_glossary_load&quot;)\n            \n            # Call the stage module\n            self.logger.info(&quot;Running glossary load stage...&quot;)\n            exit_code = glossary_load.run_stage(self.job_dir, &quot;03_glossary_load&quot;)\n            \n            if exit_code != 0:\n                self.logger.error(&quot;Glossary load stage failed&quot;)\n                return False\n            \n            self.logger.info(&quot;\u2713 Glossary load complete&quot;)\n            return True\n            \n        except Exception as e:\n            self.logger.error(f&quot;Failed to load glossary system: {e}&quot;, exc_info=True)\n            if self.debug:\n                self.logger.debug(traceback.format_exc())\n            self.logger.warning(&quot;Continuing without glossary system&quot;)\n            return True  # Non-blocking failure\n    \n    def _stage_source_separation(self) -&gt; bool:\n        &quot;&quot;&quot;Stage 2: Source separation - Extract vocals, remove background music&quot;&quot;&quot;\n        \n        # Check if source separation is enabled\n        sep_config = self.job_config.get(&quot;source_separation&quot;, {})\n        enabled = sep_config.get(&quot;enabled&quot;, False)\n        \n        if not enabled:\n            self.logger.info(&quot;Source separation is disabled (skipping)&quot;)\n            return True\n        \n        # Input/output setup\n        input_audio = self.job_dir / &quot;01_demux&quot; / &quot;audio.wav&quot;\n        output_dir = self._stage_path(&quot;source_separation&quot;)\n        output_dir.mkdir(parents=True, exist_ok=True)\n        \n        # Log input/output\n        self.logger.info(f&quot;\U0001f4e5 Input: {input_audio.relative_to(self.job_dir)}&quot;)\n        self.logger.info(f&quot;\U0001f4e4 Output: {output_dir.relative_to(self.job_dir)}/&quot;)\n        \n        quality = sep_config.get(&quot;quality&quot;, &quot;balanced&quot;)\n        self.logger.info(f&quot;Running source separation (quality: {quality})...&quot;)\n        self.logger.info(&quot;This will extract vocals and remove background music&quot;)\n        \n        # Run the source separation script\n        script_path = self.scripts_dir / &quot;04_source_separation.py&quot;\n        \n        try:\n            # Set up environment\n            env = os.environ.copy()\n            env[\'OUTPUT_DIR\'] = str(self.job_dir)  # CRITICAL: Tell script where job directory is\n            env[\'DEBUG_MODE\'] = \'true\' if self.debug else \'false\'\n            env[\'LOG_LEVEL\'] = \'DEBUG\' if self.debug else \'INFO\'\n            \n            # Run in demucs environment\n            result = self._run_in_environment(\n                &quot;source_separation&quot;,\n                [sys.executable, str(script_path)],\n                capture_output=True,\n                text=True,\n                check=True,\n                env=env\n            )\n            \n            if self.debug:\n                self.logger.debug(f&quot;Source separation output: {result.stdout}&quot;)\n            \n            self.logger.info(&quot;\u2713 Vocals extracted successfully&quot;)\n            return True\n            \n        except subprocess.CalledProcessError as e:\n            self.logger.error(f&quot;Source separation failed: {e.stderr}&quot;, exc_info=True)\n            return False\n    \n    def _stage_lyrics_detection(self) -&gt; bool:\n        &quot;&quot;&quot;Stage 6: Lyrics detection using new modular stage&quot;&quot;&quot;\n        \n        try:\n            # Check if lyrics detection is enabled\n            lyrics_enabled = self.env_config.get(&quot;STAGE_06_LYRICS_ENABLED&quot;, &quot;true&quot;).lower() == &quot;true&quot;\n            \n            if not lyrics_enabled:\n                self.logger.info(&quot;Lyrics detection is disabled (skipping)&quot;)\n                return True\n            \n            # Import lyrics detection stage module (module name starts with number, use importlib)\n            import importlib\n            lyrics_detection = importlib.import_module(&quot;scripts.06_lyrics_detection&quot;)\n            \n            # Call the stage module\n            self.logger.info(&quot;Running lyrics detection stage...&quot;)\n            exit_code = lyrics_detection.run_stage(self.job_dir, &quot;06_lyrics_detection&quot;)\n            \n            if exit_code != 0:\n                self.logger.warning(&quot;Lyrics detection failed, continuing without lyrics metadata&quot;)\n                return True  # Non-fatal failure\n            \n            self.logger.info(&quot;\u2713 Lyrics detection complete&quot;)\n            return True\n            \n        except Exception as e:\n            self.logger.error(f&quot;Lyrics detection error: {e}&quot;, exc_info=True)\n            if self.debug:\n                self.logger.debug(traceback.format_exc())\n            self.logger.warning(&quot;Continuing pipeline without lyrics metadata&quot;)\n            return True  # Non-fatal, graceful degradation\n    \n    def _stage_pyannote_vad(self) -&gt; bool:\n        &quot;&quot;&quot;Stage 3: PyAnnote VAD for high-quality speech detection&quot;&quot;&quot;\n        \n        # Determine input audio source (from source_separation or demux)\n        sep_audio = self._stage_path(&quot;source_separation&quot;) / &quot;audio.wav&quot;\n        demux_audio = self.job_dir / &quot;01_demux&quot; / &quot;audio.wav&quot;\n        \n        if sep_audio.exists():\n            audio_file = sep_audio\n            audio_source = &quot;source-separated vocals (clean speech)&quot;\n        elif demux_audio.exists():\n            audio_file = demux_audio\n            audio_source = &quot;original audio from demux&quot;\n        else:\n            self.logger.error(&quot;No audio file found from previous stages&quot;)\n            self.logger.error(f&quot;Checked: {sep_audio}&quot;)\n            self.logger.error(f&quot;Checked: {demux_audio}&quot;)\n            return False\n        \n        # Output directory\n        output_dir = self._stage_path(&quot;pyannote_vad&quot;)\n        output_dir.mkdir(parents=True, exist_ok=True)\n        \n        # Log input/output\n        self.logger.info(f&quot;\U0001f4e5 Input: {audio_file.relative_to(self.job_dir)}&quot;)\n        self.logger.info(f&quot;\U0001f4e4 Output: {output_dir.relative_to(self.job_dir)}/&quot;)\n        self.logger.info(f&quot;Running PyAnnote VAD for voice activity detection...&quot;)\n        self.logger.info(f&quot;Using {audio_source}&quot;)\n        \n        # Get device configuration\n        device_config = self.env_config.get(&quot;WHISPERX_DEVICE&quot;, self.main_config.device_whisperx)\n        # PyAnnote works on CPU, CUDA, MPS\n        device = device_config.lower()\n        \n        self.logger.info(f&quot;PyAnnote VAD device: {device}&quot;)\n        self.logger.info(&quot;Using PyAnnote for highest quality speech detection&quot;)\n        self.logger.info(&quot;This improves transcription accuracy, especially for movies with music/noise&quot;)\n        \n        try:\n            import subprocess\n            \n            # Get Python executable from PyAnnote environment (dedicated for VAD)\n            python_exe = self.env_manager.get_python_executable(&quot;pyannote&quot;)\n            self.logger.info(f&quot;Using PyAnnote environment: {python_exe}&quot;)\n            \n            # Build path to job config file\n            job_id = self.job_config[&quot;job_id&quot;]\n            job_config_file = self.job_dir / f&quot;.{job_id}.env&quot;\n            \n            # Set up environment\n            env = os.environ.copy()\n            env[\'CONFIG_PATH\'] = str(job_config_file)\n            env[\'OUTPUT_DIR\'] = str(self.job_dir)\n            env[\'PYANNOTE_DEVICE\'] = device\n            env[\'DEBUG_MODE\'] = \'true\' if self.debug else \'false\'\n            env[\'LOG_LEVEL\'] = \'DEBUG\' if self.debug else \'INFO\'\n            env[\'AUDIO_INPUT\'] = str(audio_file)  # Pass audio path directly\n            env[\'VAD_OUTPUT_DIR\'] = str(output_dir)  # Pass VAD output directory\n            \n            # Run PyAnnote VAD script\n            script_path = PROJECT_ROOT / &quot;scripts&quot; / &quot;05_pyannote_vad.py&quot;\n            \n            result = subprocess.run(\n                [str(python_exe), str(script_path)],\n                capture_output=True,\n                text=True,\n                check=True,\n                cwd=str(PROJECT_ROOT),\n                env=env\n            )\n            \n            if self.debug and result.stdout:\n                self.logger.debug(f&quot;PyAnnote VAD output: {result.stdout}&quot;)\n            \n            # Check for output\n            segments_file = output_dir / &quot;speech_segments.json&quot;\n            if segments_file.exists():\n                # Read and log statistics\n                import json\n                with open(segments_file) as f:\n                    vad_data = json.load(f)\n                \n                if \'segments\' in vad_data:\n                    num_segments = len(vad_data[\'segments\'])\n                    self.logger.info(f&quot;VAD detected {num_segments} speech segments&quot;)\n                    \n                    # Calculate total speech duration\n                    total_duration = sum(seg[\'end\'] - seg[\'start\'] for seg in vad_data[\'segments\'])\n                    self.logger.info(f&quot;Total speech duration: {total_duration:.1f}s&quot;)\n                else:\n                    self.logger.warning(&quot;VAD output missing \'segments\' key&quot;)\n                \n                self.logger.info(f&quot;\u2713 PyAnnote VAD completed: {segments_file}&quot;)\n                return True\n            else:\n                self.logger.error(&quot;PyAnnote VAD failed - no output file generated&quot;)\n                self.logger.error(&quot;Pipeline cannot continue without VAD preprocessing&quot;)\n                return False\n                \n        except subprocess.CalledProcessError as e:\n            self.logger.error(f&quot;PyAnnote VAD error: {e.stderr}&quot;, exc_info=True)\n            self.logger.error(&quot;Pipeline cannot continue without VAD preprocessing&quot;, exc_info=True)\n            return False\n        except Exception as e:\n            self.logger.error(f&quot;Unexpected error in PyAnnote VAD: {e}&quot;, exc_info=True)\n            self.logger.error(&quot;Pipeline cannot continue without VAD preprocessing&quot;, exc_info=True)\n            return False\n    \n    def _stage_asr(self) -&gt; bool:\n        &quot;&quot;&quot;Stage 4: Transcribe audio using WhisperX or MLX-Whisper&quot;&quot;&quot;\n        \n        # Determine input audio source (from source_separation or demux)\n        sep_audio = self._stage_path(&quot;source_separation&quot;) / &quot;audio.wav&quot;\n        demux_audio = self.job_dir / &quot;01_demux&quot; / &quot;audio.wav&quot;\n        \n        if sep_audio.exists():\n            audio_file = sep_audio\n            audio_source = &quot;source-separated vocals (clean speech)&quot;\n        elif demux_audio.exists():\n            audio_file = demux_audio\n            audio_source = &quot;original audio from demux&quot;\n        else:\n            self.logger.error(&quot;No audio file found from previous stages&quot;)\n            self.logger.error(f&quot;Checked: {sep_audio}&quot;)\n            self.logger.error(f&quot;Checked: {demux_audio}&quot;)\n            return False\n        \n        # Output directory\n        output_dir = self._stage_path(&quot;asr&quot;)\n        output_dir.mkdir(parents=True, exist_ok=True)\n        \n        source_lang = self.job_config[&quot;source_language&quot;]\n        \n        # Load VAD segments if available\n        vad_segments = None\n        vad_file = self._stage_path(&quot;pyannote_vad&quot;) / &quot;speech_segments.json&quot;\n        vad_info = &quot;&quot;\n        if vad_file.exists():\n            try:\n                import json\n                with open(vad_file) as f:\n                    vad_data = json.load(f)\n                if \'segments\' in vad_data and vad_data[\'segments\']:\n                    vad_segments = vad_data[\'segments\']\n                    vad_info = f&quot; + VAD segments ({len(vad_segments)})&quot;\n                else:\n                    self.logger.warning(&quot;VAD file exists but has no segments, transcribing full audio&quot;)\n            except Exception as e:\n                self.logger.warning(f&quot;Failed to load VAD segments: {e}&quot;)\n                self.logger.warning(&quot;Proceeding with full audio transcription&quot;)\n        \n        # Log input/output\n        self.logger.info(f&quot;\U0001f4e5 Input: {audio_file.relative_to(self.job_dir)}{vad_info}&quot;)\n        self.logger.info(f&quot;\U0001f4e4 Output: {output_dir.relative_to(self.job_dir)}/&quot;)\n        self.logger.info(f&quot;Transcribing audio...&quot;)\n        self.logger.info(f&quot;Using {audio_source}&quot;)\n        \n        # Get configuration from job\'s .env file (set by prepare-job)\n        # Fall back to main config if not set in job\n        device_config = self.env_config.get(&quot;WHISPERX_DEVICE&quot;, self.main_config.device_whisperx)\n        whisper_model = self.env_config.get(&quot;WHISPER_MODEL&quot;, self.main_config.whisperx_model)\n        compute_type = self.env_config.get(&quot;WHISPER_COMPUTE_TYPE&quot;, self.main_config.whisper_compute_type)\n        batch_size = int(self.env_config.get(&quot;BATCH_SIZE&quot;, str(self.main_config.batch_size)))\n        backend = self.env_config.get(&quot;WHISPER_BACKEND&quot;, self.main_config.whisper_backend)\n        \n        self.logger.info(f&quot;Configured device: {device_config} (from job config)&quot;)\n        self.logger.info(f&quot;Using model: {whisper_model} (from job config)&quot;)\n        self.logger.info(f&quot;Compute type: {compute_type} (from job config)&quot;)\n        self.logger.info(f&quot;Batch size: {batch_size} (from job config)&quot;)\n        self.logger.info(f&quot;Backend: {backend} (from job config)&quot;)\n        \n        # Dynamic environment selection based on backend\n        asr_env = self.env_manager.get_asr_environment(backend)\n        self.logger.info(f&quot;Using ASR environment: {asr_env}&quot;)\n        \n        # Get Python executable from selected environment\n        try:\n            python_exe = self.env_manager.get_python_executable(asr_env)\n            self.logger.info(f&quot;Using WhisperX environment: {python_exe}&quot;)\n        except (ValueError, FileNotFoundError) as e:\n            self.logger.error(f&quot;Failed to get Python executable for {asr_env} environment: {e}&quot;, exc_info=True)\n            self.logger.error(&quot;Run bootstrap.sh to set up environments&quot;, exc_info=True)\n            return False\n        \n        # Run 06_whisperx_asr.py stage script in the selected environment\n        asr_script = self.scripts_dir / &quot;06_whisperx_asr.py&quot;\n        if not asr_script.exists():\n            self.logger.error(f&quot;ASR script not found: {asr_script}&quot;, exc_info=True)\n            return False\n        \n        # Run ASR stage with subprocess\n        self.logger.info(f&quot;Running stage \'asr\' in environment \'{asr_env}\'&quot;)\n        env = os.environ.copy()\n        env[&quot;OUTPUT_DIR&quot;] = str(self.job_dir)\n        env[&quot;PYTHONPATH&quot;] = f&quot;{PROJECT_ROOT}:{env.get(\'PYTHONPATH\', \'\')}&quot;\n        \n        result = subprocess.run(\n            [str(python_exe), str(asr_script)],\n            env=env,\n            capture_output=True,\n            text=True\n        )\n        \n        if result.returncode != 0:\n            self.logger.error(f&quot;ASR stage failed with exit code {result.returncode}&quot;)\n            if result.stderr:\n                self.logger.error(f&quot;Error output: {result.stderr}&quot;)\n            return False\n        \n        # Add retry logic for file detection with exponential backoff\n        import time\n        segments_file = output_dir / &quot;segments.json&quot;\n        \n        # Retry up to 5 times with exponential backoff\n        for attempt in range(5):\n            if segments_file.exists():\n                break\n            if attempt &lt; 4:\n                wait_time = 0.1 * (2 ** attempt)  # 0.1, 0.2, 0.4, 0.8, 1.6 seconds\n                self.logger.debug(f&quot;segments.json not found, retrying in {wait_time}s (attempt {attempt+1}/5)&quot;)\n                time.sleep(wait_time)\n            else:\n                self.logger.error(f&quot;Transcription failed - segments.json not found after 5 attempts&quot;)\n                self.logger.error(f&quot;  Checked: {segments_file}&quot;)\n                self.logger.error(f&quot;  Directory contents: {list(output_dir.glob(\'*\'))}&quot;)\n                return False\n        \n        # File exists, proceed with verification and copy\n        file_size = segments_file.stat().st_size\n        self.logger.info(f&quot;\u2713 Transcription completed: {segments_file.relative_to(self.job_dir)}&quot;)\n        self.logger.info(f&quot;  File size: {file_size} bytes&quot;)\n        \n        # Copy to transcripts/ for compatibility\n        import shutil\n        transcripts_dir = self.job_dir / &quot;transcripts&quot;\n        transcripts_dir.mkdir(parents=True, exist_ok=True)\n        dest_file = transcripts_dir / &quot;segments.json&quot;\n        shutil.copy2(segments_file, dest_file)\n        \n        # Verify copy\n        if dest_file.exists() and dest_file.stat().st_size == file_size:\n            self.logger.info(f&quot;\u2713 Copied to: transcripts/segments.json ({file_size} bytes)&quot;)\n            return True\n        else:\n            self.logger.error(f&quot;Copy verification failed&quot;)\n            self.logger.error(f&quot;  Source: {segments_file} ({file_size} bytes)&quot;)\n            self.logger.error(f&quot;  Dest exists: {dest_file.exists()}&quot;)\n            if dest_file.exists():\n                self.logger.error(f&quot;  Dest size: {dest_file.stat().st_size} bytes&quot;)\n            return False\n    \n    def _stage_asr_mlx(self, audio_file: Path, output_dir: Path, \n                       source_lang: str, model: str, vad_segments: list = None) -&gt; bool:\n        &quot;&quot;&quot;ASR using MLX-Whisper (Apple Silicon MPS acceleration)\n        \n        Args:\n            vad_segments: Optional list of speech segments from PyAnnote VAD\n                         Format: [{&quot;start&quot;: 0.5, &quot;end&quot;: 3.2}, ...]\n        &quot;&quot;&quot;\n        \n        # Map model names to MLX format\n        model_map = {\n            &quot;large-v3&quot;: &quot;mlx-community/whisper-large-v3-mlx&quot;,\n            &quot;large-v2&quot;: &quot;mlx-community/whisper-large-v2-mlx&quot;,\n            &quot;large&quot;: &quot;mlx-community/whisper-large-v3-mlx&quot;,\n            &quot;medium&quot;: &quot;mlx-community/whisper-medium-mlx&quot;,\n            &quot;small&quot;: &quot;mlx-community/whisper-small-mlx&quot;,\n            &quot;base&quot;: &quot;mlx-community/whisper-base-mlx&quot;,\n            &quot;tiny&quot;: &quot;mlx-community/whisper-tiny-mlx&quot;,\n        }\n        mlx_model = model_map.get(model, model)\n        \n        self.logger.info(f&quot;Mapping model \'{model}\' to MLX format: \'{mlx_model}\'&quot;)\n        \n        script_content = f&quot;&quot;&quot;\nimport mlx_whisper\nimport json\nfrom pathlib import Path\n\n# Load audio and transcribe with MLX (MPS acceleration)\naudio_file = &quot;{audio_file}&quot;\noutput_dir = Path(&quot;{output_dir}&quot;)\noutput_dir.mkdir(exist_ok=True)\n\nlogger.info(&quot;Loading MLX-Whisper model: {mlx_model}&quot;)\nlogger.info(&quot;Using MPS (Apple Silicon GPU) acceleration&quot;)\n\n# Transcribe with MLX-Whisper\n# This automatically uses MPS for acceleration\nresult = mlx_whisper.transcribe(\n    str(audio_file),\n    path_or_hf_repo=&quot;{mlx_model}&quot;,\n    language=&quot;{source_lang}&quot;,\n    verbose={\'True\' if self.debug else \'False\'}\n)\n\n# Convert to WhisperX-compatible format\nsegments = []\nif &quot;segments&quot; in result:\n    for seg in result[&quot;segments&quot;]:\n        segments.append({{\n            &quot;start&quot;: seg[&quot;start&quot;],\n            &quot;end&quot;: seg[&quot;end&quot;],\n            &quot;text&quot;: seg[&quot;text&quot;],\n            &quot;words&quot;: []  # MLX-Whisper doesn\'t provide word-level timestamps by default\n        }})\n\noutput = {{\n    &quot;segments&quot;: segments,\n    &quot;language&quot;: result.get(&quot;language&quot;, &quot;{source_lang}&quot;),\n    &quot;text&quot;: result.get(&quot;text&quot;, &quot;&quot;)\n}}\n\n# Save segments\nsegments_file = output_dir / &quot;segments.json&quot;\nwith open(segments_file, \'w\', encoding=\'utf-8\') as f:\n    json.dump(output, f, indent=2, ensure_ascii=False)\n\nlogger.info(f&quot;Transcription completed: {{len(segments)}} segments&quot;)\n&quot;&quot;&quot;\n        \n        # Write script to temp file\n        temp_script = output_dir / &quot;asr_mlx_temp.py&quot;\n        with open(temp_script, \'w\') as f:\n            f.write(script_content)\n        \n        try:\n            # Run the MLX script in venv/mlx environment\n            import subprocess\n            \n            # Get Python executable from MLX environment\n            python_exe = self.env_manager.get_python_executable(&quot;mlx&quot;)\n            self.logger.info(f&quot;Using MLX environment: {python_exe}&quot;)\n            \n            # Set up environment with debug flag\n            env = os.environ.copy()\n            env[\'DEBUG_MODE\'] = \'true\' if self.debug else \'false\'\n            env[\'LOG_LEVEL\'] = \'DEBUG\' if self.debug else \'INFO\'\n            \n            result = subprocess.run(\n                [str(python_exe), str(temp_script)],\n                capture_output=True,\n                text=True,\n                check=True,\n                cwd=str(PROJECT_ROOT),\n                env=env\n            )\n            \n            self.logger.info(f&quot;MLX-Whisper output: {result.stdout}&quot;)\n            \n            # Check for output\n            segments_file = output_dir / &quot;segments.json&quot;\n            if segments_file.exists():\n                self.logger.info(f&quot;\u2713 Transcription completed: {segments_file.relative_to(self.job_dir)}&quot;)\n                \n                # Copy to transcripts/ for compatibility\n                transcripts_dir = self.job_dir / &quot;transcripts&quot;\n                transcripts_dir.mkdir(parents=True, exist_ok=True)\n                import shutil\n                shutil.copy2(segments_file, transcripts_dir / &quot;segments.json&quot;)\n                self.logger.info(f&quot;\u2713 Copied to: transcripts/segments.json&quot;)\n                \n                return True\n            else:\n                self.logger.error(&quot;Transcription failed - no output&quot;)\n                return False\n                \n        except subprocess.CalledProcessError as e:\n            self.logger.error(f&quot;MLX-Whisper error: {e.stderr}&quot;, exc_info=True)\n            return False\n        finally:\n            # Clean up temp script\n            if temp_script.exists():\n                temp_script.unlink()\n    \n    def _stage_asr_whisperx(self, audio_file: Path, output_dir: Path,\n                           source_lang: str, model: str, device: str,\n                           compute_type: str, batch_size: int, vad_segments: list = None) -&gt; bool:\n        &quot;&quot;&quot;ASR using WhisperX (faster-whisper/CTranslate2)\n        \n        Calls the proper whisperx_asr stage script to get full functionality\n        including bias term support and proper parameter handling.\n        \n        Args:\n            vad_segments: Optional list of speech segments from PyAnnote VAD\n                         Format: [{&quot;start&quot;: 0.5, &quot;end&quot;: 3.2}, ...]\n        &quot;&quot;&quot;\n        \n        import subprocess\n        \n        # Get Python executable from WhisperX environment\n        python_exe = self.env_manager.get_python_executable(&quot;whisperx&quot;)\n        self.logger.info(f&quot;Using WhisperX environment: {python_exe}&quot;)\n        \n        # Use the proper 06_whisperx_asr script that supports all features\n        asr_script = self.scripts_dir / &quot;06_whisperx_asr.py&quot;\n        \n        if not asr_script.exists():\n            self.logger.error(f&quot;ASR script not found: {asr_script}&quot;)\n            return False\n        \n        # Set up environment variables for the stage\n        env = os.environ.copy()\n        env[\'DEBUG_MODE\'] = \'true\' if self.debug else \'false\'\n        env[\'LOG_LEVEL\'] = \'DEBUG\' if self.debug else \'INFO\'\n        env[\'CONFIG_PATH\'] = str(self.job_dir / \'job.json\')\n        env[\'JOB_DIR\'] = str(self.job_dir)\n        env[\'OUTPUT_DIR\'] = str(self.job_dir)  # StageIO uses OUTPUT_DIR\n        env[\'DEVICE_OVERRIDE\'] = device\n        \n        try:\n            result = subprocess.run(\n                [str(python_exe), str(asr_script)],\n                capture_output=True,\n                text=True,\n                check=True,\n                cwd=str(PROJECT_ROOT),\n                env=env\n            )\n            \n            self.logger.info(f&quot;ASR output: {result.stdout}&quot;)\n            \n            # Check for output\n            segments_file = output_dir / &quot;segments.json&quot;\n            if segments_file.exists():\n                self.logger.info(f&quot;\u2713 Transcription completed: {segments_file.relative_to(self.job_dir)}&quot;)\n                \n                # Copy to transcripts/ for compatibility\n                transcripts_dir = self.job_dir / &quot;transcripts&quot;\n                transcripts_dir.mkdir(parents=True, exist_ok=True)\n                import shutil\n                shutil.copy2(segments_file, transcripts_dir / &quot;segments.json&quot;)\n                self.logger.info(f&quot;\u2713 Copied to: transcripts/segments.json&quot;)\n                \n                return True\n            else:\n                self.logger.error(&quot;Transcription failed - no output&quot;)\n                return False\n                \n        except subprocess.CalledProcessError as e:\n            self.logger.error(f&quot;ASR error: {e.stderr}&quot;, exc_info=True)\n            return False\n    \n    def _stage_alignment(self) -&gt; bool:\n        &quot;&quot;&quot;Stage 5: Word-level alignment&quot;&quot;&quot;\n        \n        # Read from ASR stage output\n        segments_file = self._stage_path(&quot;asr&quot;) / &quot;segments.json&quot;\n        output_dir = self._stage_path(&quot;alignment&quot;)\n        output_dir.mkdir(parents=True, exist_ok=True)\n        \n        # Log input/output\n        self.logger.info(f&quot;\U0001f4e5 Input: {segments_file.relative_to(self.job_dir)}&quot;)\n        self.logger.info(f&quot;\U0001f4e4 Output: {output_dir.relative_to(self.job_dir)}/&quot;)\n        \n        if not segments_file.exists():\n            self.logger.error(f&quot;Segments file not found: {segments_file}&quot;)\n            return False\n        \n        # Load segments\n        with open(segments_file) as f:\n            raw_data = json.load(f)\n        \n        # Normalize to consistent format\n        data, segments = normalize_segments_data(raw_data)\n        \n        if not segments:\n            self.logger.error(&quot;No segments found in transcript&quot;)\n            return False\n        \n        # Check if segments already have word-level timestamps\n        has_words = segments[0].get(&quot;words&quot;, []) if segments else []\n        backend = self.env_config.get(&quot;WHISPER_BACKEND&quot;, self.main_config.whisper_backend)\n        \n        if has_words and len(has_words) &gt; 0:\n            # Already aligned\n            total_words = sum(len(seg.get(&quot;words&quot;, [])) for seg in segments)\n            self.logger.info(f&quot;\u2713 Segments already have word-level timestamps&quot;)\n            self.logger.info(f&quot;  Segments: {len(segments)}, Words: {total_words}&quot;)\n            \n            # Copy to alignment output\n            output_file = output_dir / &quot;segments_aligned.json&quot;\n            with open(output_file, \'w\', encoding=\'utf-8\') as f:\n                json.dump(data, f, indent=2, ensure_ascii=False)\n            \n            self.logger.info(f&quot;\u2713 Alignment verified and saved: {output_file.relative_to(self.job_dir)}&quot;)\n            return True\n        \n        # Need to perform alignment\n        self.logger.info(f&quot;\u26a0\ufe0f  Segments missing word-level timestamps&quot;)\n        self.logger.info(f&quot;Backend: {backend}&quot;)\n        \n        if backend == &quot;mlx&quot;:\n            # Use MLX alignment\n            self.logger.info(&quot;Performing word-level alignment with MLX-Whisper...&quot;)\n            return self._perform_mlx_alignment(segments_file, output_dir)\n        else:\n            # WhisperX should have provided word timestamps already\n            self.logger.warning(&quot;WhisperX backend should provide word timestamps during transcription&quot;)\n            self.logger.warning(&quot;Proceeding without word-level alignment&quot;)\n            \n            # Copy segments anyway\n            output_file = output_dir / &quot;segments_aligned.json&quot;\n            with open(output_file, \'w\', encoding=\'utf-8\') as f:\n                json.dump(data, f, indent=2, ensure_ascii=False)\n            \n            return True\n    \n    def _perform_mlx_alignment(self, segments_file: Path, output_dir: Path) -&gt; bool:\n        &quot;&quot;&quot;Perform word-level alignment using MLX-Whisper&quot;&quot;&quot;\n        \n        # Determine audio source\n        sep_audio = self._stage_path(&quot;source_separation&quot;) / &quot;audio.wav&quot;\n        demux_audio = self.job_dir / &quot;01_demux&quot; / &quot;audio.wav&quot;\n        \n        if sep_audio.exists():\n            audio_file = sep_audio\n        elif demux_audio.exists():\n            audio_file = demux_audio\n        else:\n            self.logger.error(&quot;No audio file found for alignment&quot;)\n            return False\n        \n        source_lang = self.job_config[&quot;source_language&quot;]\n        whisper_model = self.env_config.get(&quot;WHISPER_MODEL&quot;, self.main_config.whisperx_model)\n        \n        # Map model names to MLX format\n        model_map = {\n            &quot;large-v3&quot;: &quot;mlx-community/whisper-large-v3-mlx&quot;,\n            &quot;large-v2&quot;: &quot;mlx-community/whisper-large-v2-mlx&quot;,\n            &quot;large&quot;: &quot;mlx-community/whisper-large-v3-mlx&quot;,\n            &quot;medium&quot;: &quot;mlx-community/whisper-medium-mlx&quot;,\n            &quot;small&quot;: &quot;mlx-community/whisper-small-mlx&quot;,\n            &quot;base&quot;: &quot;mlx-community/whisper-base-mlx&quot;,\n            &quot;tiny&quot;: &quot;mlx-community/whisper-tiny-mlx&quot;,\n        }\n        mlx_model = model_map.get(whisper_model, whisper_model)\n        \n        output_file = output_dir / &quot;segments_aligned.json&quot;\n        \n        self.logger.info(f&quot;Audio: {audio_file.relative_to(self.job_dir)}&quot;)\n        self.logger.info(f&quot;Model: {mlx_model}&quot;)\n        self.logger.info(f&quot;Language: {source_lang}&quot;)\n        \n        # Use mlx_alignment.py script\n        alignment_script = self.scripts_dir / &quot;mlx_alignment.py&quot;\n        \n        if not alignment_script.exists():\n            self.logger.error(f&quot;MLX alignment script not found: {alignment_script}&quot;)\n            return False\n        \n        # Get Python executable from MLX environment\n        python_exe = self.env_manager.get_python_executable(&quot;mlx&quot;)\n        \n        try:\n            import subprocess\n            \n            cmd = [\n                str(python_exe),\n                str(alignment_script),\n                str(audio_file),\n                str(segments_file),\n                str(output_file),\n                &quot;--model&quot;, mlx_model,\n                &quot;--language&quot;, source_lang\n            ]\n            \n            if self.debug:\n                cmd.append(&quot;--debug&quot;)\n            \n            result = subprocess.run(\n                cmd,\n                capture_output=True,\n                text=True,\n                check=True,\n                cwd=str(PROJECT_ROOT)\n            )\n            \n            if self.debug:\n                self.logger.debug(f&quot;Alignment output: {result.stdout}&quot;)\n            \n            # Verify output\n            if output_file.exists():\n                with open(output_file) as f:\n                    raw_data = json.load(f)\n                \n                data, segments = normalize_segments_data(raw_data)\n                total_words = sum(len(seg.get(&quot;words&quot;, [])) for seg in segments)\n                \n                self.logger.info(f&quot;\u2713 Alignment completed: {len(segments)} segments, {total_words} words&quot;)\n                self.logger.info(f&quot;\u2713 Output saved: {output_file.relative_to(self.job_dir)}&quot;)\n                return True\n            else:\n                self.logger.error(&quot;Alignment output file not created&quot;)\n                return False\n                \n        except subprocess.CalledProcessError as e:\n            self.logger.error(f&quot;MLX alignment failed: {e}&quot;, exc_info=True)\n            if e.stderr:\n                self.logger.error(f&quot;Error output: {e.stderr}&quot;, exc_info=True)\n            return False\n    \n    \n    def _stage_ner(self) -&gt; bool:\n        &quot;&quot;&quot;Stage 5: Named Entity Recognition using new modular stage&quot;&quot;&quot;\n        \n        try:\n            # Check if NER is enabled\n            ner_enabled = self.env_config.get(&quot;STAGE_05_NER_ENABLED&quot;, &quot;true&quot;).lower() == &quot;true&quot;\n            \n            if not ner_enabled:\n                self.logger.info(&quot;NER stage is disabled (skipping)&quot;)\n                return True\n            \n            # Import NER stage module (module name starts with number, use importlib)\n            import importlib\n            ner_stage = importlib.import_module(&quot;scripts.05_ner&quot;)\n            \n            # Call the stage module\n            self.logger.info(&quot;Running NER stage...&quot;)\n            exit_code = ner_stage.run_stage(self.job_dir, &quot;05_ner&quot;)\n            \n            if exit_code != 0:\n                self.logger.warning(&quot;NER stage failed, continuing without NER data&quot;)\n                return True  # Non-fatal failure\n            \n            self.logger.info(&quot;\u2713 NER stage complete&quot;)\n            return True\n            \n        except Exception as e:\n            self.logger.error(f&quot;NER stage error: {e}&quot;, exc_info=True)\n            if self.debug:\n                self.logger.debug(traceback.format_exc())\n            self.logger.warning(&quot;Continuing without NER data&quot;)\n            return True  # Non-fatal, graceful degradation\n    \n    def _stage_subtitle_gen(self) -&gt; bool:\n        &quot;&quot;&quot;Stage 9: Subtitle generation using new modular stage&quot;&quot;&quot;\n        \n        try:\n            # Check if subtitle generation is enabled\n            subtitle_enabled = self.env_config.get(&quot;STAGE_09_SUBTITLE_ENABLED&quot;, &quot;true&quot;).lower() == &quot;true&quot;\n            \n            if not subtitle_enabled:\n                self.logger.info(&quot;Subtitle generation is disabled (skipping)&quot;)\n                return True\n            \n            # Import subtitle generation stage module (module name starts with number, use importlib)\n            import importlib\n            subtitle_gen = importlib.import_module(&quot;scripts.09_subtitle_gen&quot;)\n            \n            # Call the stage module\n            self.logger.info(&quot;Running subtitle generation stage...&quot;)\n            exit_code = subtitle_gen.run_stage(self.job_dir, &quot;09_subtitle_gen&quot;)\n            \n            if exit_code != 0:\n                self.logger.error(&quot;Subtitle generation failed&quot;)\n                return False  # Fatal failure\n            \n            self.logger.info(&quot;\u2713 Subtitle generation complete&quot;)\n            return True\n            \n        except Exception as e:\n            self.logger.error(f&quot;Subtitle generation error: {e}&quot;, exc_info=True)\n            if self.debug:\n                self.logger.debug(traceback.format_exc())\n            return False  # Fatal failure\n    \n    def _stage_export_transcript(self) -&gt; bool:\n        &quot;&quot;&quot;Stage: Export plain text transcript&quot;&quot;&quot;\n        \n        # Read from ASR stage output (already copied to transcripts/)\n        segments_file = self.job_dir / &quot;transcripts&quot; / &quot;segments.json&quot;\n        output_txt = self.job_dir / &quot;transcripts&quot; / &quot;transcript.txt&quot;\n        \n        # Log input/output\n        self.logger.info(f&quot;\U0001f4e5 Input: {segments_file.relative_to(self.job_dir)}&quot;)\n        self.logger.info(f&quot;\U0001f4e4 Output: {output_txt.relative_to(self.job_dir)}&quot;)\n        self.logger.info(&quot;Exporting plain text transcript...&quot;)\n        \n        if not segments_file.exists():\n            self.logger.error(f&quot;Segments file not found: {segments_file}&quot;)\n            return False\n        \n        try:\n            with open(segments_file) as f:\n                data = json.load(f)\n            \n            if &quot;segments&quot; not in data:\n                self.logger.error(&quot;No segments in JSON file&quot;)\n                return False\n            \n            # Extract text from all segments\n            lines = []\n            for segment in data[&quot;segments&quot;]:\n                text = segment.get(&quot;text&quot;, &quot;&quot;).strip()\n                if text:\n                    lines.append(text)\n            \n            # Write to text file\n            with open(output_txt, \'w\', encoding=\'utf-8\') as f:\n                f.write(&quot;\\n&quot;.join(lines))\n            \n            self.logger.info(f&quot;\u2713 Plain text transcript exported: {output_txt.name}&quot;)\n            self.logger.info(f&quot;Total lines: {len(lines)}&quot;)\n            return True\n            \n        except Exception as e:\n            self.logger.error(f&quot;Error exporting transcript: {e}&quot;, exc_info=True)\n            return False\n    \n    def _stage_load_transcript(self) -&gt; bool:\n        &quot;&quot;&quot;Stage: Load transcript from ASR stage&quot;&quot;&quot;\n        \n        # Prefer cleaned transcript from transcripts/ (after hallucination removal)\n        # Fall back to raw ASR output if not available\n        transcript_file = self.job_dir / &quot;transcripts&quot; / &quot;segments.json&quot;\n        segments_file = self._stage_path(&quot;asr&quot;) / &quot;segments.json&quot;\n        \n        # Use cleaned transcript if available, otherwise raw ASR output\n        if transcript_file.exists():\n            load_file = transcript_file\n            self.logger.info(&quot;Using cleaned transcript (after hallucination removal)&quot;)\n        elif segments_file.exists():\n            load_file = segments_file\n            self.logger.info(&quot;Using raw ASR transcript&quot;)\n        else:\n            self.logger.error(&quot;Transcript not found in transcripts/ or asr stage!&quot;)\n            self.logger.error(&quot;Run transcribe workflow first!&quot;)\n            return False\n        \n        # Log input\n        self.logger.info(f&quot;\U0001f4e5 Input: {load_file.relative_to(self.job_dir)}&quot;)\n        self.logger.info(&quot;Loading transcript...&quot;)\n        \n        with open(load_file) as f:\n            raw_data = json.load(f)\n        \n        # Normalize data format (handles both list and dict formats)\n        data, segments = normalize_segments_data(raw_data)\n        \n        if not segments or len(segments) == 0:\n            self.logger.error(&quot;No segments in transcript&quot;)\n            return False\n        \n        self.logger.info(f&quot;Loaded {len(segments)} segments&quot;)\n        return True\n    \n    def _stage_hybrid_translation(self) -&gt; bool:\n        &quot;&quot;&quot;\n        Stage: Hybrid translation - IndicTrans2 for dialogue, LLM for songs\n        \n        Combines:\n        - IndicTrans2 for dialogue (fast, accurate, free)\n        - LLM with film context for songs/poetry (creative, culturally aware)\n        \n        Automatically routes segments based on lyrics detection results.\n        Falls back to standard IndicTrans2 if LLM unavailable or disabled.\n        &quot;&quot;&quot;\n        self.logger.info(&quot;Running hybrid translation...&quot;)\n        \n        # Check if enabled\n        use_hybrid = self.env_config.get(&quot;USE_HYBRID_TRANSLATION&quot;, &quot;true&quot;).lower() == &quot;true&quot;\n        if not use_hybrid:\n            self.logger.info(&quot;Hybrid translation disabled, using standard IndicTrans2&quot;)\n            return self._stage_indictrans2_translation()\n        \n        # Get configuration\n        use_llm_for_songs = self.env_config.get(&quot;USE_LLM_FOR_SONGS&quot;, &quot;true&quot;).lower() == &quot;true&quot;\n        llm_provider = self.env_config.get(&quot;LLM_PROVIDER&quot;, &quot;anthropic&quot;)\n        source_lang = self.job_config[&quot;source_language&quot;]\n        # Handle both target_language (singular) and target_languages (plural)\n        target_lang = self._get_target_language()\n        if not target_lang:\n            target_langs = self.job_config.get(&quot;target_languages&quot;, [])\n            target_lang = target_langs[0] if target_langs else None\n        \n        if not target_lang:\n            raise ValueError(&quot;No target language specified in job config&quot;)\n        \n        self.logger.info(f&quot;Configuration:&quot;)\n        self.logger.info(f&quot;  Translation: {source_lang} \u2192 {target_lang}&quot;)\n        self.logger.info(f&quot;  LLM provider: {llm_provider}&quot;)\n        self.logger.info(f&quot;  LLM for songs: {use_llm_for_songs}&quot;)\n        \n        # Get film context\n        film_title = self.job_config.get(&quot;title&quot;, &quot;&quot;)\n        film_year = self.job_config.get(&quot;year&quot;, &quot;&quot;)\n        film_context = None\n        \n        if film_title and film_year:\n            prompt_file = PROJECT_ROOT / &quot;glossary&quot; / &quot;prompts&quot; / f&quot;{film_title.lower().replace(\' \', \'_\')}_{film_year}.txt&quot;\n            if prompt_file.exists():\n                with open(prompt_file, \'r\', encoding=\'utf-8\') as f:\n                    film_context = f.read()\n                self.logger.info(f&quot;\u2713 Loaded film context: {prompt_file.name}&quot;)\n            else:\n                self.logger.info(f&quot;No film context found: {prompt_file.name}&quot;)\n        \n        # Input/output files (prefer lyrics-enhanced segments if available)\n        segments_file = self._stage_path(&quot;lyrics_detection&quot;) / &quot;segments.json&quot;\n        if not segments_file.exists():\n            segments_file = self.job_dir / &quot;lyrics_detection&quot; / &quot;segments.json&quot;\n        if not segments_file.exists():\n            segments_file = self._stage_path(&quot;asr&quot;) / &quot;segments.json&quot;\n        \n        if not segments_file.exists():\n            self.logger.error(f&quot;Segments file not found: {segments_file}&quot;)\n            return False\n        \n        # Output to translation stage directory\n        output_dir = self._stage_path(&quot;translation&quot;)\n        output_dir.mkdir(parents=True, exist_ok=True)\n        output_file = output_dir / f&quot;segments_{target_lang}.json&quot;\n        \n        # Log input/output\n        self.logger.info(f&quot;\U0001f4e5 Input: {segments_file.relative_to(self.job_dir)}&quot;)\n        self.logger.info(f&quot;\U0001f4e4 Output: {output_file.relative_to(self.job_dir)}&quot;)\n        \n        try:\n            # Get Python executable from LLM environment\n            python_exe = self.env_manager.get_python_executable(&quot;llm&quot;)\n            self.logger.info(f&quot;Using LLM environment: {python_exe}&quot;)\n            \n            # Build environment variables\n            env = os.environ.copy()\n            env[\'OUTPUT_DIR\'] = str(self.job_dir)  # CRITICAL: Tell script where job directory is\n            env[\'CONFIG_PATH\'] = str(self.job_dir / f&quot;.{self.job_config[\'job_id\']}.env&quot;)\n            env[\'SOURCE_LANG\'] = source_lang\n            env[\'TARGET_LANG\'] = target_lang\n            env[\'USE_LLM_FOR_SONGS\'] = str(use_llm_for_songs).lower()\n            env[\'LLM_PROVIDER\'] = llm_provider\n            env[\'DEBUG_MODE\'] = \'true\' if self.debug else \'false\'\n            env[\'LOG_LEVEL\'] = \'DEBUG\' if self.debug else \'INFO\'\n            env[\'SEGMENTS_FILE\'] = str(segments_file)\n            env[\'OUTPUT_FILE\'] = str(output_file)\n            env[\'JOB_DIR\'] = str(self.job_dir)\n            \n            if film_title:\n                env[\'FILM_TITLE\'] = film_title\n            if film_year:\n                env[\'FILM_YEAR\'] = str(film_year)\n            \n            # Pass glossary snapshot if available\n            if hasattr(self, \'glossary_manager\') and self.glossary_manager:\n                glossary_snapshot = self._stage_path(&quot;glossary_load&quot;) / &quot;glossary_snapshot.json&quot;\n                if glossary_snapshot.exists():\n                    env[\'GLOSSARY_SNAPSHOT\'] = str(glossary_snapshot)\n                    self.logger.info(f&quot;Using glossary snapshot for translation&quot;)\n            \n            # Run hybrid translator\n            script_path = PROJECT_ROOT / &quot;scripts&quot; / &quot;hybrid_translator.py&quot;\n            \n            self.logger.info(f&quot;Running: {script_path}&quot;)\n            \n            result = subprocess.run(\n                [str(python_exe), str(script_path)],\n                capture_output=True,\n                text=True,\n                check=True,\n                cwd=str(PROJECT_ROOT),\n                env=env\n            )\n            \n            if self.debug and result.stdout:\n                self.logger.debug(f&quot;Hybrid translation output:\\n{result.stdout}&quot;)\n            \n            if output_file.exists():\n                # Load and report statistics\n                with open(output_file, \'r\', encoding=\'utf-8\') as f:\n                    raw_data = json.load(f)\n                \n                data, segments = normalize_segments_data(raw_data)\n                \n                stats = data.get(\'translation_stats\', {})\n                if stats:\n                    self.logger.info(f&quot;Translation statistics:&quot;)\n                    self.logger.info(f&quot;  Total segments: {stats.get(\'total_segments\', 0)}&quot;)\n                    self.logger.info(f&quot;  Dialogue segments: {stats.get(\'dialogue_segments\', 0)}&quot;)\n                    self.logger.info(f&quot;  Song segments: {stats.get(\'song_segments\', 0)}&quot;)\n                    self.logger.info(f&quot;  IndicTrans2 used: {stats.get(\'indictrans2_used\', 0)}&quot;)\n                    self.logger.info(f&quot;  LLM used: {stats.get(\'llm_used\', 0)}&quot;)\n                \n                # Apply glossary post-processing if available\n                if hasattr(self, \'glossary_manager\') and self.glossary_manager:\n                    try:\n                        glossary_applied_count = 0\n                        for segment in segments:\n                            if \'text\' in segment:\n                                original_text = segment[\'text\']\n                                polished_text = self.glossary_manager.apply_to_text(\n                                    original_text,\n                                    context=&quot;translation&quot;\n                                )\n                                if polished_text != original_text:\n                                    segment[\'text\'] = polished_text\n                                    glossary_applied_count += 1\n                        \n                        if glossary_applied_count &gt; 0:\n                            # Save the glossary-enhanced version\n                            with open(output_file, \'w\', encoding=\'utf-8\') as f:\n                                json.dump(data, f, indent=2, ensure_ascii=False)\n                            self.logger.info(f&quot;\u2713 Glossary applied to {glossary_applied_count} segments&quot;)\n                    except Exception as e:\n                        self.logger.warning(f&quot;Failed to apply glossary: {e}&quot;)\n                \n                # Copy to transcripts/ for compatibility\n                transcripts_dir = self.job_dir / &quot;transcripts&quot;\n                transcripts_dir.mkdir(parents=True, exist_ok=True)\n                import shutil\n                shutil.copy2(output_file, transcripts_dir / &quot;segments_translated.json&quot;)\n                \n                self.logger.info(f&quot;\u2713 Hybrid translation completed: {output_file.relative_to(self.job_dir)}&quot;)\n                self.logger.info(f&quot;\u2713 Copied to: transcripts/segments_translated.json&quot;)\n                return True\n            else:\n                self.logger.error(&quot;Hybrid translation failed - no output file&quot;)\n                return False\n                \n        except subprocess.CalledProcessError as e:\n            self.logger.error(f&quot;Hybrid translation error: {e.stderr}&quot;, exc_info=True)\n            self.logger.warning(&quot;Falling back to standard IndicTrans2&quot;)\n            return self._stage_indictrans2_translation()\n        except Exception as e:\n            self.logger.error(f&quot;Unexpected error in hybrid translation: {e}&quot;, exc_info=True)\n            if self.debug:\n                import traceback\n                self.logger.debug(traceback.format_exc())\n            self.logger.warning(&quot;Falling back to standard IndicTrans2&quot;)\n            return self._stage_indictrans2_translation()\n    \n    def _stage_indictrans2_translation(self) -&gt; bool:\n        &quot;&quot;&quot;Stage 7: Translate using IndicTrans2&quot;&quot;&quot;\n        \n        # Input from lyrics detection (if available) or ASR\n        segments_file = self._stage_path(&quot;lyrics_detection&quot;) / &quot;segments.json&quot;\n        if not segments_file.exists():\n            segments_file = self.job_dir / &quot;lyrics_detection&quot; / &quot;segments.json&quot;\n        if not segments_file.exists():\n            segments_file = self._stage_path(&quot;asr&quot;) / &quot;segments.json&quot;\n        \n        if not segments_file.exists():\n            self.logger.error(f&quot;Segments file not found&quot;)\n            return False\n        \n        source_lang = self.job_config[&quot;source_language&quot;]\n        target_lang = self._get_target_language()\n        \n        # Output to translation stage directory\n        output_dir = self._stage_path(&quot;translation&quot;)\n        output_dir.mkdir(parents=True, exist_ok=True)\n        output_file = output_dir / f&quot;segments_{target_lang}.json&quot;\n        \n        # Log input/output\n        self.logger.info(f&quot;\U0001f4e5 Input: {segments_file.relative_to(self.job_dir)}&quot;)\n        self.logger.info(f&quot;\U0001f4e4 Output: {output_file.relative_to(self.job_dir)}&quot;)\n        self.logger.info(&quot;Translating with IndicTrans2...&quot;)\n        \n        # Get configuration from job\'s .env file (set by prepare-job)\n        device = self.env_config.get(&quot;INDICTRANS2_DEVICE&quot;, self.main_config.indictrans2_device)\n        num_beams = self.env_config.get(&quot;INDICTRANS2_NUM_BEAMS&quot;, &quot;4&quot;)\n        max_tokens = self.env_config.get(&quot;INDICTRANS2_MAX_NEW_TOKENS&quot;, &quot;128&quot;)\n        \n        # Dynamically select model based on language pair (no hardcoding)\n        # Model selection happens in indictrans2_translator.py based on source/target\n        \n        self.logger.info(f&quot;Using IndicTrans2 device: {device} (from job config)&quot;)\n        self.logger.info(f&quot;Translation: {source_lang} \u2192 {target_lang}&quot;)\n        self.logger.info(f&quot;Num beams: {num_beams} (from job config)&quot;)\n        \n        # Use existing IndicTrans2 translator in venv/indictrans2\n        # Get Python executable from IndicTrans2 environment\n        python_exe = self.env_manager.get_python_executable(&quot;indictrans2&quot;)\n        self.logger.info(f&quot;Using IndicTrans2 environment: {python_exe}&quot;)\n        \n        cmd = [\n            str(python_exe), &quot;-c&quot;,\n            f&quot;&quot;&quot;\nimport json\nfrom pathlib import Path\nfrom scripts.indictrans2_translator import translate_whisperx_result\n\n# Load segments\nwith open(\'{segments_file}\') as f:\n    segments = json.load(f)\n\n# Translate with job-configured settings\nfrom shared.logger import PipelineLogger\nlog_file = Path(\'{self.job_dir / &quot;logs&quot;}\') / \'indictrans2_translation.log\'\nlogger = PipelineLogger(module_name=\'indictrans2\', log_file=log_file, log_level=\'{&quot;DEBUG&quot; if self.debug else &quot;INFO&quot;}\')\n\n# Set device for IndicTrans2 (from job config)\nimport os\nos.environ[\'INDICTRANS2_DEVICE\'] = \'{device}\'\nos.environ[\'INDICTRANS2_NUM_BEAMS\'] = \'{num_beams}\'\nos.environ[\'INDICTRANS2_MAX_NEW_TOKENS\'] = \'{max_tokens}\'\n\n# translate_whisperx_result will auto-select the right model based on language pair\ntranslated = translate_whisperx_result(segments, \'{source_lang}\', \'{target_lang}\', logger)\n\n# Save\nwith open(\'{output_file}\', \'w\') as f:\n    json.dump(translated, f, indent=2)\n\nlogger.info(f&quot;Translated {{len(translated[\'segments\'])}} segments&quot;)\n&quot;&quot;&quot;\n        ]\n        \n        try:\n            # Set up environment with debug flag\n            env = os.environ.copy()\n            env[\'DEBUG_MODE\'] = \'true\' if self.debug else \'false\'\n            env[\'LOG_LEVEL\'] = \'DEBUG\' if self.debug else \'INFO\'\n            \n            # Pass glossary snapshot if available\n            if hasattr(self, \'glossary_manager\') and self.glossary_manager:\n                glossary_snapshot = self._stage_path(&quot;glossary_load&quot;) / &quot;glossary_snapshot.json&quot;\n                if glossary_snapshot.exists():\n                    env[\'GLOSSARY_SNAPSHOT\'] = str(glossary_snapshot)\n                    self.logger.info(f&quot;Using glossary snapshot for IndicTrans2 translation&quot;)\n            \n            result = subprocess.run(\n                cmd,\n                capture_output=True,\n                text=True,\n                check=True,\n                env=env\n            )\n            \n            if output_file.exists():\n                # Apply glossary post-processing if available\n                if hasattr(self, \'glossary_manager\') and self.glossary_manager:\n                    try:\n                        with open(output_file, \'r\', encoding=\'utf-8\') as f:\n                            raw_data = json.load(f)\n                        \n                        data, segments = normalize_segments_data(raw_data)\n                        \n                        glossary_applied_count = 0\n                        for segment in segments:\n                            if \'text\' in segment:\n                                original_text = segment[\'text\']\n                                polished_text = self.glossary_manager.apply_to_text(\n                                    original_text,\n                                    context=&quot;translation&quot;\n                                )\n                                if polished_text != original_text:\n                                    segment[\'text\'] = polished_text\n                                    glossary_applied_count += 1\n                        \n                        if glossary_applied_count &gt; 0:\n                            # Save the glossary-enhanced version\n                            with open(output_file, \'w\', encoding=\'utf-8\') as f:\n                                json.dump(data, f, indent=2, ensure_ascii=False)\n                            self.logger.info(f&quot;\u2713 Glossary applied to {glossary_applied_count} segments&quot;)\n                    except Exception as e:\n                        self.logger.warning(f&quot;Failed to apply glossary: {e}&quot;)\n                \n                # Copy to transcripts/ for compatibility\n                transcripts_dir = self.job_dir / &quot;transcripts&quot;\n                transcripts_dir.mkdir(parents=True, exist_ok=True)\n                import shutil\n                shutil.copy2(output_file, transcripts_dir / &quot;segments_translated.json&quot;)\n                \n                self.logger.info(f&quot;\u2713 Translation completed: {output_file.relative_to(self.job_dir)}&quot;)\n                self.logger.info(f&quot;\u2713 Copied to: transcripts/segments_translated.json&quot;)\n                return True\n            else:\n                self.logger.error(&quot;Translation failed&quot;)\n                return False\n                \n        except subprocess.CalledProcessError as e:\n            self.logger.error(f&quot;Translation error: {e.stderr}&quot;, exc_info=True)\n            return False\n    \n    def _stage_subtitle_generation(self) -&gt; bool:\n        &quot;&quot;&quot;Stage 8: Generate SRT subtitle file in target language&quot;&quot;&quot;\n        \n        target_lang = self._get_target_language()\n        \n        # Read from translation stage\n        segments_file = self._stage_path(&quot;translation&quot;) / f&quot;segments_{target_lang}.json&quot;\n        \n        if not segments_file.exists():\n            # Fallback to old location\n            segments_file = self.job_dir / &quot;transcripts&quot; / &quot;segments_translated.json&quot;\n        \n        if not segments_file.exists():\n            self.logger.error(f&quot;Translated segments not found: {segments_file}&quot;)\n            return False\n        \n        # Output to subtitle generation stage directory\n        output_dir = self._stage_path(&quot;subtitle_generation&quot;)\n        output_dir.mkdir(parents=True, exist_ok=True)\n        \n        # Generate filename\n        title = self.job_config.get(&quot;title&quot;, &quot;output&quot;)\n        output_srt = output_dir / f&quot;{title}.{target_lang}.srt&quot;\n        \n        # Log input/output\n        self.logger.info(f&quot;\U0001f4e5 Input: {segments_file.relative_to(self.job_dir)}&quot;)\n        self.logger.info(f&quot;\U0001f4e4 Output: {output_srt.relative_to(self.job_dir)}&quot;)\n        self.logger.info(&quot;Generating subtitles...&quot;)\n        \n        # Load translated segments\n        try:\n            with open(segments_file, \'r\', encoding=\'utf-8\') as f:\n                raw_data = json.load(f)\n                data, segments = normalize_segments_data(raw_data)\n        except Exception as e:\n            self.logger.error(f&quot;Failed to load segments: {e}&quot;, exc_info=True)\n            return False\n        \n        # Generate SRT file\n        if generate_srt_from_segments(segments, output_srt):\n            # Copy to subtitles/ for compatibility\n            subtitles_dir = self.job_dir / &quot;subtitles&quot;\n            subtitles_dir.mkdir(parents=True, exist_ok=True)\n            final_output = subtitles_dir / output_srt.name\n            \n            # Only copy if source and destination are different\n            if output_srt != final_output:\n                import shutil\n                shutil.copy2(output_srt, final_output)\n                self.logger.info(f&quot;\u2713 Subtitles generated: {output_srt.relative_to(self.job_dir)}&quot;)\n                self.logger.info(f&quot;\u2713 Copied to: subtitles/{output_srt.name}&quot;)\n            else:\n                self.logger.info(f&quot;\u2713 Subtitles generated: {output_srt.relative_to(self.job_dir)}&quot;)\n            \n            return True\n        else:\n            self.logger.error(&quot;Subtitle generation failed&quot;)\n            return False\n    \n    def _stage_subtitle_generation_source(self) -&gt; bool:\n        &quot;&quot;&quot;Stage 8: Generate SRT subtitle file in source language&quot;&quot;&quot;\n        \n        source_lang = self.job_config[&quot;source_language&quot;]\n        \n        # Read from ASR stage (or transcripts copy)\n        segments_file = self._stage_path(&quot;asr&quot;) / &quot;segments.json&quot;\n        if not segments_file.exists():\n            segments_file = self.job_dir / &quot;transcripts&quot; / &quot;segments.json&quot;\n        \n        if not segments_file.exists():\n            self.logger.error(f&quot;Segments not found: {segments_file}&quot;)\n            return False\n        \n        # Output to subtitle generation stage directory\n        output_dir = self._stage_path(&quot;subtitle_generation&quot;)\n        output_dir.mkdir(parents=True, exist_ok=True)\n        \n        # Generate filename\n        title = self.job_config.get(&quot;title&quot;, &quot;output&quot;)\n        output_srt = output_dir / f&quot;{title}.{source_lang}.srt&quot;\n        \n        # Log input/output\n        self.logger.info(f&quot;\U0001f4e5 Input: {segments_file.relative_to(self.job_dir)}&quot;)\n        self.logger.info(f&quot;\U0001f4e4 Output: {output_srt.relative_to(self.job_dir)}&quot;)\n        self.logger.info(&quot;Generating source language subtitles...&quot;)\n        \n        # Load segments\n        try:\n            with open(segments_file, \'r\', encoding=\'utf-8\') as f:\n                raw_data = json.load(f)\n                data, segments = normalize_segments_data(raw_data)\n        except Exception as e:\n            self.logger.error(f&quot;Failed to load segments: {e}&quot;, exc_info=True)\n            return False\n        \n        # Generate SRT file\n        if generate_srt_from_segments(segments, output_srt):\n            # Copy to subtitles/ for compatibility\n            subtitles_dir = self.job_dir / &quot;subtitles&quot;\n            subtitles_dir.mkdir(parents=True, exist_ok=True)\n            final_output = subtitles_dir / output_srt.name\n            \n            # Only copy if source and destination are different\n            if output_srt != final_output:\n                import shutil\n                shutil.copy2(output_srt, final_output)\n                self.logger.info(f&quot;\u2713 Source subtitles generated: {output_srt.relative_to(self.job_dir)}&quot;)\n                self.logger.info(f&quot;\u2713 Copied to: subtitles/{output_srt.name}&quot;)\n            else:\n                self.logger.info(f&quot;\u2713 Source subtitles generated: {output_srt.relative_to(self.job_dir)}&quot;)\n            \n            return True\n        else:\n            self.logger.error(&quot;Source subtitle generation failed&quot;)\n            return False\n    \n    def _stage_hybrid_translation_multi(self, target_lang: str) -&gt; bool:\n        &quot;&quot;&quot;\n        Stage: Hybrid translation for multiple target languages (subtitle workflow)\n        \n        Wrapper around _stage_hybrid_translation for multi-language subtitle workflow.\n        Temporarily updates job_config with current target language.\n        &quot;&quot;&quot;\n        # Temporarily set target language for this translation\n        original_target = self._get_target_language()\n        # Set both formats for compatibility\n        self.job_config[&quot;target_language&quot;] = target_lang\n        if &quot;target_languages&quot; in self.job_config:\n            self.job_config[&quot;target_languages&quot;] = [target_lang]\n        \n        try:\n            # Call main hybrid translation stage\n            result = self._stage_hybrid_translation()\n            \n            # If successful, rename output file to include language code\n            if result:\n                generic_output = self.job_dir / &quot;transcripts&quot; / &quot;segments_translated.json&quot;\n                lang_specific_output = self.job_dir / &quot;transcripts&quot; / f&quot;segments_translated_{target_lang}.json&quot;\n                \n                if generic_output.exists():\n                    # Copy to language-specific file\n                    import shutil\n                    shutil.copy2(generic_output, lang_specific_output)\n                    self.logger.info(f&quot;\u2713 Translation saved: {lang_specific_output.name}&quot;)\n            \n            return result\n            \n        finally:\n            # Restore original target language\n            if original_target:\n                self.job_config[&quot;target_language&quot;] = original_target\n                if &quot;target_languages&quot; in self.job_config:\n                    self.job_config[&quot;target_languages&quot;] = [original_target]\n    \n    def _stage_indictrans2_translation_multi(self, target_lang: str) -&gt; bool:\n        &quot;&quot;&quot;Translate to specific target language (for multi-language support)&quot;&quot;&quot;\n        self.logger.info(f&quot;Translating to {target_lang.upper()}...&quot;)\n        \n        segments_file = self.job_dir / &quot;transcripts&quot; / &quot;segments.json&quot;\n        output_file = self.job_dir / &quot;transcripts&quot; / f&quot;segments_translated_{target_lang}.json&quot;\n        \n        source_lang = self.job_config[&quot;source_language&quot;]\n        \n        # Get configuration from job\'s .env file\n        device = self.env_config.get(&quot;INDICTRANS2_DEVICE&quot;, self.main_config.indictrans2_device)\n        num_beams = self.env_config.get(&quot;INDICTRANS2_NUM_BEAMS&quot;, &quot;4&quot;)\n        max_tokens = self.env_config.get(&quot;INDICTRANS2_MAX_NEW_TOKENS&quot;, &quot;128&quot;)\n        \n        self.logger.info(f&quot;Using IndicTrans2 device: {device} (from job config)&quot;)\n        self.logger.info(f&quot;Translation: {source_lang} \u2192 {target_lang}&quot;)\n        \n        # Use existing IndicTrans2 translator in venv/indictrans2\n        # Get Python executable from IndicTrans2 environment\n        python_exe = self.env_manager.get_python_executable(&quot;indictrans2&quot;)\n        self.logger.info(f&quot;Using IndicTrans2 environment: {python_exe}&quot;)\n        \n        cmd = [\n            str(python_exe), &quot;-c&quot;,\n            f&quot;&quot;&quot;\nimport json\nfrom pathlib import Path\nfrom scripts.indictrans2_translator import translate_whisperx_result\n\n# Load segments\nwith open(\'{segments_file}\') as f:\n    segments = json.load(f)\n\n# Translate with job-configured settings\nfrom shared.logger import PipelineLogger\nlog_file = Path(\'{self.job_dir / &quot;logs&quot;}\') / \'indictrans2_translation_{target_lang}.log\'\nlogger = PipelineLogger(module_name=\'indictrans2_{target_lang}\', log_file=log_file, log_level=\'{&quot;DEBUG&quot; if self.debug else &quot;INFO&quot;}\')\n\n# Set device for IndicTrans2 (from job config)\nimport os\nos.environ[\'INDICTRANS2_DEVICE\'] = \'{device}\'\nos.environ[\'INDICTRANS2_NUM_BEAMS\'] = \'{num_beams}\'\nos.environ[\'INDICTRANS2_MAX_NEW_TOKENS\'] = \'{max_tokens}\'\n\n# translate_whisperx_result will auto-select the right model based on language pair\ntranslated = translate_whisperx_result(segments, \'{source_lang}\', \'{target_lang}\', logger)\n\n# Save\nwith open(\'{output_file}\', \'w\') as f:\n    json.dump(translated, f, indent=2)\n\nlogger.info(f&quot;Translated {{len(translated[\'segments\'])}} segments to {target_lang}&quot;)\n&quot;&quot;&quot;\n        ]\n        \n        try:\n            stage_name = f&quot;indictrans2_translation_{target_lang}&quot;\n            \n            result = self._run_in_environment(\n                stage_name,\n                cmd,\n                capture_output=True,\n                text=True,\n                check=True\n            )\n            \n            if output_file.exists():\n                self.logger.info(f&quot;Translation to {target_lang.upper()} completed: {output_file}&quot;)\n                return True\n            else:\n                self.logger.error(f&quot;Translation to {target_lang} failed&quot;)\n                return False\n                \n        except subprocess.CalledProcessError as e:\n            self.logger.error(f&quot;Translation to {target_lang} error: {e.stderr}&quot;, exc_info=True)\n            return False\n    \n    def _stage_nllb_translation(self) -&gt; bool:\n        &quot;&quot;&quot;Stage 2 (translate): Translate using NLLB for non-Indic languages&quot;&quot;&quot;\n        self.logger.info(&quot;Translating with NLLB...&quot;)\n        \n        segments_file = self.job_dir / &quot;transcripts&quot; / &quot;segments.json&quot;\n        output_file = self.job_dir / &quot;transcripts&quot; / &quot;segments_translated.json&quot;\n        \n        source_lang = self.job_config[&quot;source_language&quot;]\n        target_lang = self._get_target_language()\n        \n        # Get configuration from job\'s .env file (set by prepare-job)\n        device = self.env_config.get(&quot;NLLB_DEVICE&quot;, &quot;mps&quot;)\n        model_size = self.env_config.get(&quot;NLLB_MODEL_SIZE&quot;, &quot;600M&quot;)\n        \n        # Model name based on size\n        model_map = {\n            &quot;600M&quot;: &quot;facebook/nllb-200-distilled-600M&quot;,\n            &quot;1.3B&quot;: &quot;facebook/nllb-200-1.3B&quot;,\n            &quot;3.3B&quot;: &quot;facebook/nllb-200-3.3B&quot;\n        }\n        model_name = model_map.get(model_size, &quot;facebook/nllb-200-distilled-600M&quot;)\n        \n        self.logger.info(f&quot;Using NLLB model: {model_name}&quot;)\n        self.logger.info(f&quot;Device: {device} (from job config)&quot;)\n        self.logger.info(f&quot;Translation: {source_lang} \u2192 {target_lang}&quot;)\n        \n        # Get Python executable from NLLB environment\n        python_exe = self.env_manager.get_python_executable(&quot;nllb&quot;)\n        self.logger.info(f&quot;Using NLLB environment: {python_exe}&quot;)\n        \n        cmd = [\n            str(python_exe), &quot;-c&quot;,\n            f&quot;&quot;&quot;\nimport json\nfrom pathlib import Path\nfrom scripts.nllb_translator import translate_whisperx_result, NLLBConfig\n\n# Load segments\nwith open(\'{segments_file}\') as f:\n    segments = json.load(f)\n\n# Setup logging\nfrom shared.logger import PipelineLogger\nlog_file = Path(\'{self.job_dir / &quot;logs&quot;}\') / \'nllb_translation.log\'\nlogger = PipelineLogger(module_name=\'nllb\', log_file=log_file, log_level=\'{&quot;DEBUG&quot; if self.debug else &quot;INFO&quot;}\')\n\n# Configure NLLB\nconfig = NLLBConfig(\n    model_name=\'{model_name}\',\n    device=\'{device}\'\n)\n\n# Translate\ntranslated = translate_whisperx_result(segments, \'{source_lang}\', \'{target_lang}\', logger: logging.Logger, config)\n\n# Save\nwith open(\'{output_file}\', \'w\') as f:\n    json.dump(translated, f, indent=2, ensure_ascii=False)\n\nlogger.info(f&quot;Translated {{len(translated[\'segments\'])}} segments&quot;)\n&quot;&quot;&quot;\n        ]\n        \n        try:\n            # Set up environment with debug flag\n            env = os.environ.copy()\n            env[\'DEBUG_MODE\'] = \'true\' if self.debug else \'false\'\n            env[\'LOG_LEVEL\'] = \'DEBUG\' if self.debug else \'INFO\'\n            \n            result = subprocess.run(\n                cmd,\n                capture_output=True,\n                text=True,\n                check=True,\n                cwd=str(PROJECT_ROOT),\n                env=env\n            )\n            \n            if output_file.exists():\n                self.logger.info(f&quot;Translation completed: {output_file}&quot;)\n                return True\n            else:\n                self.logger.error(&quot;Translation failed&quot;)\n                return False\n                \n        except subprocess.CalledProcessError as e:\n            self.logger.error(f&quot;Translation error: {e.stderr}&quot;, exc_info=True)\n            return False\n    \n    def _stage_nllb_translation_multi(self, target_lang: str) -&gt; bool:\n        &quot;&quot;&quot;Translate to specific target language using NLLB (for multi-language support)&quot;&quot;&quot;\n        self.logger.info(f&quot;Translating to {target_lang.upper()} with NLLB...&quot;)\n        \n        segments_file = self.job_dir / &quot;transcripts&quot; / &quot;segments.json&quot;\n        output_file = self.job_dir / &quot;transcripts&quot; / f&quot;segments_translated_{target_lang}.json&quot;\n        \n        source_lang = self.job_config[&quot;source_language&quot;]\n        \n        # Get configuration from job\'s .env file\n        device = self.env_config.get(&quot;NLLB_DEVICE&quot;, &quot;mps&quot;)\n        model_size = self.env_config.get(&quot;NLLB_MODEL_SIZE&quot;, &quot;600M&quot;)\n        \n        # Model name based on size\n        model_map = {\n            &quot;600M&quot;: &quot;facebook/nllb-200-distilled-600M&quot;,\n            &quot;1.3B&quot;: &quot;facebook/nllb-200-1.3B&quot;,\n            &quot;3.3B&quot;: &quot;facebook/nllb-200-3.3B&quot;\n        }\n        model_name = model_map.get(model_size, &quot;facebook/nllb-200-distilled-600M&quot;)\n        \n        self.logger.info(f&quot;Using NLLB model: {model_name}&quot;)\n        self.logger.info(f&quot;Device: {device}&quot;)\n        self.logger.info(f&quot;Translation: {source_lang} \u2192 {target_lang}&quot;)\n        \n        # Get Python executable from NLLB environment\n        python_exe = self.env_manager.get_python_executable(&quot;nllb&quot;)\n        self.logger.info(f&quot;Using NLLB environment: {python_exe}&quot;)\n        \n        cmd = [\n            str(python_exe), &quot;-c&quot;,\n            f&quot;&quot;&quot;\nimport json\nfrom pathlib import Path\nfrom scripts.nllb_translator import translate_whisperx_result, NLLBConfig\n\n# Load segments\nwith open(\'{segments_file}\') as f:\n    segments = json.load(f)\n\n# Setup logging\nfrom shared.logger import PipelineLogger\nlog_file = Path(\'{self.job_dir / &quot;logs&quot;}\') / \'nllb_{target_lang}_translation.log\'\nlogger = PipelineLogger(module_name=\'nllb_{target_lang}\', log_file=log_file, log_level=\'{&quot;DEBUG&quot; if self.debug else &quot;INFO&quot;}\')\n\n# Configure NLLB\nconfig = NLLBConfig(\n    model_name=\'{model_name}\',\n    device=\'{device}\'\n)\n\n# Translate\ntranslated = translate_whisperx_result(segments, \'{source_lang}\', \'{target_lang}\', logger: logging.Logger, config)\n\n# Save\nwith open(\'{output_file}\', \'w\') as f:\n    json.dump(translated, f, indent=2, ensure_ascii=False)\n\nlogger.info(f&quot;Translated {{len(translated[\'segments\'])}} segments to {target_lang}&quot;)\n&quot;&quot;&quot;\n        ]\n        \n        try:\n            # Set up environment with debug flag\n            env = os.environ.copy()\n            env[\'DEBUG_MODE\'] = \'true\' if self.debug else \'false\'\n            env[\'LOG_LEVEL\'] = \'DEBUG\' if self.debug else \'INFO\'\n            \n            result = subprocess.run(\n                cmd,\n                capture_output=True,\n                text=True,\n                check=True,\n                cwd=str(PROJECT_ROOT),\n                env=env\n            )\n            \n            if output_file.exists():\n                self.logger.info(f&quot;Translation to {target_lang.upper()} completed: {output_file}&quot;)\n                return True\n            else:\n                self.logger.error(f&quot;Translation to {target_lang} failed&quot;)\n                return False\n                \n        except subprocess.CalledProcessError as e:\n            self.logger.error(f&quot;Translation to {target_lang} error: {e.stderr}&quot;, exc_info=True)\n            return False\n    \n    def _stage_subtitle_generation_target_multi(self, target_lang: str) -&gt; bool:\n        &quot;&quot;&quot;Generate subtitle file for specific target language&quot;&quot;&quot;\n        self.logger.info(f&quot;Generating {target_lang.upper()} subtitles...&quot;)\n        \n        segments_file = self.job_dir / &quot;transcripts&quot; / f&quot;segments_translated_{target_lang}.json&quot;\n        \n        # Generate filename\n        title = self.job_config.get(&quot;title&quot;, &quot;output&quot;)\n        output_srt = self.job_dir / &quot;subtitles&quot; / f&quot;{title}.{target_lang}.srt&quot;\n        \n        # Load translated segments\n        try:\n            with open(segments_file, \'r\', encoding=\'utf-8\') as f:\n                raw_data = json.load(f)\n                data, segments = normalize_segments_data(raw_data)\n        except Exception as e:\n            self.logger.error(f&quot;Failed to load {target_lang} segments: {e}&quot;, exc_info=True)\n            return False\n        \n        # Generate SRT file\n        if generate_srt_from_segments(segments, output_srt):\n            self.logger.info(f&quot;{target_lang.upper()} subtitles generated: {output_srt}&quot;)\n            return True\n        else:\n            self.logger.error(f&quot;{target_lang} subtitle generation failed&quot;, exc_info=True)\n            return False\n    \n    def _stage_subtitle_generation_target(self) -&gt; bool:\n        &quot;&quot;&quot;Stage 3b (subtitle workflow): Generate SRT subtitle file in target language&quot;&quot;&quot;\n        self.logger.info(&quot;Generating target language subtitles...&quot;)\n        \n        segments_file = self.job_dir / &quot;transcripts&quot; / &quot;segments_translated.json&quot;\n        target_lang = self._get_target_language()\n        \n        # Generate filename\n        title = self.job_config.get(&quot;title&quot;, &quot;output&quot;)\n        output_srt = self.job_dir / &quot;subtitles&quot; / f&quot;{title}.{target_lang}.srt&quot;\n        \n        # Load translated segments\n        try:\n            with open(segments_file, \'r\', encoding=\'utf-8\') as f:\n                raw_data = json.load(f)\n                data, segments = normalize_segments_data(raw_data)\n        except Exception as e:\n            self.logger.error(f&quot;Failed to load segments: {e}&quot;, exc_info=True)\n            return False\n        \n        # Generate SRT file\n        if generate_srt_from_segments(segments, output_srt):\n            self.logger.info(f&quot;Target subtitles generated: {output_srt}&quot;)\n            return True\n        else:\n            self.logger.error(&quot;Target subtitle generation failed&quot;, exc_info=True)\n            return False\n    \n    def _stage_hinglish_detection(self) -&gt; bool:\n        &quot;&quot;&quot;Stage 3b (subtitle workflow): Detect and tag word-level languages in Hinglish subtitles&quot;&quot;&quot;\n        self.logger.info(&quot;Running Hinglish word-level language detection...&quot;)\n        \n        source_lang = self.job_config[&quot;source_language&quot;]\n        title = self.job_config.get(&quot;title&quot;, &quot;output&quot;)\n        \n        # Source subtitle file\n        source_srt = self.job_dir / &quot;subtitles&quot; / f&quot;{title}.{source_lang}.srt&quot;\n        \n        if not source_srt.exists():\n            self.logger.warning(f&quot;Source subtitle not found: {source_srt}&quot;)\n            self.logger.warning(&quot;Skipping Hinglish detection&quot;)\n            return True  # Not a failure, just skip\n        \n        # Output files\n        tagged_srt = self.job_dir / &quot;subtitles&quot; / f&quot;{title}.{source_lang}.tagged.srt&quot;\n        analysis_json = self.job_dir / &quot;subtitles&quot; / f&quot;{title}.{source_lang}.analysis.json&quot;\n        \n        self.logger.info(f&quot;Analyzing: {source_srt}&quot;)\n        \n        # Get Python executable from common environment\n        python_exe = self.env_manager.get_python_executable(&quot;common&quot;)\n        \n        # Run hinglish detector\n        cmd = [\n            str(python_exe),\n            str(self.scripts_dir / &quot;hinglish_word_detector.py&quot;),\n            str(source_srt),\n            &quot;-o&quot;, str(tagged_srt),\n            &quot;-j&quot;, str(analysis_json)\n        ]\n        \n        if self.debug:\n            cmd.append(&quot;-v&quot;)\n        \n        try:\n            result = subprocess.run(\n                cmd,\n                capture_output=True,\n                text=True,\n                check=True\n            )\n            \n            self.logger.info(f&quot;\u2713 Tagged SRT created: {tagged_srt}&quot;)\n            self.logger.info(f&quot;\u2713 Analysis JSON created: {analysis_json}&quot;)\n            \n            # Parse and log statistics from output\n            if &quot;Hindi words:&quot; in result.stdout:\n                for line in result.stdout.split(\'\\n\'):\n                    if \'words:\' in line.lower() or \'Total\' in line:\n                        self.logger.info(f&quot;  {line.strip()}&quot;)\n            \n            return True\n            \n        except subprocess.CalledProcessError as e:\n            self.logger.error(f&quot;Hinglish detection failed: {e.stderr}&quot;, exc_info=True)\n            self.logger.warning(&quot;Continuing without Hinglish detection...&quot;)\n            return True  # Don\'t fail the pipeline, just warn\n    \n    def _stage_mux(self) -&gt; bool:\n        &quot;&quot;&quot;Stage 9: Mux video with multiple subtitle tracks (up to 5)&quot;&quot;&quot;\n        \n        # Get configuration\n        input_media = Path(self.job_config[&quot;input_media&quot;])\n        title = self.job_config.get(&quot;title&quot;, &quot;output&quot;)\n        source_lang = self.job_config[&quot;source_language&quot;]\n        target_languages = self.job_config.get(&quot;target_languages&quot;, [])\n        \n        # Get media processing configuration for clipping\n        media_config = self.job_config.get(&quot;media_processing&quot;, {})\n        processing_mode = media_config.get(&quot;mode&quot;, &quot;full&quot;)\n        start_time = media_config.get(&quot;start_time&quot;, &quot;&quot;)\n        end_time = media_config.get(&quot;end_time&quot;, &quot;&quot;)\n        \n        # Collect all subtitle files (target languages + source)\n        # Try from 08_subtitle_generation first, fallback to subtitles/\n        subtitle_files = []\n        subtitle_langs = []\n        \n        subtitle_dir = self._stage_path(&quot;subtitle_generation&quot;)\n        fallback_dir = self.job_dir / &quot;subtitles&quot;\n        \n        # Add target language subtitles\n        for target_lang in target_languages:\n            target_srt = subtitle_dir / f&quot;{title}.{target_lang}.srt&quot;\n            if not target_srt.exists():\n                target_srt = fallback_dir / f&quot;{title}.{target_lang}.srt&quot;\n            \n            if not target_srt.exists():\n                self.logger.error(f&quot;Target subtitle not found: {target_lang}&quot;)\n                return False\n            subtitle_files.append(target_srt)\n            subtitle_langs.append(target_lang)\n        \n        # Add source language subtitle\n        source_srt = subtitle_dir / f&quot;{title}.{source_lang}.srt&quot;\n        if not source_srt.exists():\n            source_srt = fallback_dir / f&quot;{title}.{source_lang}.srt&quot;\n        \n        if not source_srt.exists():\n            self.logger.error(f&quot;Source subtitle not found: {source_lang}&quot;)\n            return False\n        subtitle_files.append(source_srt)\n        subtitle_langs.append(source_lang)\n        \n        # Output directory\n        output_dir = self.job_dir / &quot;10_mux&quot;\n        output_dir.mkdir(parents=True, exist_ok=True)\n        \n        # Log input/output\n        self.logger.info(f&quot;\U0001f4e5 Input video: {input_media.name}&quot;)\n        for i, (sub_file, lang) in enumerate(zip(subtitle_files, subtitle_langs), 1):\n            self.logger.info(f&quot;\U0001f4e5 Input subtitle {i}: {sub_file.relative_to(self.job_dir)} ({lang})&quot;)\n        \n        self.logger.info(f&quot;Muxing video with {len(subtitle_files)} subtitle tracks: {\', \'.join(subtitle_langs)}&quot;)\n        \n        # Detect source file extension and use same format for output\n        source_ext = input_media.suffix.lower()  # e.g., \'.mp4\', \'.mkv\', \'.avi\'\n        \n        # Determine output format\n        if source_ext in [\'.mp4\', \'.m4v\']:\n            output_ext = \'.mp4\'\n            subtitle_codec = \'mov_text\'  # MP4 subtitle format\n        elif source_ext in [\'.mkv\', \'.webm\']:\n            output_ext = \'.mkv\'\n            subtitle_codec = \'srt\'  # MKV subtitle format\n        elif source_ext in [\'.avi\']:\n            output_ext = \'.mkv\'  # AVI doesn\'t support subtitle tracks well, use MKV\n            subtitle_codec = \'srt\'\n            self.logger.info(f&quot;Source is AVI, using MKV for subtitle support&quot;)\n        else:\n            # Default to MKV for unknown formats (best subtitle support)\n            output_ext = \'.mkv\'\n            subtitle_codec = \'srt\'\n            self.logger.info(f&quot;Unknown format {source_ext}, using MKV for subtitle support&quot;)\n        \n        # Output video file in 09_mux directory\n        output_video = output_dir / f&quot;{title}_subtitled{output_ext}&quot;\n        \n        # Also create copy in media subdirectory for user convenience\n        media_name = input_media.stem\n        media_output_subdir = self.job_dir / &quot;media&quot; / media_name\n        media_output_subdir.mkdir(parents=True, exist_ok=True)\n        media_output_video = media_output_subdir / f&quot;{title}_subtitled{output_ext}&quot;\n        \n        self.logger.info(f&quot;\U0001f4e4 Output: {output_video.relative_to(self.job_dir)}&quot;)\n        self.logger.info(f&quot;Output format: {output_ext} (source: {source_ext})&quot;)\n        \n        # Build ffmpeg command\n        cmd = [&quot;ffmpeg&quot;, &quot;-y&quot;]\n        \n        # Add log level based on debug mode\n        if not self.debug:\n            cmd.extend([&quot;-loglevel&quot;, &quot;error&quot;])\n        \n        # Add clipping if configured\n        if processing_mode == &quot;clip&quot; and start_time:\n            cmd.extend([&quot;-ss&quot;, start_time])\n        \n        # Add input files - video first, then all subtitles\n        cmd.extend([&quot;-i&quot;, str(input_media)])  # Video input (index 0)\n        for sub_file in subtitle_files:\n            cmd.extend([&quot;-i&quot;, str(sub_file)])  # Subtitle inputs (indices 1, 2, 3...)\n        \n        # Add end time if clipping\n        if processing_mode == &quot;clip&quot; and end_time:\n            cmd.extend([&quot;-to&quot;, end_time])\n        \n        # Map streams: video, audio, all subtitles\n        cmd.extend([\n            &quot;-map&quot;, &quot;0:v&quot;,  # Video from input 0\n            &quot;-map&quot;, &quot;0:a&quot;,  # Audio from input 0\n        ])\n        \n        # Map all subtitle files\n        for i in range(len(subtitle_files)):\n            cmd.extend([&quot;-map&quot;, str(i + 1)])  # Subtitle from input 1, 2, 3...\n        \n        # Copy codecs (no re-encoding)\n        cmd.extend([&quot;-c&quot;, &quot;copy&quot;])\n        \n        # Set subtitle codec based on output format\n        cmd.extend([&quot;-c:s&quot;, subtitle_codec])\n        \n        # Add metadata for each subtitle track\n        # Map 2-letter codes to ISO 639-2 (3-letter) for better player compatibility\n        lang_map_iso639_2 = {\n            &quot;hi&quot;: &quot;hin&quot;,  # Hindi\n            &quot;en&quot;: &quot;eng&quot;,  # English\n            &quot;gu&quot;: &quot;guj&quot;,  # Gujarati\n            &quot;ta&quot;: &quot;tam&quot;,  # Tamil\n            &quot;te&quot;: &quot;tel&quot;,  # Telugu\n            &quot;bn&quot;: &quot;ben&quot;,  # Bengali\n            &quot;mr&quot;: &quot;mar&quot;,  # Marathi\n            &quot;kn&quot;: &quot;kan&quot;,  # Kannada\n            &quot;ml&quot;: &quot;mal&quot;,  # Malayalam\n            &quot;pa&quot;: &quot;pan&quot;,  # Punjabi\n            &quot;ur&quot;: &quot;urd&quot;,  # Urdu\n            &quot;as&quot;: &quot;asm&quot;,  # Assamese\n            &quot;or&quot;: &quot;ori&quot;,  # Odia\n            &quot;ne&quot;: &quot;nep&quot;,  # Nepali\n            &quot;sd&quot;: &quot;snd&quot;,  # Sindhi\n            &quot;si&quot;: &quot;sin&quot;,  # Sinhala\n            &quot;sa&quot;: &quot;san&quot;,  # Sanskrit\n        }\n        \n        # Map to full language names for display\n        lang_names = {\n            &quot;hin&quot;: &quot;Hindi&quot;, &quot;hi&quot;: &quot;Hindi&quot;,\n            &quot;eng&quot;: &quot;English&quot;, &quot;en&quot;: &quot;English&quot;,\n            &quot;guj&quot;: &quot;Gujarati&quot;, &quot;gu&quot;: &quot;Gujarati&quot;,\n            &quot;tam&quot;: &quot;Tamil&quot;, &quot;ta&quot;: &quot;Tamil&quot;,\n            &quot;tel&quot;: &quot;Telugu&quot;, &quot;te&quot;: &quot;Telugu&quot;,\n            &quot;ben&quot;: &quot;Bengali&quot;, &quot;bn&quot;: &quot;Bengali&quot;,\n            &quot;mar&quot;: &quot;Marathi&quot;, &quot;mr&quot;: &quot;Marathi&quot;,\n            &quot;kan&quot;: &quot;Kannada&quot;, &quot;kn&quot;: &quot;Kannada&quot;,\n            &quot;mal&quot;: &quot;Malayalam&quot;, &quot;ml&quot;: &quot;Malayalam&quot;,\n            &quot;pan&quot;: &quot;Punjabi&quot;, &quot;pa&quot;: &quot;Punjabi&quot;,\n            &quot;urd&quot;: &quot;Urdu&quot;, &quot;ur&quot;: &quot;Urdu&quot;,\n            &quot;asm&quot;: &quot;Assamese&quot;, &quot;as&quot;: &quot;Assamese&quot;,\n            &quot;ori&quot;: &quot;Odia&quot;, &quot;or&quot;: &quot;Odia&quot;,\n            &quot;nep&quot;: &quot;Nepali&quot;, &quot;ne&quot;: &quot;Nepali&quot;,\n            &quot;snd&quot;: &quot;Sindhi&quot;, &quot;sd&quot;: &quot;Sindhi&quot;,\n            &quot;sin&quot;: &quot;Sinhala&quot;, &quot;si&quot;: &quot;Sinhala&quot;,\n            &quot;san&quot;: &quot;Sanskrit&quot;, &quot;sa&quot;: &quot;Sanskrit&quot;,\n        }\n        \n        for i, lang in enumerate(subtitle_langs):\n            # Convert to ISO 639-2 (3-letter code)\n            lang_iso = lang_map_iso639_2.get(lang, lang)\n            # Get full language name\n            lang_title = lang_names.get(lang, lang.upper())\n            \n            cmd.extend([\n                &quot;-metadata:s:s:&quot; + str(i), f&quot;language={lang_iso}&quot;,\n                &quot;-metadata:s:s:&quot; + str(i), f&quot;title={lang_title}&quot;,\n            ])\n        \n        # Output file\n        cmd.append(str(output_video))\n        \n        mode_str = f&quot;clipped ({start_time} to {end_time})&quot; if processing_mode == &quot;clip&quot; else &quot;full&quot;\n        self.logger.info(f&quot;Creating {mode_str} video with {len(subtitle_files)} subtitle tracks...&quot;)\n        for i, lang in enumerate(subtitle_langs):\n            lang_iso = lang_map_iso639_2.get(lang, lang)\n            lang_title = lang_names.get(lang, lang.upper())\n            self.logger.info(f&quot;  \u2022 Track {i}: {lang_title} ({lang_iso})&quot;)\n        \n        try:\n            # Set up environment with debug flag\n            env = os.environ.copy()\n            env[\'DEBUG_MODE\'] = \'true\' if self.debug else \'false\'\n            env[\'LOG_LEVEL\'] = \'DEBUG\' if self.debug else \'INFO\'\n            \n            result = subprocess.run(\n                cmd,\n                capture_output=True,\n                text=True,\n                check=True,\n                env=env\n            )\n            \n            if self.debug and result.stderr:\n                self.logger.debug(f&quot;FFmpeg output: {result.stderr}&quot;)\n            \n            if output_video.exists():\n                size_mb = output_video.stat().st_size / (1024 * 1024)\n                self.logger.info(f&quot;\u2713 Video created: {output_video.relative_to(self.job_dir)} ({size_mb:.1f} MB)&quot;)\n                self.logger.info(f&quot;\u2713 Video contains {len(subtitle_files)} subtitle tracks: {\', \'.join([l.upper() for l in subtitle_langs])}&quot;)\n                \n                # Also copy to media subdirectory for user convenience\n                import shutil\n                shutil.copy2(output_video, media_output_video)\n                self.logger.info(f&quot;\u2713 Copy saved to: {media_output_video.relative_to(self.job_dir)}&quot;)\n                \n                return True\n            else:\n                self.logger.error(&quot;Video muxing failed - no output file&quot;)\n                return False\n                \n        except subprocess.CalledProcessError as e:\n            self.logger.error(f&quot;FFmpeg muxing error: {e.stderr}&quot;, exc_info=True)\n            return False\n    \n    # ========================================================================\n    # Main Execution\n    # ========================================================================\n    \n    def run(self) -&gt; bool:\n        &quot;&quot;&quot;Execute pipeline based on workflow&quot;&quot;&quot;\n        self.logger.info(f&quot;Starting pipeline: {self.workflow}&quot;)\n        self.logger.info(f&quot;Job ID: {self.job_config[\'job_id\']}&quot;)\n        self.logger.info(f&quot;Job directory: {self.job_dir}&quot;)\n        \n        self.manifest[&quot;status&quot;] = &quot;running&quot;\n        self._save_manifest()\n        \n        if self.workflow == &quot;transcribe&quot;:\n            success = self.run_transcribe_workflow()\n        elif self.workflow == &quot;translate&quot;:\n            success = self.run_translate_workflow()\n        elif self.workflow == &quot;subtitle&quot;:\n            success = self.run_subtitle_workflow()\n        else:\n            self.logger.error(f&quot;Unknown workflow: {self.workflow}&quot;)\n            success = False\n        \n        if success:\n            self.manifest[&quot;status&quot;] = &quot;completed&quot;\n            self.logger.info(&quot;=&quot; * 80)\n            self.logger.info(&quot;PIPELINE COMPLETED SUCCESSFULLY&quot;)\n            self.logger.info(&quot;=&quot; * 80)\n        else:\n            self.manifest[&quot;status&quot;] = &quot;failed&quot;\n            self.logger.error(&quot;=&quot; * 80)\n            self.logger.error(&quot;PIPELINE FAILED&quot;)\n            self.logger.error(&quot;=&quot; * 80)\n        \n        self._save_manifest()\n        return success\n    \n    def _stage_hallucination_removal(self) -&gt; bool:\n        &quot;&quot;&quot;\n        Stage: Remove hallucinations from ASR transcript\n        \n        Detects and removes looping/repetition hallucinations from WhisperX output.\n        Runs after ASR, before alignment.\n        \n        Follows developer standards:\n        - Uses Config class for configuration\n        - Proper error handling with graceful degradation\n        - Logging with PipelineLogger\n        - Respects opt-out (HALLUCINATION_REMOVAL_ENABLED=false)\n        \n        Returns:\n            bool: True if successful or disabled, False on error\n        &quot;&quot;&quot;\n        self.logger.info(&quot;Running hallucination removal...&quot;)\n        \n        # Check if enabled (default: true, opt-out)\n        enabled = self.env_config.get(\'HALLUCINATION_REMOVAL_ENABLED\', \'true\').lower() == \'true\'\n        if not enabled:\n            self.logger.info(&quot;Hallucination removal is disabled (HALLUCINATION_REMOVAL_ENABLED=false)&quot;)\n            self.logger.info(&quot;Skipping stage - segments will be used as-is&quot;)\n            return True\n        \n        # Configuration with defaults (following developer standards)\n        loop_threshold = int(self.env_config.get(\'HALLUCINATION_LOOP_THRESHOLD\', \'3\'))\n        max_repeats = int(self.env_config.get(\'HALLUCINATION_MAX_REPEATS\', \'2\'))\n        \n        self.logger.info(f&quot;Configuration:&quot;)\n        self.logger.info(f&quot;  Loop threshold: {loop_threshold} (min consecutive repeats to consider hallucination)&quot;)\n        self.logger.info(f&quot;  Max repeats: {max_repeats} (max occurrences to keep)&quot;)\n        \n        # Input/output paths\n        segments_file = self.job_dir / &quot;transcripts&quot; / &quot;segments.json&quot;\n        if not segments_file.exists():\n            self.logger.error(f&quot;Segments file not found: {segments_file}&quot;)\n            self.logger.error(&quot;Run ASR stage first!&quot;)\n            return False\n        \n        try:\n            # Load segments\n            with open(segments_file, \'r\', encoding=\'utf-8\') as f:\n                data = json.load(f)\n            \n            # Extract segments and metadata\n            if isinstance(data, dict):\n                segments = data.get(\'segments\', [])\n                metadata = {k: v for k, v in data.items() if k != \'segments\'}\n            else:\n                segments = data\n                metadata = {}\n            \n            if not segments:\n                self.logger.warning(&quot;No segments found - nothing to clean&quot;)\n                return True\n            \n            original_count = len(segments)\n            self.logger.info(f&quot;Processing {original_count} segments...&quot;)\n            \n            # Import hallucination remover (late import to avoid issues)\n            sys.path.insert(0, str(SCRIPT_DIR))\n            from hallucination_removal import HallucinationRemover\n            \n            # Create remover instance\n            remover = HallucinationRemover(\n                loop_threshold=loop_threshold,\n                max_repeats=max_repeats,\n                logger=None  # Use pipeline logger instead\n            )\n            \n            # Detect loops manually (for logging)\n            loops = remover.detect_looping_hallucinations(segments)\n            \n            if loops:\n                self.logger.warning(f&quot;Detected {len(loops)} hallucination loop(s):&quot;)\n                for start_idx, end_idx, text in loops:\n                    count = end_idx - start_idx + 1\n                    self.logger.warning(f&quot;  \u2022 \'{text}\' repeated {count} times (segments {start_idx}-{end_idx})&quot;)\n            else:\n                self.logger.info(&quot;No hallucination loops detected - segments are clean&quot;)\n            \n            # Remove loops\n            cleaned_segments = remover.remove_looping_hallucinations(segments, loops)\n            cleaned_count = len(cleaned_segments)\n            removed_count = original_count - cleaned_count\n            \n            # Log results\n            if removed_count &gt; 0:\n                self.logger.info(f&quot;Removed {removed_count} hallucinated segments&quot;)\n                self.logger.info(f&quot;Kept {cleaned_count}/{original_count} segments ({cleaned_count/original_count*100:.1f}%)&quot;)\n                \n                # Calculate repetition improvement\n                before_texts = [seg.get(\'text\', \'\').strip() for seg in segments if seg.get(\'text\', \'\').strip()]\n                after_texts = [seg.get(\'text\', \'\').strip() for seg in cleaned_segments if seg.get(\'text\', \'\').strip()]\n                \n                before_unique = len(set(before_texts))\n                after_unique = len(set(after_texts))\n                \n                before_rep_rate = 1.0 - (before_unique / len(before_texts)) if before_texts else 0.0\n                after_rep_rate = 1.0 - (after_unique / len(after_texts)) if after_texts else 0.0\n                \n                if before_rep_rate &gt; 0:\n                    improvement = ((before_rep_rate - after_rep_rate) / before_rep_rate) * 100\n                    self.logger.info(f&quot;Repetition rate improved: {before_rep_rate:.1%} \u2192 {after_rep_rate:.1%} ({improvement:.0f}% better)&quot;)\n            else:\n                self.logger.info(&quot;No hallucinations found - transcript is clean&quot;)\n            \n            # Save cleaned segments (backup original first)\n            backup_file = segments_file.with_suffix(\'.json.pre-hallucination-removal\')\n            if not backup_file.exists():\n                import shutil\n                shutil.copy2(segments_file, backup_file)\n                self.logger.info(f&quot;Backed up original segments: {backup_file.name}&quot;)\n            \n            # Write cleaned segments\n            output_data = metadata.copy()\n            output_data[\'segments\'] = cleaned_segments\n            output_data[\'hallucination_removal\'] = {\n                \'enabled\': True,\n                \'original_count\': original_count,\n                \'cleaned_count\': cleaned_count,\n                \'removed_count\': removed_count,\n                \'loops_detected\': len(loops),\n                \'loop_threshold\': loop_threshold,\n                \'max_repeats\': max_repeats\n            }\n            \n            with open(segments_file, \'w\', encoding=\'utf-8\') as f:\n                json.dump(output_data, f, indent=2, ensure_ascii=False)\n            \n            self.logger.info(f&quot;Cleaned segments saved: {segments_file}&quot;)\n            self.logger.info(&quot;\u2705 Hallucination removal completed successfully&quot;)\n            return True\n            \n        except Exception as e:\n            self.logger.error(f&quot;Error in hallucination removal: {e}&quot;, exc_info=True)\n            if self.main_config.debug_mode:\n                import traceback\n                self.logger.error(f&quot;Traceback: {traceback.format_exc()}&quot;, exc_info=True)\n            \n            # Graceful degradation - continue with original segments\n            self.logger.warning(&quot;Continuing with original segments (graceful degradation)&quot;)\n            return True  # Don\'t fail pipeline, just skip cleaning\n\n\ndef main() -&gt; Any:\n    &quot;&quot;&quot;Main.&quot;&quot;&quot;\n    parser = argparse.ArgumentParser(\n        description=&quot;IndicTrans2 Pipeline Orchestrator&quot;\n    )\n    \n    parser.add_argument(\n        &quot;--job-dir&quot;,\n        type=Path,\n        required=True,\n        help=&quot;Job directory&quot;\n    )\n    \n    parser.add_argument(\n        &quot;--resume&quot;,\n        action=&quot;store_true&quot;,\n        help=&quot;Resume from last completed stage&quot;\n    )\n    \n    args = parser.parse_args()\n    \n    if not args.job_dir.exists():\n        logger.info(f&quot;\u274c Error: Job directory not found: {args.job_dir}&quot;)\n        return 1\n    \n    # Create and run pipeline\n    pipeline = IndicTrans2Pipeline(args.job_dir, args.resume)\n    success = pipeline.run()\n    \n    return 0 if success else 1\n\n\nif __name__ == &quot;__main__&quot;:\n    sys.exit(main())\n'">/Users/rpatel/Projects/Active/cp-whisperx-app/tests/integration/test_workflow_integration.py:345: Could not verify run-pipeline.py: run-pipeline.py should have main execution guard
assert "if __name__ == '__main__'" in '#!/usr/bin/env python3\n"""\nIndicTrans2 Pipeline Orchestrator\n\nSimplified pipeline execution for IndicTrans2 workflows:\n- Transcribe workflow: demux \u2192 asr \u2192 alignment\n- Translate workflow: load_transcript \u2192 indictrans2_translation \u2192 subtitle_generation\n\nReuses existing infrastructure:\n- shared/logger.py for logging\n- shared/manifest.py for tracking\n- Existing stage implementations where possible\n"""\n\n# Standard library\nimport sys\nimport os\nimport json\nimport argparse\nimport subprocess\nimport traceback\nimport logging\nfrom pathlib import Path\nfrom datetime import datetime\nfrom typing import Optional, Dict, List, Any\n\n# Add paths for imports\nSCRIPT_DIR = Path(__file__).parent\nPROJECT_ROOT = SCRIPT_DIR.parent\nsys.path.insert(0, str(PROJECT_ROOT))\n\nfrom shared.logger import PipelineLogger, get_logger\nfrom shared.environment_manager import EnvironmentManager\nfrom scripts.config_loader import Config\nfrom shared.stage_order import get_stage_dir\nfrom shared.stage_dependencies import (\n    validate_stage_dependencies,\n    get_workflow_stages,\n    get_execution_order\n)\n\n# Initialize logger\nlogger = get_logger(__name__)\n\n\ndef format_timestamp_srt(seconds: float) -&gt; str:\n    """Format seconds as SRT timestamp (HH:MM:SS,mmm)"""\n    hours = int(seconds // 3600)\n    minutes = int((seconds % 3600) // 60)\n    secs = int(seconds % 60)\n    millis = int((seconds % 1) * 1000)\n    return f"{hours:02d}:{minutes:02d}:{secs:02d},{millis:03d}"\n\n\ndef normalize_segments_data(data: Dict[str, Any]) -&gt; Any:\n    """\n    Normalize segments data to consistent dict format.\n    Handles both list [...] and dict {"segments": [...]} formats.\n    \n    Args:\n        data: Either a list of segments or a dict containing segments\n        \n    Returns:\n        Tuple of (data_dict, segments_list)\n    """\n    if isinstance(data, list):\n        segments = data\n        data = {"segments": segments}\n    elif isinstance(data, dict):\n        segments = data.get("segments", [])\n    else:\n        segments = []\n    \n    return data, segments\n\n\ndef generate_srt_from_segments(segments: List[Dict], output_path: Path) -&gt; bool:\n    """Generate SRT subtitle file from segments"""\n    try:\n        with open(output_path, \'w\', encoding=\'utf-8\') as f:\n            for i, segment in enumerate(segments, 1):\n                # Segment number\n                f.write(f"{i}\\n")\n                \n                # Timestamps\n                start = format_timestamp_srt(segment.get(\'start\', 0))\n                end = format_timestamp_srt(segment.get(\'end\', 0))\n                f.write(f"{start} --&gt; {end}\\n")\n                \n                # Text\n                text = segment.get(\'text\', \'\').strip()\n                f.write(f"{text}\\n")\n                \n                # Blank line between segments\n                f.write("\\n")\n        \n        return True\n    except Exception as e:\n        return False\n\n\n\nclass IndicTrans2Pipeline:\n    """Pipeline orchestrator for IndicTrans2 workflows"""\n    \n    def __init__(self, job_dir: Path, resume: bool = False):\n        """  Init  ."""\n        self.job_dir = job_dir\n        self.resume = resume\n        \n        # Set scripts directory\n        self.scripts_dir = PROJECT_ROOT / "scripts"\n        \n        # Load main configuration for fallback defaults\n        self.main_config = Config(PROJECT_ROOT)\n        \n        # Load job configuration\n        self.job_config = self._load_config("job.json")\n        self.manifest = self._load_config("manifest.json")\n        \n        # Initialize environment manager\n        self.env_manager = EnvironmentManager(PROJECT_ROOT)\n        \n        # Load job-specific environment configuration\n        self.env_config = self._load_env_config()\n        \n        # Get debug mode from job config\n        self.debug = self.job_config.get("debug", False)\n        log_level = "DEBUG" if self.debug else "INFO"\n        \n        # Setup logging - DUAL logging architecture:\n        # 1. Main pipeline log: High-level orchestration\n        # 2. Stage logs: Detailed logs in each stage subdirectory\n        log_dir = job_dir / "logs"\n        log_dir.mkdir(exist_ok=True)\n        \n        # Create main pipeline log file (99_pipeline_*.log for clarity)\n        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")\n        log_file = log_dir / f"99_pipeline_{timestamp}.log"\n        \n        self.logger = PipelineLogger(\n            module_name="pipeline",\n            log_file=log_file,\n            log_level=log_level\n        )\n        \n        self.logger.info("=" * 80)\n        self.logger.info("PIPELINE LOGGING ARCHITECTURE")\n        self.logger.info("=" * 80)\n        self.logger.info(f"\U0001f4cb Main pipeline log: {log_file.relative_to(job_dir)}")\n        self.logger.info(f"\U0001f4cb Stage logs: Each stage writes to its own subdirectory")\n        self.logger.info(f"\U0001f4cb Stage manifests: Track inputs/outputs/intermediate files")\n        self.logger.info("")\n        \n        if self.debug:\n            self.logger.info("\U0001f41b DEBUG MODE ENABLED - Verbose logging active")\n        \n        self.workflow = self.job_config["workflow"]\n        \n        # Initialize glossary manager (will be loaded in stage)\n        self.glossary_manager = None\n        \n        # Log cache configuration\n        cache_config = self.env_manager.hardware_cache.get("cache", {})\n        if cache_config:\n            self.logger.info("\U0001f4e6 Model cache configuration:")\n            if "hf_home" in cache_config:\n                hf_cache = PROJECT_ROOT / cache_config["hf_home"]\n                if hf_cache.exists():\n                    # Count cached models\n                    hub_dir = hf_cache / "hub"\n                    if hub_dir.exists():\n                        model_count = len([d for d in hub_dir.iterdir() if d.is_dir() and d.name.startswith("models--")])\n                        self.logger.info(f"  HuggingFace cache: {cache_config[\'hf_home\']} ({model_count} models cached)")\n                    else:\n                        self.logger.info(f"  HuggingFace cache: {cache_config[\'hf_home\']} (empty)")\n                else:\n                    self.logger.warning(f"  HuggingFace cache not found: {cache_config[\'hf_home\']}")\n            if "torch_home" in cache_config:\n                self.logger.info(f"  PyTorch cache: {cache_config[\'torch_home\']}")\n            if "mlx_home" in cache_config:\n                self.logger.info(f"  MLX cache: {cache_config[\'mlx_home\']}")\n        \n        # Log environment information\n        envs = self.job_config.get("environments", {})\n        if envs:\n            self.logger.info(f"Multi-environment mode: {len(envs)} environment(s) configured")\n            for env_name, env_path in envs.items():\n                installed = "\u2713" if self.env_manager.is_environment_installed(env_name) else "\u2717"\n                self.logger.info(f"  {installed} {env_name}: {env_path}")\n    \n    def _stage_path(self, stage_name: str) -&gt; Path:\n        """\n        Get the path to a stage directory using centralized stage ordering.\n        \n        Args:\n            stage_name: Name of the stage\n            \n        Returns:\n            Path to stage directory\n        """\n        return self.job_dir / get_stage_dir(stage_name)\n    \n    def _load_env_config(self) -&gt; Dict[str, str]:\n        """Load job-specific .env file created by prepare-job"""\n        job_id = self.job_config["job_id"]\n        env_file = self.job_dir / f".{job_id}.env"\n        \n        if not env_file.exists():\n            self.logger.warning(f"Job .env file not found: {env_file}")\n            return {}\n        \n        config = {}\n        with open(env_file) as f:\n            for line in f:\n                line = line.strip()\n                if line and not line.startswith(\'#\') and \'=\' in line:\n                    key, value = line.split(\'=\', 1)\n                    config[key] = value\n        \n        return config\n    \n    def _is_indic_language(self, lang_code: str) -&gt; bool:\n        """Check if a language code is an Indic language supported by IndicTrans2"""\n        indic_languages = {\n            "hi", "as", "bn", "gu", "kn", "ml", "mr", "or", "pa", "ta", "te", "ur",\n            "ne", "sd", "si", "sa", "ks", "doi", "mni", "kok", "mai", "sat"\n        }\n        return lang_code in indic_languages\n        \n    def _load_config(self, filename: str) -&gt; Dict:\n        """Load JSON configuration file"""\n        config_file = self.job_dir / filename\n        if not config_file.exists():\n            raise FileNotFoundError(f"Configuration not found: {config_file}")\n        \n        with open(config_file) as f:\n            return json.load(f)\n    \n    def _save_manifest(self) -&gt; None:\n        """Save manifest to file"""\n        manifest_file = self.job_dir / "manifest.json"\n        self.manifest["updated_at"] = datetime.now().isoformat()\n        \n        with open(manifest_file, \'w\') as f:\n            json.dump(self.manifest, f, indent=2)\n    \n    def _update_stage_status(self, stage_name: str, status: str, \n                            duration: Optional[float] = None):\n        """Update stage status in manifest"""\n        for stage in self.manifest["stages"]:\n            if stage["name"] == stage_name:\n                stage["status"] = status\n                if status == "running":\n                    stage["start_time"] = datetime.now().isoformat()\n                elif status in ["completed", "failed"]:\n                    stage["end_time"] = datetime.now().isoformat()\n                    if duration:\n                        stage["duration_seconds"] = duration\n                break\n        \n        self._save_manifest()\n    \n    def _get_stage_environment(self, stage_name: str) -&gt; Optional[str]:\n        """Get the required environment for a stage"""\n        stage_envs = self.job_config.get("stage_environments", {})\n        \n        # Try exact match first\n        if stage_name in stage_envs:\n            return stage_envs[stage_name]\n        \n        # Handle dynamic translation stage names (e.g., indictrans2_translation_en)\n        if stage_name.startswith("indictrans2_translation_"):\n            # Look for generic "translation" mapping\n            return stage_envs.get("translation") or "indictrans2"\n        \n        # Handle dynamic NLLB translation stage names\n        if stage_name.startswith("nllb_translation_"):\n            return stage_envs.get("nllb_translation") or "nllb"\n        \n        # Handle dynamic subtitle generation stage names\n        if stage_name.startswith("subtitle_generation_"):\n            return stage_envs.get("subtitle_gen") or stage_envs.get("subtitle_generation") or "common"\n        \n        # Default environment mappings for stages without explicit config\n        default_envs = {\n            "source_separation": "common",\n            "pyannote_vad": "pyannote"\n        }\n        \n        if stage_name in default_envs:\n            return default_envs[stage_name]\n        \n        return None\n    \n    def _get_target_language(self) -&gt; Optional[str]:\n        """\n        Get target language from job config, handling both singular and plural forms.\n        \n        Returns:\n            Target language code, or None if not set\n        """\n        # Try singular form first (legacy)\n        target_lang = self.job_config.get("target_language")\n        if target_lang:\n            return target_lang\n        \n        # Try plural form (new format)\n        target_langs = self.job_config.get("target_languages", [])\n        if target_langs:\n            return target_langs[0]\n        \n        return None\n    \n    def _run_in_environment(self, stage_name: str, command: List[str], **kwargs) -&gt; subprocess.CompletedProcess:\n        """\n        Run a command in the appropriate environment for a stage\n        \n        Args:\n            stage_name: Name of the stage\n            command: Command to run\n            **kwargs: Additional arguments for subprocess.run\n            \n        Returns:\n            CompletedProcess instance\n        """\n        env_name = self._get_stage_environment(stage_name)\n        \n        if env_name:\n            self.logger.info(f"Running stage \'{stage_name}\' in environment \'{env_name}\'")\n            python_exe = self.env_manager.get_python_executable(env_name)\n            \n            # Start with provided env if any, otherwise use os.environ\n            env = kwargs.get(\'env\', os.environ).copy()\n            \n            # Set up environment variables for the virtual environment\n            env["VIRTUAL_ENV"] = str(self.env_manager.get_environment_path(env_name))\n            env_bin = self.env_manager.get_environment_path(env_name) / "bin"\n            env["PATH"] = f"{env_bin}:{env[\'PATH\']}"\n            env["DEBUG_MODE"] = \'true\' if self.debug else \'false\'\n            env["LOG_LEVEL"] = \'DEBUG\' if self.debug else \'INFO\'\n            \n            # Replace python in command with environment-specific python\n            if command[0] == "python" or command[0] == "python3":\n                command[0] = str(python_exe)\n            \n            kwargs[\'env\'] = env\n        else:\n            self.logger.warning(f"No environment specified for stage \'{stage_name}\', using current environment")\n            \n            # Set debug environment variables\n            if \'env\' not in kwargs:\n                kwargs[\'env\'] = os.environ.copy()\n            kwargs[\'env\'][\'DEBUG_MODE\'] = \'true\' if self.debug else \'false\'\n            kwargs[\'env\'][\'LOG_LEVEL\'] = \'DEBUG\' if self.debug else \'INFO\'\n        \n        return subprocess.run(command, **kwargs)\n    \n    def _check_indictrans2_available(self) -&gt; bool:\n        """Check if IndicTrans2 environment is available"""\n        try:\n            # Check if indictrans2 environment exists and is valid\n            return self.env_manager.is_environment_installed("indictrans2")\n        except Exception as e:\n            self.logger.debug(f"IndicTrans2 check failed: {e}")\n            return False\n    \n    def run_transcribe_workflow(self) -&gt; bool:\n        """\n        Execute transcribe workflow stages:\n        1. Demux - Extract audio\n        1.5. Source Separation - Extract vocals (if enabled)\n        2. PyAnnote VAD - Detect speech segments (highest quality)\n        3. ASR - Transcribe using WhisperX\n        4. Alignment - Word-level timestamps\n        5. Export - Generate plain text transcript\n        """\n        self.logger.info("=" * 80)\n        self.logger.info("TRANSCRIBE WORKFLOW")\n        self.logger.info("=" * 80)\n        \n        # Build stages list\n        stages = [("demux", self._stage_demux)]\n        \n        # Add TMDB enrichment if enabled (BEFORE ASR for metadata context)\n        if self.job_config.get("tmdb_enrichment", {}).get("enabled", False):\n            stages.append(("tmdb", self._stage_tmdb_enrichment))\n            # Add glossary load after TMDB (to use enrichment data)\n            stages.append(("glossary_load", self._stage_glossary_load))\n        \n        # Add source separation if enabled\n        sep_config = self.job_config.get("source_separation", {})\n        if sep_config.get("enabled", False):\n            stages.append(("source_separation", self._stage_source_separation))\n        \n        # Add core ASR stages\n        stages.extend([\n            ("pyannote_vad", self._stage_pyannote_vad),\n            ("asr", self._stage_asr),\n            ("hallucination_removal", self._stage_hallucination_removal),\n            ("alignment", self._stage_alignment),\n        ])\n        \n        # Add lyrics detection AFTER ASR (optional)\n        lyrics_enabled = self.env_config.get("LYRICS_DETECTION_ENABLED", "true").lower() == "true"\n        if lyrics_enabled:\n            stages.append(("lyrics_detection", self._stage_lyrics_detection))\n        \n        # Final export stage\n        stages.append(("export_transcript", self._stage_export_transcript))\n        \n        return self._execute_stages(stages)\n    \n    def run_translate_workflow(self) -&gt; bool:\n        """\n        Execute translate workflow stages:\n        Auto-executes transcribe workflow if transcript doesn\'t exist\n        1. Demux - Extract audio (if needed)\n        2. ASR - Transcribe (if needed)\n        3. Alignment - Word timestamps (if needed)\n        4. Load Transcript - Load segments.json\n        5. IndicTrans2 Translation - Translate text\n        6. Subtitle Generation - Create SRT in target language\n        """\n        self.logger.info("=" * 80)\n        self.logger.info("TRANSLATE WORKFLOW")\n        self.logger.info("=" * 80)\n        \n        # Get target language\n        target_lang = self._get_target_language()\n        \n        # Check if transcript exists, if not run transcribe stages first\n        segments_file = self.job_dir / "transcripts" / "segments.json"\n        \n        if not segments_file.exists():\n            self.logger.info("\U0001f4dd Transcript not found - auto-executing transcribe workflow first")\n            self.logger.info("")\n            \n            # Run transcribe stages\n            transcribe_stages = [("demux", self._stage_demux)]\n            \n            # Add TMDB enrichment if enabled (BEFORE ASR for metadata context)\n            if self.job_config.get("tmdb_enrichment", {}).get("enabled", False):\n                transcribe_stages.append(("tmdb", self._stage_tmdb_enrichment))\n                # Add glossary load after TMDB (to use enrichment data)\n                transcribe_stages.append(("glossary_load", self._stage_glossary_load))\n            \n            # Add source separation if enabled\n            sep_config = self.job_config.get("source_separation", {})\n            if sep_config.get("enabled", False):\n                transcribe_stages.append(("source_separation", self._stage_source_separation))\n            \n            # Add core ASR stages\n            transcribe_stages.extend([\n                ("pyannote_vad", self._stage_pyannote_vad),\n                ("asr", self._stage_asr),\n                ("hallucination_removal", self._stage_hallucination_removal),\n                ("alignment", self._stage_alignment),\n            ])\n            \n            # Add lyrics detection AFTER ASR (optional)\n            lyrics_enabled = self.env_config.get("LYRICS_DETECTION_ENABLED", "true").lower() == "true"\n            if lyrics_enabled:\n                transcribe_stages.append(("lyrics_detection", self._stage_lyrics_detection))\n            \n            # Final export stage\n            transcribe_stages.append(("export_transcript", self._stage_export_transcript))\n            \n            if not self._execute_stages(transcribe_stages):\n                self.logger.error("Transcribe workflow failed - cannot proceed with translation")\n                return False\n            \n            self.logger.info("")\n            self.logger.info("\u2705 Transcribe workflow completed successfully")\n            self.logger.info("=" * 80)\n            self.logger.info("CONTINUING WITH TRANSLATION")\n            self.logger.info("=" * 80)\n        else:\n            self.logger.info("\u2713 Transcript found - skipping transcribe stages")\n        \n        # Route to appropriate translator based on target language\n        translate_stages = [("load_transcript", self._stage_load_transcript)]\n        \n        # Check if hybrid translation is enabled\n        use_hybrid = self.env_config.get("USE_HYBRID_TRANSLATION", "true").lower() == "true"\n        \n        if use_hybrid:\n            # Use hybrid translation (IndicTrans2 + LLM for songs)\n            self.logger.info(f"Using hybrid translation for {target_lang}")\n            translate_stages.append(("hybrid_translation", self._stage_hybrid_translation))\n        elif self._is_indic_language(target_lang):\n            # Use IndicTrans2 for Indic languages\n            self.logger.info(f"Using IndicTrans2 for Indic language: {target_lang}")\n            translate_stages.append(("indictrans2_translation", self._stage_indictrans2_translation))\n        else:\n            # Use NLLB for non-Indic languages\n            self.logger.info(f"Using NLLB for non-Indic language: {target_lang}")\n            translate_stages.append(("nllb_translation", self._stage_nllb_translation))\n        \n        translate_stages.append(("subtitle_generation", self._stage_subtitle_generation))\n        \n        return self._execute_stages(translate_stages)\n    \n    def run_subtitle_workflow(self) -&gt; bool:\n        """\n        Execute subtitle workflow stages:\n        Auto-executes transcribe + translate workflows if needed\n        Generates subtitles in source and multiple target languages (up to 5)\n        1. Demux - Extract audio (if needed)\n        2. ASR - Transcribe (if needed)\n        3. Alignment - Word timestamps (if needed)\n        4. Load Transcript - Load segments.json\n        5. IndicTrans2 Translation - Translate text (for each target language)\n        6. Subtitle Generation (Target) - Create SRT for each target language\n        7. Subtitle Generation (Source) - Create SRT in source language\n        8. Mux - Embed all subtitle tracks in video\n        """\n        self.logger.info("=" * 80)\n        self.logger.info("SUBTITLE WORKFLOW")\n        self.logger.info("=" * 80)\n        \n        # Get target languages from config\n        target_languages = self.job_config.get("target_languages", [])\n        if not target_languages:\n            self.logger.error("No target languages configured!")\n            return False\n        \n        self.logger.info(f"Target languages: {\', \'.join(target_languages)}")\n        \n        # Check IndicTrans2 availability\n        if not self._check_indictrans2_available():\n            self.logger.error("IndicTrans2 not available!")\n            self.logger.error("Please install: ./install-indictrans2.sh")\n            return False\n        \n        # Check if transcript exists, if not run transcribe stages first\n        segments_file = self.job_dir / "transcripts" / "segments.json"\n        \n        if not segments_file.exists():\n            self.logger.info("\U0001f4dd Transcript not found - auto-executing transcribe workflow first")\n            self.logger.info("")\n            \n            # Run transcribe stages\n            transcribe_stages = [("demux", self._stage_demux)]\n            \n            # Add TMDB enrichment if enabled (BEFORE ASR for metadata context)\n            if self.job_config.get("tmdb_enrichment", {}).get("enabled", False):\n                transcribe_stages.append(("tmdb", self._stage_tmdb_enrichment))\n                # Add glossary load after TMDB (to use enrichment data)\n                transcribe_stages.append(("glossary_load", self._stage_glossary_load))\n            \n            # Add source separation if enabled\n            sep_config = self.job_config.get("source_separation", {})\n            if sep_config.get("enabled", False):\n                transcribe_stages.append(("source_separation", self._stage_source_separation))\n            \n            # Add core ASR stages\n            transcribe_stages.extend([\n                ("pyannote_vad", self._stage_pyannote_vad),\n                ("asr", self._stage_asr),\n                ("hallucination_removal", self._stage_hallucination_removal),\n                ("alignment", self._stage_alignment),\n            ])\n            \n            # Add lyrics detection AFTER ASR (optional)\n            lyrics_enabled = self.env_config.get("LYRICS_DETECTION_ENABLED", "true").lower() == "true"\n            if lyrics_enabled:\n                transcribe_stages.append(("lyrics_detection", self._stage_lyrics_detection))\n            \n            # Final stage\n            transcribe_stages.append(("export_transcript", self._stage_export_transcript))\n            \n            if not self._execute_stages(transcribe_stages):\n                self.logger.error("Transcribe workflow failed - cannot proceed with subtitle generation")\n                return False\n            \n            self.logger.info("")\n            self.logger.info("\u2705 Transcribe workflow completed successfully")\n            self.logger.info("=" * 80)\n            self.logger.info("CONTINUING WITH TRANSLATION AND SUBTITLE GENERATION")\n            self.logger.info("=" * 80)\n        else:\n            self.logger.info("\u2713 Transcript found - skipping transcribe stages")\n        \n        # Build subtitle stages dynamically\n        subtitle_stages = [("load_transcript", self._stage_load_transcript)]\n        \n        # Check if hybrid translation is enabled\n        use_hybrid = self.env_config.get("USE_HYBRID_TRANSLATION", "true").lower() == "true"\n        \n        # Add translation and subtitle generation for each target language\n        for target_lang in target_languages:\n            # Route to appropriate translator based on language and hybrid setting\n            if use_hybrid:\n                # Use hybrid translation (IndicTrans2 + LLM for songs)\n                subtitle_stages.append((\n                    f"hybrid_translation_{target_lang}",\n                    lambda tl=target_lang: self._stage_hybrid_translation_multi(tl)\n                ))\n            elif self._is_indic_language(target_lang):\n                # Use IndicTrans2 for Indic languages\n                subtitle_stages.append((\n                    f"indictrans2_translation_{target_lang}",\n                    lambda tl=target_lang: self._stage_indictrans2_translation_multi(tl)\n                ))\n            else:\n                # Use NLLB for non-Indic languages\n                subtitle_stages.append((\n                    f"nllb_translation_{target_lang}",\n                    lambda tl=target_lang: self._stage_nllb_translation_multi(tl)\n                ))\n            \n            subtitle_stages.append((\n                f"subtitle_generation_{target_lang}",\n                lambda tl=target_lang: self._stage_subtitle_generation_target_multi(tl)\n            ))\n        \n        # Add source subtitle generation\n        subtitle_stages.append(("subtitle_generation_source", self._stage_subtitle_generation_source))\n        \n        # Add Hinglish detection for source subtitles if source language is Hindi/Indic\n        source_lang = self.job_config.get("source_language", "")\n        hinglish_detection = self.job_config.get("hinglish_detection", {})\n        if hinglish_detection.get("enabled", True) and source_lang in ["hi", "hin", "hin_Deva"]:\n            subtitle_stages.append(("hinglish_detection", self._stage_hinglish_detection))\n        \n        # Add mux stage\n        subtitle_stages.append(("mux", self._stage_mux))\n        \n        return self._execute_stages(subtitle_stages)\n    \n    def _execute_stages(self, stages: List[tuple]) -&gt; bool:\n        """Execute list of stages"""\n        for stage_name, stage_func in stages:\n            # Check if resuming and stage already completed\n            if self.resume:\n                stage_status = self._get_stage_status(stage_name)\n                if stage_status == "completed":\n                    self.logger.info(f"\u23ed  Stage {stage_name}: SKIPPED (already completed)")\n                    continue\n            \n            # Execute stage\n            self.logger.info(f"\u25b6\ufe0f  Stage {stage_name}: STARTING")\n            self._update_stage_status(stage_name, "running")\n            \n            start_time = datetime.now()\n            \n            try:\n                success = stage_func()\n                \n                duration = (datetime.now() - start_time).total_seconds()\n                \n                if success:\n                    self.logger.info(f"\u2705 Stage {stage_name}: COMPLETED ({duration:.1f}s)")\n                    self._update_stage_status(stage_name, "completed", duration)\n                else:\n                    self.logger.error(f"\u274c Stage {stage_name}: FAILED")\n                    self._update_stage_status(stage_name, "failed", duration)\n                    return False\n                    \n            except Exception as e:\n                duration = (datetime.now() - start_time).total_seconds()\n                self.logger.error(f"\u274c Stage {stage_name}: EXCEPTION: {e}", exc_info=True)\n                if self.debug:\n                    self.logger.error(f"Traceback: {traceback.format_exc()}", exc_info=True)\n                self._update_stage_status(stage_name, "failed", duration)\n                return False\n        \n        return True\n    \n    def _get_stage_status(self, stage_name: str) -&gt; Optional[str]:\n        """Get current status of a stage"""\n        for stage in self.manifest["stages"]:\n            if stage["name"] == stage_name:\n                return stage["status"]\n        return None\n    \n    # ========================================================================\n    # Stage Implementations\n    # ========================================================================\n    \n    def _stage_demux(self) -&gt; bool:\n        """Stage 1: Extract audio from video (full or clipped)"""\n        \n        # Initialize stage I/O and manifest\n        from shared.stage_utils import StageIO\n        stage_io = StageIO("demux", self.job_dir, enable_manifest=True)\n        stage_logger = stage_io.get_stage_logger("DEBUG" if self.debug else "INFO")\n        \n        # Input/output setup\n        input_media = Path(self.job_config["input_media"])\n        stage_dir = stage_io.stage_dir\n        audio_output = stage_io.get_output_path("audio.wav")\n        \n        # Track input in manifest\n        stage_io.track_input(input_media, "video", format=input_media.suffix[1:])\n        \n        # Log input/output (to both stage log and pipeline log)\n        self.logger.info(f"\U0001f4e5 Input: {input_media.relative_to(PROJECT_ROOT) if input_media.is_relative_to(PROJECT_ROOT) else input_media}")\n        self.logger.info(f"\U0001f4e4 Output: {audio_output.relative_to(self.job_dir)}")\n        stage_logger.info(f"Input media: {input_media}")\n        stage_logger.info(f"Output directory: {stage_dir}")\n        \n        # Get media processing configuration\n        media_config = self.job_config.get("media_processing", {})\n        processing_mode = media_config.get("mode", "full")\n        start_time = media_config.get("start_time", "")\n        end_time = media_config.get("end_time", "")\n        \n        # Add config to manifest\n        stage_io.set_config({\n            "processing_mode": processing_mode,\n            "start_time": start_time,\n            "end_time": end_time,\n            "sample_rate": "16000",\n            "channels": "1"\n        })\n        \n        if processing_mode == "clip":\n            self.logger.info(f"Extracting audio clip (from {start_time} to {end_time})...")\n            stage_logger.info(f"Clip mode: {start_time} to {end_time}")\n        else:\n            self.logger.info("Extracting audio from full media...")\n            stage_logger.info("Full media extraction")\n        \n        # Build ffmpeg command\n        cmd = ["ffmpeg", "-y"]\n        \n        # Add log level based on debug mode\n        if not self.debug:\n            cmd.extend(["-loglevel", "error"])\n        \n        # Add start time if clipping\n        if processing_mode == "clip" and start_time:\n            cmd.extend(["-ss", start_time])\n        \n        # Add input file\n        cmd.extend(["-i", str(input_media)])\n        \n        # Add end time if clipping\n        if processing_mode == "clip" and end_time:\n            cmd.extend(["-to", end_time])\n        \n        # Add output options\n        cmd.extend([\n            "-vn",  # No video\n            "-acodec", "pcm_s16le",\n            "-ar", "16000",  # 16kHz for Whisper\n            "-ac", "1",  # Mono\n            str(audio_output)\n        ])\n        \n        try:\n            # Set up environment with debug flag\n            env = os.environ.copy()\n            env[\'DEBUG_MODE\'] = \'true\' if self.debug else \'false\'\n            env[\'LOG_LEVEL\'] = \'DEBUG\' if self.debug else \'INFO\'\n            \n            result = subprocess.run(\n                cmd,\n                capture_output=True,\n                text=True,\n                check=True,\n                env=env\n            )\n            \n            if self.debug and result.stderr:\n                self.logger.debug(f"FFmpeg output: {result.stderr}")\n                stage_logger.debug(f"FFmpeg stderr:\\n{result.stderr}")\n            \n            if audio_output.exists():\n                size_mb = audio_output.stat().st_size / (1024 * 1024)\n                mode_str = f"clip ({start_time} to {end_time})" if processing_mode == "clip" else "full media"\n                \n                # Track output in manifest\n                stage_io.track_output(audio_output, "audio", \n                                     format="wav", \n                                     sample_rate=16000,\n                                     channels=1,\n                                     size_mb=round(size_mb, 2))\n                \n                # Finalize manifest with success\n                stage_io.finalize(status="success", \n                                 output_size_mb=round(size_mb, 2),\n                                 processing_mode=processing_mode)\n                \n                self.logger.info(f"\u2713 Audio extracted from {mode_str}: {audio_output.name} ({size_mb:.1f} MB)")\n                stage_logger.info(f"Successfully extracted audio: {size_mb:.1f} MB")\n                stage_logger.info(f"Stage log: {stage_io.stage_log.relative_to(self.job_dir)}")\n                stage_logger.info(f"Stage manifest: {stage_io.manifest_path.relative_to(self.job_dir)}")\n                return True\n            else:\n                stage_io.add_error("Audio extraction failed - no output file")\n                stage_io.finalize(status="failed")\n                self.logger.error("Audio extraction failed")\n                stage_logger.error("Audio extraction failed - no output file")\n                return False\n                \n        except subprocess.CalledProcessError as e:\n            self.logger.error(f"FFmpeg failed: {e}", exc_info=True)\n            stage_logger.error(f"FFmpeg command failed: {e.stderr if e.stderr else str(e, exc_info=True)}", exc_info=True)\n            stage_io.add_error(f"FFmpeg command failed: {e}")\n            stage_io.finalize(status="failed", error="Demux failed")\n            return False\n        \n        except FileNotFoundError as e:\n            self.logger.error(f"Input file not found: {e}", exc_info=True)\n            stage_logger.error(f"Input file not found: {e}", exc_info=True)\n            stage_io.add_error(f"Input file not found: {e}")\n            stage_io.finalize(status="failed", error=str(e))\n            return False\n        \n        except IOError as e:\n            self.logger.error(f"I/O error: {e}", exc_info=True)\n            stage_logger.error(f"I/O error during demux: {e}", exc_info=True)\n            stage_io.add_error(f"I/O error: {e}")\n            stage_io.finalize(status="failed", error=str(e))\n            return False\n        \n        except Exception as e:\n            self.logger.error(f"Unexpected error: {e}", exc_info=True)\n            stage_logger.error(f"Unexpected error during demux: {e}", exc_info=True)\n            stage_io.add_error(f"Unexpected error: {e}")\n            stage_io.finalize(status="failed", error=str(e))\n            return False\n    \n    def _stage_tmdb_enrichment(self) -&gt; bool:\n        """Stage 3: TMDB enrichment - Fetch movie metadata and generate glossaries"""\n        \n        # Check if TMDB enrichment is enabled\n        tmdb_config = self.job_config.get("tmdb_enrichment", {})\n        enabled = tmdb_config.get("enabled", False)\n        \n        if not enabled:\n            self.logger.info("TMDB enrichment is disabled (skipping)")\n            return True\n        \n        title = tmdb_config.get("title") or self.job_config.get("title")\n        year = tmdb_config.get("year") or self.job_config.get("year")\n        \n        if not title:\n            self.logger.warning("No movie title provided - skipping TMDB enrichment")\n            return True\n        \n        # Input/output setup\n        output_dir = self._stage_path("tmdb")\n        output_dir.mkdir(parents=True, exist_ok=True)\n        \n        # Log input/output\n        self.logger.info(f"\U0001f4e5 Input: Title=\'{title}\', Year={year or \'N/A\'}")\n        self.logger.info(f"\U0001f4e4 Output: {output_dir.relative_to(self.job_dir)}/")\n        self.logger.info(f"Fetching TMDB metadata for: {title}" + (f" ({year})" if year else ""))\n        \n        # Run the TMDB enrichment script\n        script_path = self.scripts_dir / "tmdb_enrichment_stage.py"\n        \n        try:\n            # Set up environment\n            env = os.environ.copy()\n            env[\'OUTPUT_DIR\'] = str(self.job_dir)\n            env[\'DEBUG_MODE\'] = \'true\' if self.debug else \'false\'\n            env[\'LOG_LEVEL\'] = \'DEBUG\' if self.debug else \'INFO\'\n            \n            # Build command arguments\n            cmd = [sys.executable, str(script_path), "--job-dir", str(self.job_dir)]\n            if title:\n                cmd.extend(["--title", title])\n            if year:\n                cmd.extend(["--year", str(year)])\n            \n            # Run in common environment (TMDB/NER tools)\n            result = self._run_in_environment(\n                "tmdb",  # Stage name for environment lookup\n                cmd,\n                capture_output=True,\n                text=True,\n                check=True,\n                env=env\n            )\n            \n            if self.debug:\n                self.logger.debug(f"TMDB enrichment output: {result.stdout}")\n            \n            # Check if enrichment file was created\n            enrichment_file = self._stage_path("tmdb") / "enrichment.json"\n            if enrichment_file.exists():\n                with open(enrichment_file, \'r\', encoding=\'utf-8\') as f:\n                    data = json.load(f)\n                    \n                # Log success with details\n                cast_count = len(data.get(\'cast\', []))\n                crew_count = len(data.get(\'crew\', []))\n                soundtrack_count = len(data.get(\'soundtrack\', []))\n                \n                self.logger.info(f"\u2713 TMDB metadata fetched successfully")\n                if cast_count &gt; 0:\n                    self.logger.info(f"  Cast: {cast_count} actors")\n                if crew_count &gt; 0:\n                    self.logger.info(f"  Crew: {crew_count} members")\n                if soundtrack_count &gt; 0:\n                    self.logger.info(f"  Soundtrack: {soundtrack_count} songs")\n            else:\n                self.logger.warning("TMDB enrichment completed but no output file found")\n            \n            return True\n            \n        except subprocess.CalledProcessError as e:\n            self.logger.error(f"TMDB enrichment failed: {e.stderr if e.stderr else str(e, exc_info=True)}")\n            self.logger.warning("Continuing without TMDB metadata")\n            return True  # Non-blocking failure\n    \n    def _stage_glossary_load(self) -&gt; bool:\n        """Stage 3: Load glossary system using new modular stage"""\n        \n        try:\n            # Check if glossary is enabled\n            glossary_enabled = self.env_config.get("STAGE_03_GLOSSARY_ENABLED", "true").lower() == "true"\n            \n            if not glossary_enabled:\n                self.logger.info("Glossary system is disabled (skipping)")\n                return True\n            \n            # Import glossary load stage module (module name starts with number, use importlib)\n            import importlib\n            glossary_load = importlib.import_module("scripts.03_glossary_load")\n            \n            # Call the stage module\n            self.logger.info("Running glossary load stage...")\n            exit_code = glossary_load.run_stage(self.job_dir, "03_glossary_load")\n            \n            if exit_code != 0:\n                self.logger.error("Glossary load stage failed")\n                return False\n            \n            self.logger.info("\u2713 Glossary load complete")\n            return True\n            \n        except Exception as e:\n            self.logger.error(f"Failed to load glossary system: {e}", exc_info=True)\n            if self.debug:\n                self.logger.debug(traceback.format_exc())\n            self.logger.warning("Continuing without glossary system")\n            return True  # Non-blocking failure\n    \n    def _stage_source_separation(self) -&gt; bool:\n        """Stage 2: Source separation - Extract vocals, remove background music"""\n        \n        # Check if source separation is enabled\n        sep_config = self.job_config.get("source_separation", {})\n        enabled = sep_config.get("enabled", False)\n        \n        if not enabled:\n            self.logger.info("Source separation is disabled (skipping)")\n            return True\n        \n        # Input/output setup\n        input_audio = self.job_dir / "01_demux" / "audio.wav"\n        output_dir = self._stage_path("source_separation")\n        output_dir.mkdir(parents=True, exist_ok=True)\n        \n        # Log input/output\n        self.logger.info(f"\U0001f4e5 Input: {input_audio.relative_to(self.job_dir)}")\n        self.logger.info(f"\U0001f4e4 Output: {output_dir.relative_to(self.job_dir)}/")\n        \n        quality = sep_config.get("quality", "balanced")\n        self.logger.info(f"Running source separation (quality: {quality})...")\n        self.logger.info("This will extract vocals and remove background music")\n        \n        # Run the source separation script\n        script_path = self.scripts_dir / "04_source_separation.py"\n        \n        try:\n            # Set up environment\n            env = os.environ.copy()\n            env[\'OUTPUT_DIR\'] = str(self.job_dir)  # CRITICAL: Tell script where job directory is\n            env[\'DEBUG_MODE\'] = \'true\' if self.debug else \'false\'\n            env[\'LOG_LEVEL\'] = \'DEBUG\' if self.debug else \'INFO\'\n            \n            # Run in demucs environment\n            result = self._run_in_environment(\n                "source_separation",\n                [sys.executable, str(script_path)],\n                capture_output=True,\n                text=True,\n                check=True,\n                env=env\n            )\n            \n            if self.debug:\n                self.logger.debug(f"Source separation output: {result.stdout}")\n            \n            self.logger.info("\u2713 Vocals extracted successfully")\n            return True\n            \n        except subprocess.CalledProcessError as e:\n            self.logger.error(f"Source separation failed: {e.stderr}", exc_info=True)\n            return False\n    \n    def _stage_lyrics_detection(self) -&gt; bool:\n        """Stage 6: Lyrics detection using new modular stage"""\n        \n        try:\n            # Check if lyrics detection is enabled\n            lyrics_enabled = self.env_config.get("STAGE_06_LYRICS_ENABLED", "true").lower() == "true"\n            \n            if not lyrics_enabled:\n                self.logger.info("Lyrics detection is disabled (skipping)")\n                return True\n            \n            # Import lyrics detection stage module (module name starts with number, use importlib)\n            import importlib\n            lyrics_detection = importlib.import_module("scripts.06_lyrics_detection")\n            \n            # Call the stage module\n            self.logger.info("Running lyrics detection stage...")\n            exit_code = lyrics_detection.run_stage(self.job_dir, "06_lyrics_detection")\n            \n            if exit_code != 0:\n                self.logger.warning("Lyrics detection failed, continuing without lyrics metadata")\n                return True  # Non-fatal failure\n            \n            self.logger.info("\u2713 Lyrics detection complete")\n            return True\n            \n        except Exception as e:\n            self.logger.error(f"Lyrics detection error: {e}", exc_info=True)\n            if self.debug:\n                self.logger.debug(traceback.format_exc())\n            self.logger.warning("Continuing pipeline without lyrics metadata")\n            return True  # Non-fatal, graceful degradation\n    \n    def _stage_pyannote_vad(self) -&gt; bool:\n        """Stage 3: PyAnnote VAD for high-quality speech detection"""\n        \n        # Determine input audio source (from source_separation or demux)\n        sep_audio = self._stage_path("source_separation") / "audio.wav"\n        demux_audio = self.job_dir / "01_demux" / "audio.wav"\n        \n        if sep_audio.exists():\n            audio_file = sep_audio\n            audio_source = "source-separated vocals (clean speech)"\n        elif demux_audio.exists():\n            audio_file = demux_audio\n            audio_source = "original audio from demux"\n        else:\n            self.logger.error("No audio file found from previous stages")\n            self.logger.error(f"Checked: {sep_audio}")\n            self.logger.error(f"Checked: {demux_audio}")\n            return False\n        \n        # Output directory\n        output_dir = self._stage_path("pyannote_vad")\n        output_dir.mkdir(parents=True, exist_ok=True)\n        \n        # Log input/output\n        self.logger.info(f"\U0001f4e5 Input: {audio_file.relative_to(self.job_dir)}")\n        self.logger.info(f"\U0001f4e4 Output: {output_dir.relative_to(self.job_dir)}/")\n        self.logger.info(f"Running PyAnnote VAD for voice activity detection...")\n        self.logger.info(f"Using {audio_source}")\n        \n        # Get device configuration\n        device_config = self.env_config.get("WHISPERX_DEVICE", self.main_config.device_whisperx)\n        # PyAnnote works on CPU, CUDA, MPS\n        device = device_config.lower()\n        \n        self.logger.info(f"PyAnnote VAD device: {device}")\n        self.logger.info("Using PyAnnote for highest quality speech detection")\n        self.logger.info("This improves transcription accuracy, especially for movies with music/noise")\n        \n        try:\n            import subprocess\n            \n            # Get Python executable from PyAnnote environment (dedicated for VAD)\n            python_exe = self.env_manager.get_python_executable("pyannote")\n            self.logger.info(f"Using PyAnnote environment: {python_exe}")\n            \n            # Build path to job config file\n            job_id = self.job_config["job_id"]\n            job_config_file = self.job_dir / f".{job_id}.env"\n            \n            # Set up environment\n            env = os.environ.copy()\n            env[\'CONFIG_PATH\'] = str(job_config_file)\n            env[\'OUTPUT_DIR\'] = str(self.job_dir)\n            env[\'PYANNOTE_DEVICE\'] = device\n            env[\'DEBUG_MODE\'] = \'true\' if self.debug else \'false\'\n            env[\'LOG_LEVEL\'] = \'DEBUG\' if self.debug else \'INFO\'\n            env[\'AUDIO_INPUT\'] = str(audio_file)  # Pass audio path directly\n            env[\'VAD_OUTPUT_DIR\'] = str(output_dir)  # Pass VAD output directory\n            \n            # Run PyAnnote VAD script\n            script_path = PROJECT_ROOT / "scripts" / "05_pyannote_vad.py"\n            \n            result = subprocess.run(\n                [str(python_exe), str(script_path)],\n                capture_output=True,\n                text=True,\n                check=True,\n                cwd=str(PROJECT_ROOT),\n                env=env\n            )\n            \n            if self.debug and result.stdout:\n                self.logger.debug(f"PyAnnote VAD output: {result.stdout}")\n            \n            # Check for output\n            segments_file = output_dir / "speech_segments.json"\n            if segments_file.exists():\n                # Read and log statistics\n                import json\n                with open(segments_file) as f:\n                    vad_data = json.load(f)\n                \n                if \'segments\' in vad_data:\n                    num_segments = len(vad_data[\'segments\'])\n                    self.logger.info(f"VAD detected {num_segments} speech segments")\n                    \n                    # Calculate total speech duration\n                    total_duration = sum(seg[\'end\'] - seg[\'start\'] for seg in vad_data[\'segments\'])\n                    self.logger.info(f"Total speech duration: {total_duration:.1f}s")\n                else:\n                    self.logger.warning("VAD output missing \'segments\' key")\n                \n                self.logger.info(f"\u2713 PyAnnote VAD completed: {segments_file}")\n                return True\n            else:\n                self.logger.error("PyAnnote VAD failed - no output file generated")\n                self.logger.error("Pipeline cannot continue without VAD preprocessing")\n                return False\n                \n        except subprocess.CalledProcessError as e:\n            self.logger.error(f"PyAnnote VAD error: {e.stderr}", exc_info=True)\n            self.logger.error("Pipeline cannot continue without VAD preprocessing", exc_info=True)\n            return False\n        except Exception as e:\n            self.logger.error(f"Unexpected error in PyAnnote VAD: {e}", exc_info=True)\n            self.logger.error("Pipeline cannot continue without VAD preprocessing", exc_info=True)\n            return False\n    \n    def _stage_asr(self) -&gt; bool:\n        """Stage 4: Transcribe audio using WhisperX or MLX-Whisper"""\n        \n        # Determine input audio source (from source_separation or demux)\n        sep_audio = self._stage_path("source_separation") / "audio.wav"\n        demux_audio = self.job_dir / "01_demux" / "audio.wav"\n        \n        if sep_audio.exists():\n            audio_file = sep_audio\n            audio_source = "source-separated vocals (clean speech)"\n        elif demux_audio.exists():\n            audio_file = demux_audio\n            audio_source = "original audio from demux"\n        else:\n            self.logger.error("No audio file found from previous stages")\n            self.logger.error(f"Checked: {sep_audio}")\n            self.logger.error(f"Checked: {demux_audio}")\n            return False\n        \n        # Output directory\n        output_dir = self._stage_path("asr")\n        output_dir.mkdir(parents=True, exist_ok=True)\n        \n        source_lang = self.job_config["source_language"]\n        \n        # Load VAD segments if available\n        vad_segments = None\n        vad_file = self._stage_path("pyannote_vad") / "speech_segments.json"\n        vad_info = ""\n        if vad_file.exists():\n            try:\n                import json\n                with open(vad_file) as f:\n                    vad_data = json.load(f)\n                if \'segments\' in vad_data and vad_data[\'segments\']:\n                    vad_segments = vad_data[\'segments\']\n                    vad_info = f" + VAD segments ({len(vad_segments)})"\n                else:\n                    self.logger.warning("VAD file exists but has no segments, transcribing full audio")\n            except Exception as e:\n                self.logger.warning(f"Failed to load VAD segments: {e}")\n                self.logger.warning("Proceeding with full audio transcription")\n        \n        # Log input/output\n        self.logger.info(f"\U0001f4e5 Input: {audio_file.relative_to(self.job_dir)}{vad_info}")\n        self.logger.info(f"\U0001f4e4 Output: {output_dir.relative_to(self.job_dir)}/")\n        self.logger.info(f"Transcribing audio...")\n        self.logger.info(f"Using {audio_source}")\n        \n        # Get configuration from job\'s .env file (set by prepare-job)\n        # Fall back to main config if not set in job\n        device_config = self.env_config.get("WHISPERX_DEVICE", self.main_config.device_whisperx)\n        whisper_model = self.env_config.get("WHISPER_MODEL", self.main_config.whisperx_model)\n        compute_type = self.env_config.get("WHISPER_COMPUTE_TYPE", self.main_config.whisper_compute_type)\n        batch_size = int(self.env_config.get("BATCH_SIZE", str(self.main_config.batch_size)))\n        backend = self.env_config.get("WHISPER_BACKEND", self.main_config.whisper_backend)\n        \n        self.logger.info(f"Configured device: {device_config} (from job config)")\n        self.logger.info(f"Using model: {whisper_model} (from job config)")\n        self.logger.info(f"Compute type: {compute_type} (from job config)")\n        self.logger.info(f"Batch size: {batch_size} (from job config)")\n        self.logger.info(f"Backend: {backend} (from job config)")\n        \n        # Dynamic environment selection based on backend\n        asr_env = self.env_manager.get_asr_environment(backend)\n        self.logger.info(f"Using ASR environment: {asr_env}")\n        \n        # Get Python executable from selected environment\n        try:\n            python_exe = self.env_manager.get_python_executable(asr_env)\n            self.logger.info(f"Using WhisperX environment: {python_exe}")\n        except (ValueError, FileNotFoundError) as e:\n            self.logger.error(f"Failed to get Python executable for {asr_env} environment: {e}", exc_info=True)\n            self.logger.error("Run bootstrap.sh to set up environments", exc_info=True)\n            return False\n        \n        # Run 06_whisperx_asr.py stage script in the selected environment\n        asr_script = self.scripts_dir / "06_whisperx_asr.py"\n        if not asr_script.exists():\n            self.logger.error(f"ASR script not found: {asr_script}", exc_info=True)\n            return False\n        \n        # Run ASR stage with subprocess\n        self.logger.info(f"Running stage \'asr\' in environment \'{asr_env}\'")\n        env = os.environ.copy()\n        env["OUTPUT_DIR"] = str(self.job_dir)\n        env["PYTHONPATH"] = f"{PROJECT_ROOT}:{env.get(\'PYTHONPATH\', \'\')}"\n        \n        result = subprocess.run(\n            [str(python_exe), str(asr_script)],\n            env=env,\n            capture_output=True,\n            text=True\n        )\n        \n        if result.returncode != 0:\n            self.logger.error(f"ASR stage failed with exit code {result.returncode}")\n            if result.stderr:\n                self.logger.error(f"Error output: {result.stderr}")\n            return False\n        \n        # Add retry logic for file detection with exponential backoff\n        import time\n        segments_file = output_dir / "segments.json"\n        \n        # Retry up to 5 times with exponential backoff\n        for attempt in range(5):\n            if segments_file.exists():\n                break\n            if attempt &lt; 4:\n                wait_time = 0.1 * (2 ** attempt)  # 0.1, 0.2, 0.4, 0.8, 1.6 seconds\n                self.logger.debug(f"segments.json not found, retrying in {wait_time}s (attempt {attempt+1}/5)")\n                time.sleep(wait_time)\n            else:\n                self.logger.error(f"Transcription failed - segments.json not found after 5 attempts")\n                self.logger.error(f"  Checked: {segments_file}")\n                self.logger.error(f"  Directory contents: {list(output_dir.glob(\'*\'))}")\n                return False\n        \n        # File exists, proceed with verification and copy\n        file_size = segments_file.stat().st_size\n        self.logger.info(f"\u2713 Transcription completed: {segments_file.relative_to(self.job_dir)}")\n        self.logger.info(f"  File size: {file_size} bytes")\n        \n        # Copy to transcripts/ for compatibility\n        import shutil\n        transcripts_dir = self.job_dir / "transcripts"\n        transcripts_dir.mkdir(parents=True, exist_ok=True)\n        dest_file = transcripts_dir / "segments.json"\n        shutil.copy2(segments_file, dest_file)\n        \n        # Verify copy\n        if dest_file.exists() and dest_file.stat().st_size == file_size:\n            self.logger.info(f"\u2713 Copied to: transcripts/segments.json ({file_size} bytes)")\n            return True\n        else:\n            self.logger.error(f"Copy verification failed")\n            self.logger.error(f"  Source: {segments_file} ({file_size} bytes)")\n            self.logger.error(f"  Dest exists: {dest_file.exists()}")\n            if dest_file.exists():\n                self.logger.error(f"  Dest size: {dest_file.stat().st_size} bytes")\n            return False\n    \n    def _stage_asr_mlx(self, audio_file: Path, output_dir: Path, \n                       source_lang: str, model: str, vad_segments: list = None) -&gt; bool:\n        """ASR using MLX-Whisper (Apple Silicon MPS acceleration)\n        \n        Args:\n            vad_segments: Optional list of speech segments from PyAnnote VAD\n                         Format: [{"start": 0.5, "end": 3.2}, ...]\n        """\n        \n        # Map model names to MLX format\n        model_map = {\n            "large-v3": "mlx-community/whisper-large-v3-mlx",\n            "large-v2": "mlx-community/whisper-large-v2-mlx",\n            "large": "mlx-community/whisper-large-v3-mlx",\n            "medium": "mlx-community/whisper-medium-mlx",\n            "small": "mlx-community/whisper-small-mlx",\n            "base": "mlx-community/whisper-base-mlx",\n            "tiny": "mlx-community/whisper-tiny-mlx",\n        }\n        mlx_model = model_map.get(model, model)\n        \n        self.logger.info(f"Mapping model \'{model}\' to MLX format: \'{mlx_model}\'")\n        \n        script_content = f"""\nimport mlx_whisper\nimport json\nfrom pathlib import Path\n\n# Load audio and transcribe with MLX (MPS acceleration)\naudio_file = "{audio_file}"\noutput_dir = Path("{output_dir}")\noutput_dir.mkdir(exist_ok=True)\n\nlogger.info("Loading MLX-Whisper model: {mlx_model}")\nlogger.info("Using MPS (Apple Silicon GPU) acceleration")\n\n# Transcribe with MLX-Whisper\n# This automatically uses MPS for acceleration\nresult = mlx_whisper.transcribe(\n    str(audio_file),\n    path_or_hf_repo="{mlx_model}",\n    language="{source_lang}",\n    verbose={\'True\' if self.debug else \'False\'}\n)\n\n# Convert to WhisperX-compatible format\nsegments = []\nif "segments" in result:\n    for seg in result["segments"]:\n        segments.append({{\n            "start": seg["start"],\n            "end": seg["end"],\n            "text": seg["text"],\n            "words": []  # MLX-Whisper doesn\'t provide word-level timestamps by default\n        }})\n\noutput = {{\n    "segments": segments,\n    "language": result.get("language", "{source_lang}"),\n    "text": result.get("text", "")\n}}\n\n# Save segments\nsegments_file = output_dir / "segments.json"\nwith open(segments_file, \'w\', encoding=\'utf-8\') as f:\n    json.dump(output, f, indent=2, ensure_ascii=False)\n\nlogger.info(f"Transcription completed: {{len(segments)}} segments")\n"""\n        \n        # Write script to temp file\n        temp_script = output_dir / "asr_mlx_temp.py"\n        with open(temp_script, \'w\') as f:\n            f.write(script_content)\n        \n        try:\n            # Run the MLX script in venv/mlx environment\n            import subprocess\n            \n            # Get Python executable from MLX environment\n            python_exe = self.env_manager.get_python_executable("mlx")\n            self.logger.info(f"Using MLX environment: {python_exe}")\n            \n            # Set up environment with debug flag\n            env = os.environ.copy()\n            env[\'DEBUG_MODE\'] = \'true\' if self.debug else \'false\'\n            env[\'LOG_LEVEL\'] = \'DEBUG\' if self.debug else \'INFO\'\n            \n            result = subprocess.run(\n                [str(python_exe), str(temp_script)],\n                capture_output=True,\n                text=True,\n                check=True,\n                cwd=str(PROJECT_ROOT),\n                env=env\n            )\n            \n            self.logger.info(f"MLX-Whisper output: {result.stdout}")\n            \n            # Check for output\n            segments_file = output_dir / "segments.json"\n            if segments_file.exists():\n                self.logger.info(f"\u2713 Transcription completed: {segments_file.relative_to(self.job_dir)}")\n                \n                # Copy to transcripts/ for compatibility\n                transcripts_dir = self.job_dir / "transcripts"\n                transcripts_dir.mkdir(parents=True, exist_ok=True)\n                import shutil\n                shutil.copy2(segments_file, transcripts_dir / "segments.json")\n                self.logger.info(f"\u2713 Copied to: transcripts/segments.json")\n                \n                return True\n            else:\n                self.logger.error("Transcription failed - no output")\n                return False\n                \n        except subprocess.CalledProcessError as e:\n            self.logger.error(f"MLX-Whisper error: {e.stderr}", exc_info=True)\n            return False\n        finally:\n            # Clean up temp script\n            if temp_script.exists():\n                temp_script.unlink()\n    \n    def _stage_asr_whisperx(self, audio_file: Path, output_dir: Path,\n                           source_lang: str, model: str, device: str,\n                           compute_type: str, batch_size: int, vad_segments: list = None) -&gt; bool:\n        """ASR using WhisperX (faster-whisper/CTranslate2)\n        \n        Calls the proper whisperx_asr stage script to get full functionality\n        including bias term support and proper parameter handling.\n        \n        Args:\n            vad_segments: Optional list of speech segments from PyAnnote VAD\n                         Format: [{"start": 0.5, "end": 3.2}, ...]\n        """\n        \n        import subprocess\n        \n        # Get Python executable from WhisperX environment\n        python_exe = self.env_manager.get_python_executable("whisperx")\n        self.logger.info(f"Using WhisperX environment: {python_exe}")\n        \n        # Use the proper 06_whisperx_asr script that supports all features\n        asr_script = self.scripts_dir / "06_whisperx_asr.py"\n        \n        if not asr_script.exists():\n            self.logger.error(f"ASR script not found: {asr_script}")\n            return False\n        \n        # Set up environment variables for the stage\n        env = os.environ.copy()\n        env[\'DEBUG_MODE\'] = \'true\' if self.debug else \'false\'\n        env[\'LOG_LEVEL\'] = \'DEBUG\' if self.debug else \'INFO\'\n        env[\'CONFIG_PATH\'] = str(self.job_dir / \'job.json\')\n        env[\'JOB_DIR\'] = str(self.job_dir)\n        env[\'OUTPUT_DIR\'] = str(self.job_dir)  # StageIO uses OUTPUT_DIR\n        env[\'DEVICE_OVERRIDE\'] = device\n        \n        try:\n            result = subprocess.run(\n                [str(python_exe), str(asr_script)],\n                capture_output=True,\n                text=True,\n                check=True,\n                cwd=str(PROJECT_ROOT),\n                env=env\n            )\n            \n            self.logger.info(f"ASR output: {result.stdout}")\n            \n            # Check for output\n            segments_file = output_dir / "segments.json"\n            if segments_file.exists():\n                self.logger.info(f"\u2713 Transcription completed: {segments_file.relative_to(self.job_dir)}")\n                \n                # Copy to transcripts/ for compatibility\n                transcripts_dir = self.job_dir / "transcripts"\n                transcripts_dir.mkdir(parents=True, exist_ok=True)\n                import shutil\n                shutil.copy2(segments_file, transcripts_dir / "segments.json")\n                self.logger.info(f"\u2713 Copied to: transcripts/segments.json")\n                \n                return True\n            else:\n                self.logger.error("Transcription failed - no output")\n                return False\n                \n        except subprocess.CalledProcessError as e:\n            self.logger.error(f"ASR error: {e.stderr}", exc_info=True)\n            return False\n    \n    def _stage_alignment(self) -&gt; bool:\n        """Stage 5: Word-level alignment"""\n        \n        # Read from ASR stage output\n        segments_file = self._stage_path("asr") / "segments.json"\n        output_dir = self._stage_path("alignment")\n        output_dir.mkdir(parents=True, exist_ok=True)\n        \n        # Log input/output\n        self.logger.info(f"\U0001f4e5 Input: {segments_file.relative_to(self.job_dir)}")\n        self.logger.info(f"\U0001f4e4 Output: {output_dir.relative_to(self.job_dir)}/")\n        \n        if not segments_file.exists():\n            self.logger.error(f"Segments file not found: {segments_file}")\n            return False\n        \n        # Load segments\n        with open(segments_file) as f:\n            raw_data = json.load(f)\n        \n        # Normalize to consistent format\n        data, segments = normalize_segments_data(raw_data)\n        \n        if not segments:\n            self.logger.error("No segments found in transcript")\n            return False\n        \n        # Check if segments already have word-level timestamps\n        has_words = segments[0].get("words", []) if segments else []\n        backend = self.env_config.get("WHISPER_BACKEND", self.main_config.whisper_backend)\n        \n        if has_words and len(has_words) &gt; 0:\n            # Already aligned\n            total_words = sum(len(seg.get("words", [])) for seg in segments)\n            self.logger.info(f"\u2713 Segments already have word-level timestamps")\n            self.logger.info(f"  Segments: {len(segments)}, Words: {total_words}")\n            \n            # Copy to alignment output\n            output_file = output_dir / "segments_aligned.json"\n            with open(output_file, \'w\', encoding=\'utf-8\') as f:\n                json.dump(data, f, indent=2, ensure_ascii=False)\n            \n            self.logger.info(f"\u2713 Alignment verified and saved: {output_file.relative_to(self.job_dir)}")\n            return True\n        \n        # Need to perform alignment\n        self.logger.info(f"\u26a0\ufe0f  Segments missing word-level timestamps")\n        self.logger.info(f"Backend: {backend}")\n        \n        if backend == "mlx":\n            # Use MLX alignment\n            self.logger.info("Performing word-level alignment with MLX-Whisper...")\n            return self._perform_mlx_alignment(segments_file, output_dir)\n        else:\n            # WhisperX should have provided word timestamps already\n            self.logger.warning("WhisperX backend should provide word timestamps during transcription")\n            self.logger.warning("Proceeding without word-level alignment")\n            \n            # Copy segments anyway\n            output_file = output_dir / "segments_aligned.json"\n            with open(output_file, \'w\', encoding=\'utf-8\') as f:\n                json.dump(data, f, indent=2, ensure_ascii=False)\n            \n            return True\n    \n    def _perform_mlx_alignment(self, segments_file: Path, output_dir: Path) -&gt; bool:\n        """Perform word-level alignment using MLX-Whisper"""\n        \n        # Determine audio source\n        sep_audio = self._stage_path("source_separation") / "audio.wav"\n        demux_audio = self.job_dir / "01_demux" / "audio.wav"\n        \n        if sep_audio.exists():\n            audio_file = sep_audio\n        elif demux_audio.exists():\n            audio_file = demux_audio\n        else:\n            self.logger.error("No audio file found for alignment")\n            return False\n        \n        source_lang = self.job_config["source_language"]\n        whisper_model = self.env_config.get("WHISPER_MODEL", self.main_config.whisperx_model)\n        \n        # Map model names to MLX format\n        model_map = {\n            "large-v3": "mlx-community/whisper-large-v3-mlx",\n            "large-v2": "mlx-community/whisper-large-v2-mlx",\n            "large": "mlx-community/whisper-large-v3-mlx",\n            "medium": "mlx-community/whisper-medium-mlx",\n            "small": "mlx-community/whisper-small-mlx",\n            "base": "mlx-community/whisper-base-mlx",\n            "tiny": "mlx-community/whisper-tiny-mlx",\n        }\n        mlx_model = model_map.get(whisper_model, whisper_model)\n        \n        output_file = output_dir / "segments_aligned.json"\n        \n        self.logger.info(f"Audio: {audio_file.relative_to(self.job_dir)}")\n        self.logger.info(f"Model: {mlx_model}")\n        self.logger.info(f"Language: {source_lang}")\n        \n        # Use mlx_alignment.py script\n        alignment_script = self.scripts_dir / "mlx_alignment.py"\n        \n        if not alignment_script.exists():\n            self.logger.error(f"MLX alignment script not found: {alignment_script}")\n            return False\n        \n        # Get Python executable from MLX environment\n        python_exe = self.env_manager.get_python_executable("mlx")\n        \n        try:\n            import subprocess\n            \n            cmd = [\n                str(python_exe),\n                str(alignment_script),\n                str(audio_file),\n                str(segments_file),\n                str(output_file),\n                "--model", mlx_model,\n                "--language", source_lang\n            ]\n            \n            if self.debug:\n                cmd.append("--debug")\n            \n            result = subprocess.run(\n                cmd,\n                capture_output=True,\n                text=True,\n                check=True,\n                cwd=str(PROJECT_ROOT)\n            )\n            \n            if self.debug:\n                self.logger.debug(f"Alignment output: {result.stdout}")\n            \n            # Verify output\n            if output_file.exists():\n                with open(output_file) as f:\n                    raw_data = json.load(f)\n                \n                data, segments = normalize_segments_data(raw_data)\n                total_words = sum(len(seg.get("words", [])) for seg in segments)\n                \n                self.logger.info(f"\u2713 Alignment completed: {len(segments)} segments, {total_words} words")\n                self.logger.info(f"\u2713 Output saved: {output_file.relative_to(self.job_dir)}")\n                return True\n            else:\n                self.logger.error("Alignment output file not created")\n                return False\n                \n        except subprocess.CalledProcessError as e:\n            self.logger.error(f"MLX alignment failed: {e}", exc_info=True)\n            if e.stderr:\n                self.logger.error(f"Error output: {e.stderr}", exc_info=True)\n            return False\n    \n    \n    def _stage_ner(self) -&gt; bool:\n        """Stage 5: Named Entity Recognition using new modular stage"""\n        \n        try:\n            # Check if NER is enabled\n            ner_enabled = self.env_config.get("STAGE_05_NER_ENABLED", "true").lower() == "true"\n            \n            if not ner_enabled:\n                self.logger.info("NER stage is disabled (skipping)")\n                return True\n            \n            # Import NER stage module (module name starts with number, use importlib)\n            import importlib\n            ner_stage = importlib.import_module("scripts.05_ner")\n            \n            # Call the stage module\n            self.logger.info("Running NER stage...")\n            exit_code = ner_stage.run_stage(self.job_dir, "05_ner")\n            \n            if exit_code != 0:\n                self.logger.warning("NER stage failed, continuing without NER data")\n                return True  # Non-fatal failure\n            \n            self.logger.info("\u2713 NER stage complete")\n            return True\n            \n        except Exception as e:\n            self.logger.error(f"NER stage error: {e}", exc_info=True)\n            if self.debug:\n                self.logger.debug(traceback.format_exc())\n            self.logger.warning("Continuing without NER data")\n            return True  # Non-fatal, graceful degradation\n    \n    def _stage_subtitle_gen(self) -&gt; bool:\n        """Stage 9: Subtitle generation using new modular stage"""\n        \n        try:\n            # Check if subtitle generation is enabled\n            subtitle_enabled = self.env_config.get("STAGE_09_SUBTITLE_ENABLED", "true").lower() == "true"\n            \n            if not subtitle_enabled:\n                self.logger.info("Subtitle generation is disabled (skipping)")\n                return True\n            \n            # Import subtitle generation stage module (module name starts with number, use importlib)\n            import importlib\n            subtitle_gen = importlib.import_module("scripts.09_subtitle_gen")\n            \n            # Call the stage module\n            self.logger.info("Running subtitle generation stage...")\n            exit_code = subtitle_gen.run_stage(self.job_dir, "09_subtitle_gen")\n            \n            if exit_code != 0:\n                self.logger.error("Subtitle generation failed")\n                return False  # Fatal failure\n            \n            self.logger.info("\u2713 Subtitle generation complete")\n            return True\n            \n        except Exception as e:\n            self.logger.error(f"Subtitle generation error: {e}", exc_info=True)\n            if self.debug:\n                self.logger.debug(traceback.format_exc())\n            return False  # Fatal failure\n    \n    def _stage_export_transcript(self) -&gt; bool:\n        """Stage: Export plain text transcript"""\n        \n        # Read from ASR stage output (already copied to transcripts/)\n        segments_file = self.job_dir / "transcripts" / "segments.json"\n        output_txt = self.job_dir / "transcripts" / "transcript.txt"\n        \n        # Log input/output\n        self.logger.info(f"\U0001f4e5 Input: {segments_file.relative_to(self.job_dir)}")\n        self.logger.info(f"\U0001f4e4 Output: {output_txt.relative_to(self.job_dir)}")\n        self.logger.info("Exporting plain text transcript...")\n        \n        if not segments_file.exists():\n            self.logger.error(f"Segments file not found: {segments_file}")\n            return False\n        \n        try:\n            with open(segments_file) as f:\n                data = json.load(f)\n            \n            if "segments" not in data:\n                self.logger.error("No segments in JSON file")\n                return False\n            \n            # Extract text from all segments\n            lines = []\n            for segment in data["segments"]:\n                text = segment.get("text", "").strip()\n                if text:\n                    lines.append(text)\n            \n            # Write to text file\n            with open(output_txt, \'w\', encoding=\'utf-8\') as f:\n                f.write("\\n".join(lines))\n            \n            self.logger.info(f"\u2713 Plain text transcript exported: {output_txt.name}")\n            self.logger.info(f"Total lines: {len(lines)}")\n            return True\n            \n        except Exception as e:\n            self.logger.error(f"Error exporting transcript: {e}", exc_info=True)\n            return False\n    \n    def _stage_load_transcript(self) -&gt; bool:\n        """Stage: Load transcript from ASR stage"""\n        \n        # Prefer cleaned transcript from transcripts/ (after hallucination removal)\n        # Fall back to raw ASR output if not available\n        transcript_file = self.job_dir / "transcripts" / "segments.json"\n        segments_file = self._stage_path("asr") / "segments.json"\n        \n        # Use cleaned transcript if available, otherwise raw ASR output\n        if transcript_file.exists():\n            load_file = transcript_file\n            self.logger.info("Using cleaned transcript (after hallucination removal)")\n        elif segments_file.exists():\n            load_file = segments_file\n            self.logger.info("Using raw ASR transcript")\n        else:\n            self.logger.error("Transcript not found in transcripts/ or asr stage!")\n            self.logger.error("Run transcribe workflow first!")\n            return False\n        \n        # Log input\n        self.logger.info(f"\U0001f4e5 Input: {load_file.relative_to(self.job_dir)}")\n        self.logger.info("Loading transcript...")\n        \n        with open(load_file) as f:\n            raw_data = json.load(f)\n        \n        # Normalize data format (handles both list and dict formats)\n        data, segments = normalize_segments_data(raw_data)\n        \n        if not segments or len(segments) == 0:\n            self.logger.error("No segments in transcript")\n            return False\n        \n        self.logger.info(f"Loaded {len(segments)} segments")\n        return True\n    \n    def _stage_hybrid_translation(self) -&gt; bool:\n        """\n        Stage: Hybrid translation - IndicTrans2 for dialogue, LLM for songs\n        \n        Combines:\n        - IndicTrans2 for dialogue (fast, accurate, free)\n        - LLM with film context for songs/poetry (creative, culturally aware)\n        \n        Automatically routes segments based on lyrics detection results.\n        Falls back to standard IndicTrans2 if LLM unavailable or disabled.\n        """\n        self.logger.info("Running hybrid translation...")\n        \n        # Check if enabled\n        use_hybrid = self.env_config.get("USE_HYBRID_TRANSLATION", "true").lower() == "true"\n        if not use_hybrid:\n            self.logger.info("Hybrid translation disabled, using standard IndicTrans2")\n            return self._stage_indictrans2_translation()\n        \n        # Get configuration\n        use_llm_for_songs = self.env_config.get("USE_LLM_FOR_SONGS", "true").lower() == "true"\n        llm_provider = self.env_config.get("LLM_PROVIDER", "anthropic")\n        source_lang = self.job_config["source_language"]\n        # Handle both target_language (singular) and target_languages (plural)\n        target_lang = self._get_target_language()\n        if not target_lang:\n            target_langs = self.job_config.get("target_languages", [])\n            target_lang = target_langs[0] if target_langs else None\n        \n        if not target_lang:\n            raise ValueError("No target language specified in job config")\n        \n        self.logger.info(f"Configuration:")\n        self.logger.info(f"  Translation: {source_lang} \u2192 {target_lang}")\n        self.logger.info(f"  LLM provider: {llm_provider}")\n        self.logger.info(f"  LLM for songs: {use_llm_for_songs}")\n        \n        # Get film context\n        film_title = self.job_config.get("title", "")\n        film_year = self.job_config.get("year", "")\n        film_context = None\n        \n        if film_title and film_year:\n            prompt_file = PROJECT_ROOT / "glossary" / "prompts" / f"{film_title.lower().replace(\' \', \'_\')}_{film_year}.txt"\n            if prompt_file.exists():\n                with open(prompt_file, \'r\', encoding=\'utf-8\') as f:\n                    film_context = f.read()\n                self.logger.info(f"\u2713 Loaded film context: {prompt_file.name}")\n            else:\n                self.logger.info(f"No film context found: {prompt_file.name}")\n        \n        # Input/output files (prefer lyrics-enhanced segments if available)\n        segments_file = self._stage_path("lyrics_detection") / "segments.json"\n        if not segments_file.exists():\n            segments_file = self.job_dir / "lyrics_detection" / "segments.json"\n        if not segments_file.exists():\n            segments_file = self._stage_path("asr") / "segments.json"\n        \n        if not segments_file.exists():\n            self.logger.error(f"Segments file not found: {segments_file}")\n            return False\n        \n        # Output to translation stage directory\n        output_dir = self._stage_path("translation")\n        output_dir.mkdir(parents=True, exist_ok=True)\n        output_file = output_dir / f"segments_{target_lang}.json"\n        \n        # Log input/output\n        self.logger.info(f"\U0001f4e5 Input: {segments_file.relative_to(self.job_dir)}")\n        self.logger.info(f"\U0001f4e4 Output: {output_file.relative_to(self.job_dir)}")\n        \n        try:\n            # Get Python executable from LLM environment\n            python_exe = self.env_manager.get_python_executable("llm")\n            self.logger.info(f"Using LLM environment: {python_exe}")\n            \n            # Build environment variables\n            env = os.environ.copy()\n            env[\'OUTPUT_DIR\'] = str(self.job_dir)  # CRITICAL: Tell script where job directory is\n            env[\'CONFIG_PATH\'] = str(self.job_dir / f".{self.job_config[\'job_id\']}.env")\n            env[\'SOURCE_LANG\'] = source_lang\n            env[\'TARGET_LANG\'] = target_lang\n            env[\'USE_LLM_FOR_SONGS\'] = str(use_llm_for_songs).lower()\n            env[\'LLM_PROVIDER\'] = llm_provider\n            env[\'DEBUG_MODE\'] = \'true\' if self.debug else \'false\'\n            env[\'LOG_LEVEL\'] = \'DEBUG\' if self.debug else \'INFO\'\n            env[\'SEGMENTS_FILE\'] = str(segments_file)\n            env[\'OUTPUT_FILE\'] = str(output_file)\n            env[\'JOB_DIR\'] = str(self.job_dir)\n            \n            if film_title:\n                env[\'FILM_TITLE\'] = film_title\n            if film_year:\n                env[\'FILM_YEAR\'] = str(film_year)\n            \n            # Pass glossary snapshot if available\n            if hasattr(self, \'glossary_manager\') and self.glossary_manager:\n                glossary_snapshot = self._stage_path("glossary_load") / "glossary_snapshot.json"\n                if glossary_snapshot.exists():\n                    env[\'GLOSSARY_SNAPSHOT\'] = str(glossary_snapshot)\n                    self.logger.info(f"Using glossary snapshot for translation")\n            \n            # Run hybrid translator\n            script_path = PROJECT_ROOT / "scripts" / "hybrid_translator.py"\n            \n            self.logger.info(f"Running: {script_path}")\n            \n            result = subprocess.run(\n                [str(python_exe), str(script_path)],\n                capture_output=True,\n                text=True,\n                check=True,\n                cwd=str(PROJECT_ROOT),\n                env=env\n            )\n            \n            if self.debug and result.stdout:\n                self.logger.debug(f"Hybrid translation output:\\n{result.stdout}")\n            \n            if output_file.exists():\n                # Load and report statistics\n                with open(output_file, \'r\', encoding=\'utf-8\') as f:\n                    raw_data = json.load(f)\n                \n                data, segments = normalize_segments_data(raw_data)\n                \n                stats = data.get(\'translation_stats\', {})\n                if stats:\n                    self.logger.info(f"Translation statistics:")\n                    self.logger.info(f"  Total segments: {stats.get(\'total_segments\', 0)}")\n                    self.logger.info(f"  Dialogue segments: {stats.get(\'dialogue_segments\', 0)}")\n                    self.logger.info(f"  Song segments: {stats.get(\'song_segments\', 0)}")\n                    self.logger.info(f"  IndicTrans2 used: {stats.get(\'indictrans2_used\', 0)}")\n                    self.logger.info(f"  LLM used: {stats.get(\'llm_used\', 0)}")\n                \n                # Apply glossary post-processing if available\n                if hasattr(self, \'glossary_manager\') and self.glossary_manager:\n                    try:\n                        glossary_applied_count = 0\n                        for segment in segments:\n                            if \'text\' in segment:\n                                original_text = segment[\'text\']\n                                polished_text = self.glossary_manager.apply_to_text(\n                                    original_text,\n                                    context="translation"\n                                )\n                                if polished_text != original_text:\n                                    segment[\'text\'] = polished_text\n                                    glossary_applied_count += 1\n                        \n                        if glossary_applied_count &gt; 0:\n                            # Save the glossary-enhanced version\n                            with open(output_file, \'w\', encoding=\'utf-8\') as f:\n                                json.dump(data, f, indent=2, ensure_ascii=False)\n                            self.logger.info(f"\u2713 Glossary applied to {glossary_applied_count} segments")\n                    except Exception as e:\n                        self.logger.warning(f"Failed to apply glossary: {e}")\n                \n                # Copy to transcripts/ for compatibility\n                transcripts_dir = self.job_dir / "transcripts"\n                transcripts_dir.mkdir(parents=True, exist_ok=True)\n                import shutil\n                shutil.copy2(output_file, transcripts_dir / "segments_translated.json")\n                \n                self.logger.info(f"\u2713 Hybrid translation completed: {output_file.relative_to(self.job_dir)}")\n                self.logger.info(f"\u2713 Copied to: transcripts/segments_translated.json")\n                return True\n            else:\n                self.logger.error("Hybrid translation failed - no output file")\n                return False\n                \n        except subprocess.CalledProcessError as e:\n            self.logger.error(f"Hybrid translation error: {e.stderr}", exc_info=True)\n            self.logger.warning("Falling back to standard IndicTrans2")\n            return self._stage_indictrans2_translation()\n        except Exception as e:\n            self.logger.error(f"Unexpected error in hybrid translation: {e}", exc_info=True)\n            if self.debug:\n                import traceback\n                self.logger.debug(traceback.format_exc())\n            self.logger.warning("Falling back to standard IndicTrans2")\n            return self._stage_indictrans2_translation()\n    \n    def _stage_indictrans2_translation(self) -&gt; bool:\n        """Stage 7: Translate using IndicTrans2"""\n        \n        # Input from lyrics detection (if available) or ASR\n        segments_file = self._stage_path("lyrics_detection") / "segments.json"\n        if not segments_file.exists():\n            segments_file = self.job_dir / "lyrics_detection" / "segments.json"\n        if not segments_file.exists():\n            segments_file = self._stage_path("asr") / "segments.json"\n        \n        if not segments_file.exists():\n            self.logger.error(f"Segments file not found")\n            return False\n        \n        source_lang = self.job_config["source_language"]\n        target_lang = self._get_target_language()\n        \n        # Output to translation stage directory\n        output_dir = self._stage_path("translation")\n        output_dir.mkdir(parents=True, exist_ok=True)\n        output_file = output_dir / f"segments_{target_lang}.json"\n        \n        # Log input/output\n        self.logger.info(f"\U0001f4e5 Input: {segments_file.relative_to(self.job_dir)}")\n        self.logger.info(f"\U0001f4e4 Output: {output_file.relative_to(self.job_dir)}")\n        self.logger.info("Translating with IndicTrans2...")\n        \n        # Get configuration from job\'s .env file (set by prepare-job)\n        device = self.env_config.get("INDICTRANS2_DEVICE", self.main_config.indictrans2_device)\n        num_beams = self.env_config.get("INDICTRANS2_NUM_BEAMS", "4")\n        max_tokens = self.env_config.get("INDICTRANS2_MAX_NEW_TOKENS", "128")\n        \n        # Dynamically select model based on language pair (no hardcoding)\n        # Model selection happens in indictrans2_translator.py based on source/target\n        \n        self.logger.info(f"Using IndicTrans2 device: {device} (from job config)")\n        self.logger.info(f"Translation: {source_lang} \u2192 {target_lang}")\n        self.logger.info(f"Num beams: {num_beams} (from job config)")\n        \n        # Use existing IndicTrans2 translator in venv/indictrans2\n        # Get Python executable from IndicTrans2 environment\n        python_exe = self.env_manager.get_python_executable("indictrans2")\n        self.logger.info(f"Using IndicTrans2 environment: {python_exe}")\n        \n        cmd = [\n            str(python_exe), "-c",\n            f"""\nimport json\nfrom pathlib import Path\nfrom scripts.indictrans2_translator import translate_whisperx_result\n\n# Load segments\nwith open(\'{segments_file}\') as f:\n    segments = json.load(f)\n\n# Translate with job-configured settings\nfrom shared.logger import PipelineLogger\nlog_file = Path(\'{self.job_dir / "logs"}\') / \'indictrans2_translation.log\'\nlogger = PipelineLogger(module_name=\'indictrans2\', log_file=log_file, log_level=\'{"DEBUG" if self.debug else "INFO"}\')\n\n# Set device for IndicTrans2 (from job config)\nimport os\nos.environ[\'INDICTRANS2_DEVICE\'] = \'{device}\'\nos.environ[\'INDICTRANS2_NUM_BEAMS\'] = \'{num_beams}\'\nos.environ[\'INDICTRANS2_MAX_NEW_TOKENS\'] = \'{max_tokens}\'\n\n# translate_whisperx_result will auto-select the right model based on language pair\ntranslated = translate_whisperx_result(segments, \'{source_lang}\', \'{target_lang}\', logger)\n\n# Save\nwith open(\'{output_file}\', \'w\') as f:\n    json.dump(translated, f, indent=2)\n\nlogger.info(f"Translated {{len(translated[\'segments\'])}} segments")\n"""\n        ]\n        \n        try:\n            # Set up environment with debug flag\n            env = os.environ.copy()\n            env[\'DEBUG_MODE\'] = \'true\' if self.debug else \'false\'\n            env[\'LOG_LEVEL\'] = \'DEBUG\' if self.debug else \'INFO\'\n            \n            # Pass glossary snapshot if available\n            if hasattr(self, \'glossary_manager\') and self.glossary_manager:\n                glossary_snapshot = self._stage_path("glossary_load") / "glossary_snapshot.json"\n                if glossary_snapshot.exists():\n                    env[\'GLOSSARY_SNAPSHOT\'] = str(glossary_snapshot)\n                    self.logger.info(f"Using glossary snapshot for IndicTrans2 translation")\n            \n            result = subprocess.run(\n                cmd,\n                capture_output=True,\n                text=True,\n                check=True,\n                env=env\n            )\n            \n            if output_file.exists():\n                # Apply glossary post-processing if available\n                if hasattr(self, \'glossary_manager\') and self.glossary_manager:\n                    try:\n                        with open(output_file, \'r\', encoding=\'utf-8\') as f:\n                            raw_data = json.load(f)\n                        \n                        data, segments = normalize_segments_data(raw_data)\n                        \n                        glossary_applied_count = 0\n                        for segment in segments:\n                            if \'text\' in segment:\n                                original_text = segment[\'text\']\n                                polished_text = self.glossary_manager.apply_to_text(\n                                    original_text,\n                                    context="translation"\n                                )\n                                if polished_text != original_text:\n                                    segment[\'text\'] = polished_text\n                                    glossary_applied_count += 1\n                        \n                        if glossary_applied_count &gt; 0:\n                            # Save the glossary-enhanced version\n                            with open(output_file, \'w\', encoding=\'utf-8\') as f:\n                                json.dump(data, f, indent=2, ensure_ascii=False)\n                            self.logger.info(f"\u2713 Glossary applied to {glossary_applied_count} segments")\n                    except Exception as e:\n                        self.logger.warning(f"Failed to apply glossary: {e}")\n                \n                # Copy to transcripts/ for compatibility\n                transcripts_dir = self.job_dir / "transcripts"\n                transcripts_dir.mkdir(parents=True, exist_ok=True)\n                import shutil\n                shutil.copy2(output_file, transcripts_dir / "segments_translated.json")\n                \n                self.logger.info(f"\u2713 Translation completed: {output_file.relative_to(self.job_dir)}")\n                self.logger.info(f"\u2713 Copied to: transcripts/segments_translated.json")\n                return True\n            else:\n                self.logger.error("Translation failed")\n                return False\n                \n        except subprocess.CalledProcessError as e:\n            self.logger.error(f"Translation error: {e.stderr}", exc_info=True)\n            return False\n    \n    def _stage_subtitle_generation(self) -&gt; bool:\n        """Stage 8: Generate SRT subtitle file in target language"""\n        \n        target_lang = self._get_target_language()\n        \n        # Read from translation stage\n        segments_file = self._stage_path("translation") / f"segments_{target_lang}.json"\n        \n        if not segments_file.exists():\n            # Fallback to old location\n            segments_file = self.job_dir / "transcripts" / "segments_translated.json"\n        \n        if not segments_file.exists():\n            self.logger.error(f"Translated segments not found: {segments_file}")\n            return False\n        \n        # Output to subtitle generation stage directory\n        output_dir = self._stage_path("subtitle_generation")\n        output_dir.mkdir(parents=True, exist_ok=True)\n        \n        # Generate filename\n        title = self.job_config.get("title", "output")\n        output_srt = output_dir / f"{title}.{target_lang}.srt"\n        \n        # Log input/output\n        self.logger.info(f"\U0001f4e5 Input: {segments_file.relative_to(self.job_dir)}")\n        self.logger.info(f"\U0001f4e4 Output: {output_srt.relative_to(self.job_dir)}")\n        self.logger.info("Generating subtitles...")\n        \n        # Load translated segments\n        try:\n            with open(segments_file, \'r\', encoding=\'utf-8\') as f:\n                raw_data = json.load(f)\n                data, segments = normalize_segments_data(raw_data)\n        except Exception as e:\n            self.logger.error(f"Failed to load segments: {e}", exc_info=True)\n            return False\n        \n        # Generate SRT file\n        if generate_srt_from_segments(segments, output_srt):\n            # Copy to subtitles/ for compatibility\n            subtitles_dir = self.job_dir / "subtitles"\n            subtitles_dir.mkdir(parents=True, exist_ok=True)\n            final_output = subtitles_dir / output_srt.name\n            \n            # Only copy if source and destination are different\n            if output_srt != final_output:\n                import shutil\n                shutil.copy2(output_srt, final_output)\n                self.logger.info(f"\u2713 Subtitles generated: {output_srt.relative_to(self.job_dir)}")\n                self.logger.info(f"\u2713 Copied to: subtitles/{output_srt.name}")\n            else:\n                self.logger.info(f"\u2713 Subtitles generated: {output_srt.relative_to(self.job_dir)}")\n            \n            return True\n        else:\n            self.logger.error("Subtitle generation failed")\n            return False\n    \n    def _stage_subtitle_generation_source(self) -&gt; bool:\n        """Stage 8: Generate SRT subtitle file in source language"""\n        \n        source_lang = self.job_config["source_language"]\n        \n        # Read from ASR stage (or transcripts copy)\n        segments_file = self._stage_path("asr") / "segments.json"\n        if not segments_file.exists():\n            segments_file = self.job_dir / "transcripts" / "segments.json"\n        \n        if not segments_file.exists():\n            self.logger.error(f"Segments not found: {segments_file}")\n            return False\n        \n        # Output to subtitle generation stage directory\n        output_dir = self._stage_path("subtitle_generation")\n        output_dir.mkdir(parents=True, exist_ok=True)\n        \n        # Generate filename\n        title = self.job_config.get("title", "output")\n        output_srt = output_dir / f"{title}.{source_lang}.srt"\n        \n        # Log input/output\n        self.logger.info(f"\U0001f4e5 Input: {segments_file.relative_to(self.job_dir)}")\n        self.logger.info(f"\U0001f4e4 Output: {output_srt.relative_to(self.job_dir)}")\n        self.logger.info("Generating source language subtitles...")\n        \n        # Load segments\n        try:\n            with open(segments_file, \'r\', encoding=\'utf-8\') as f:\n                raw_data = json.load(f)\n                data, segments = normalize_segments_data(raw_data)\n        except Exception as e:\n            self.logger.error(f"Failed to load segments: {e}", exc_info=True)\n            return False\n        \n        # Generate SRT file\n        if generate_srt_from_segments(segments, output_srt):\n            # Copy to subtitles/ for compatibility\n            subtitles_dir = self.job_dir / "subtitles"\n            subtitles_dir.mkdir(parents=True, exist_ok=True)\n            final_output = subtitles_dir / output_srt.name\n            \n            # Only copy if source and destination are different\n            if output_srt != final_output:\n                import shutil\n                shutil.copy2(output_srt, final_output)\n                self.logger.info(f"\u2713 Source subtitles generated: {output_srt.relative_to(self.job_dir)}")\n                self.logger.info(f"\u2713 Copied to: subtitles/{output_srt.name}")\n            else:\n                self.logger.info(f"\u2713 Source subtitles generated: {output_srt.relative_to(self.job_dir)}")\n            \n            return True\n        else:\n            self.logger.error("Source subtitle generation failed")\n            return False\n    \n    def _stage_hybrid_translation_multi(self, target_lang: str) -&gt; bool:\n        """\n        Stage: Hybrid translation for multiple target languages (subtitle workflow)\n        \n        Wrapper around _stage_hybrid_translation for multi-language subtitle workflow.\n        Temporarily updates job_config with current target language.\n        """\n        # Temporarily set target language for this translation\n        original_target = self._get_target_language()\n        # Set both formats for compatibility\n        self.job_config["target_language"] = target_lang\n        if "target_languages" in self.job_config:\n            self.job_config["target_languages"] = [target_lang]\n        \n        try:\n            # Call main hybrid translation stage\n            result = self._stage_hybrid_translation()\n            \n            # If successful, rename output file to include language code\n            if result:\n                generic_output = self.job_dir / "transcripts" / "segments_translated.json"\n                lang_specific_output = self.job_dir / "transcripts" / f"segments_translated_{target_lang}.json"\n                \n                if generic_output.exists():\n                    # Copy to language-specific file\n                    import shutil\n                    shutil.copy2(generic_output, lang_specific_output)\n                    self.logger.info(f"\u2713 Translation saved: {lang_specific_output.name}")\n            \n            return result\n            \n        finally:\n            # Restore original target language\n            if original_target:\n                self.job_config["target_language"] = original_target\n                if "target_languages" in self.job_config:\n                    self.job_config["target_languages"] = [original_target]\n    \n    def _stage_indictrans2_translation_multi(self, target_lang: str) -&gt; bool:\n        """Translate to specific target language (for multi-language support)"""\n        self.logger.info(f"Translating to {target_lang.upper()}...")\n        \n        segments_file = self.job_dir / "transcripts" / "segments.json"\n        output_file = self.job_dir / "transcripts" / f"segments_translated_{target_lang}.json"\n        \n        source_lang = self.job_config["source_language"]\n        \n        # Get configuration from job\'s .env file\n        device = self.env_config.get("INDICTRANS2_DEVICE", self.main_config.indictrans2_device)\n        num_beams = self.env_config.get("INDICTRANS2_NUM_BEAMS", "4")\n        max_tokens = self.env_config.get("INDICTRANS2_MAX_NEW_TOKENS", "128")\n        \n        self.logger.info(f"Using IndicTrans2 device: {device} (from job config)")\n        self.logger.info(f"Translation: {source_lang} \u2192 {target_lang}")\n        \n        # Use existing IndicTrans2 translator in venv/indictrans2\n        # Get Python executable from IndicTrans2 environment\n        python_exe = self.env_manager.get_python_executable("indictrans2")\n        self.logger.info(f"Using IndicTrans2 environment: {python_exe}")\n        \n        cmd = [\n            str(python_exe), "-c",\n            f"""\nimport json\nfrom pathlib import Path\nfrom scripts.indictrans2_translator import translate_whisperx_result\n\n# Load segments\nwith open(\'{segments_file}\') as f:\n    segments = json.load(f)\n\n# Translate with job-configured settings\nfrom shared.logger import PipelineLogger\nlog_file = Path(\'{self.job_dir / "logs"}\') / \'indictrans2_translation_{target_lang}.log\'\nlogger = PipelineLogger(module_name=\'indictrans2_{target_lang}\', log_file=log_file, log_level=\'{"DEBUG" if self.debug else "INFO"}\')\n\n# Set device for IndicTrans2 (from job config)\nimport os\nos.environ[\'INDICTRANS2_DEVICE\'] = \'{device}\'\nos.environ[\'INDICTRANS2_NUM_BEAMS\'] = \'{num_beams}\'\nos.environ[\'INDICTRANS2_MAX_NEW_TOKENS\'] = \'{max_tokens}\'\n\n# translate_whisperx_result will auto-select the right model based on language pair\ntranslated = translate_whisperx_result(segments, \'{source_lang}\', \'{target_lang}\', logger)\n\n# Save\nwith open(\'{output_file}\', \'w\') as f:\n    json.dump(translated, f, indent=2)\n\nlogger.info(f"Translated {{len(translated[\'segments\'])}} segments to {target_lang}")\n"""\n        ]\n        \n        try:\n            stage_name = f"indictrans2_translation_{target_lang}"\n            \n            result = self._run_in_environment(\n                stage_name,\n                cmd,\n                capture_output=True,\n                text=True,\n                check=True\n            )\n            \n            if output_file.exists():\n                self.logger.info(f"Translation to {target_lang.upper()} completed: {output_file}")\n                return True\n            else:\n                self.logger.error(f"Translation to {target_lang} failed")\n                return False\n                \n        except subprocess.CalledProcessError as e:\n            self.logger.error(f"Translation to {target_lang} error: {e.stderr}", exc_info=True)\n            return False\n    \n    def _stage_nllb_translation(self) -&gt; bool:\n        """Stage 2 (translate): Translate using NLLB for non-Indic languages"""\n        self.logger.info("Translating with NLLB...")\n        \n        segments_file = self.job_dir / "transcripts" / "segments.json"\n        output_file = self.job_dir / "transcripts" / "segments_translated.json"\n        \n        source_lang = self.job_config["source_language"]\n        target_lang = self._get_target_language()\n        \n        # Get configuration from job\'s .env file (set by prepare-job)\n        device = self.env_config.get("NLLB_DEVICE", "mps")\n        model_size = self.env_config.get("NLLB_MODEL_SIZE", "600M")\n        \n        # Model name based on size\n        model_map = {\n            "600M": "facebook/nllb-200-distilled-600M",\n            "1.3B": "facebook/nllb-200-1.3B",\n            "3.3B": "facebook/nllb-200-3.3B"\n        }\n        model_name = model_map.get(model_size, "facebook/nllb-200-distilled-600M")\n        \n        self.logger.info(f"Using NLLB model: {model_name}")\n        self.logger.info(f"Device: {device} (from job config)")\n        self.logger.info(f"Translation: {source_lang} \u2192 {target_lang}")\n        \n        # Get Python executable from NLLB environment\n        python_exe = self.env_manager.get_python_executable("nllb")\n        self.logger.info(f"Using NLLB environment: {python_exe}")\n        \n        cmd = [\n            str(python_exe), "-c",\n            f"""\nimport json\nfrom pathlib import Path\nfrom scripts.nllb_translator import translate_whisperx_result, NLLBConfig\n\n# Load segments\nwith open(\'{segments_file}\') as f:\n    segments = json.load(f)\n\n# Setup logging\nfrom shared.logger import PipelineLogger\nlog_file = Path(\'{self.job_dir / "logs"}\') / \'nllb_translation.log\'\nlogger = PipelineLogger(module_name=\'nllb\', log_file=log_file, log_level=\'{"DEBUG" if self.debug else "INFO"}\')\n\n# Configure NLLB\nconfig = NLLBConfig(\n    model_name=\'{model_name}\',\n    device=\'{device}\'\n)\n\n# Translate\ntranslated = translate_whisperx_result(segments, \'{source_lang}\', \'{target_lang}\', logger: logging.Logger, config)\n\n# Save\nwith open(\'{output_file}\', \'w\') as f:\n    json.dump(translated, f, indent=2, ensure_ascii=False)\n\nlogger.info(f"Translated {{len(translated[\'segments\'])}} segments")\n"""\n        ]\n        \n        try:\n            # Set up environment with debug flag\n            env = os.environ.copy()\n            env[\'DEBUG_MODE\'] = \'true\' if self.debug else \'false\'\n            env[\'LOG_LEVEL\'] = \'DEBUG\' if self.debug else \'INFO\'\n            \n            result = subprocess.run(\n                cmd,\n                capture_output=True,\n                text=True,\n                check=True,\n                cwd=str(PROJECT_ROOT),\n                env=env\n            )\n            \n            if output_file.exists():\n                self.logger.info(f"Translation completed: {output_file}")\n                return True\n            else:\n                self.logger.error("Translation failed")\n                return False\n                \n        except subprocess.CalledProcessError as e:\n            self.logger.error(f"Translation error: {e.stderr}", exc_info=True)\n            return False\n    \n    def _stage_nllb_translation_multi(self, target_lang: str) -&gt; bool:\n        """Translate to specific target language using NLLB (for multi-language support)"""\n        self.logger.info(f"Translating to {target_lang.upper()} with NLLB...")\n        \n        segments_file = self.job_dir / "transcripts" / "segments.json"\n        output_file = self.job_dir / "transcripts" / f"segments_translated_{target_lang}.json"\n        \n        source_lang = self.job_config["source_language"]\n        \n        # Get configuration from job\'s .env file\n        device = self.env_config.get("NLLB_DEVICE", "mps")\n        model_size = self.env_config.get("NLLB_MODEL_SIZE", "600M")\n        \n        # Model name based on size\n        model_map = {\n            "600M": "facebook/nllb-200-distilled-600M",\n            "1.3B": "facebook/nllb-200-1.3B",\n            "3.3B": "facebook/nllb-200-3.3B"\n        }\n        model_name = model_map.get(model_size, "facebook/nllb-200-distilled-600M")\n        \n        self.logger.info(f"Using NLLB model: {model_name}")\n        self.logger.info(f"Device: {device}")\n        self.logger.info(f"Translation: {source_lang} \u2192 {target_lang}")\n        \n        # Get Python executable from NLLB environment\n        python_exe = self.env_manager.get_python_executable("nllb")\n        self.logger.info(f"Using NLLB environment: {python_exe}")\n        \n        cmd = [\n            str(python_exe), "-c",\n            f"""\nimport json\nfrom pathlib import Path\nfrom scripts.nllb_translator import translate_whisperx_result, NLLBConfig\n\n# Load segments\nwith open(\'{segments_file}\') as f:\n    segments = json.load(f)\n\n# Setup logging\nfrom shared.logger import PipelineLogger\nlog_file = Path(\'{self.job_dir / "logs"}\') / \'nllb_{target_lang}_translation.log\'\nlogger = PipelineLogger(module_name=\'nllb_{target_lang}\', log_file=log_file, log_level=\'{"DEBUG" if self.debug else "INFO"}\')\n\n# Configure NLLB\nconfig = NLLBConfig(\n    model_name=\'{model_name}\',\n    device=\'{device}\'\n)\n\n# Translate\ntranslated = translate_whisperx_result(segments, \'{source_lang}\', \'{target_lang}\', logger: logging.Logger, config)\n\n# Save\nwith open(\'{output_file}\', \'w\') as f:\n    json.dump(translated, f, indent=2, ensure_ascii=False)\n\nlogger.info(f"Translated {{len(translated[\'segments\'])}} segments to {target_lang}")\n"""\n        ]\n        \n        try:\n            # Set up environment with debug flag\n            env = os.environ.copy()\n            env[\'DEBUG_MODE\'] = \'true\' if self.debug else \'false\'\n            env[\'LOG_LEVEL\'] = \'DEBUG\' if self.debug else \'INFO\'\n            \n            result = subprocess.run(\n                cmd,\n                capture_output=True,\n                text=True,\n                check=True,\n                cwd=str(PROJECT_ROOT),\n                env=env\n            )\n            \n            if output_file.exists():\n                self.logger.info(f"Translation to {target_lang.upper()} completed: {output_file}")\n                return True\n            else:\n                self.logger.error(f"Translation to {target_lang} failed")\n                return False\n                \n        except subprocess.CalledProcessError as e:\n            self.logger.error(f"Translation to {target_lang} error: {e.stderr}", exc_info=True)\n            return False\n    \n    def _stage_subtitle_generation_target_multi(self, target_lang: str) -&gt; bool:\n        """Generate subtitle file for specific target language"""\n        self.logger.info(f"Generating {target_lang.upper()} subtitles...")\n        \n        segments_file = self.job_dir / "transcripts" / f"segments_translated_{target_lang}.json"\n        \n        # Generate filename\n        title = self.job_config.get("title", "output")\n        output_srt = self.job_dir / "subtitles" / f"{title}.{target_lang}.srt"\n        \n        # Load translated segments\n        try:\n            with open(segments_file, \'r\', encoding=\'utf-8\') as f:\n                raw_data = json.load(f)\n                data, segments = normalize_segments_data(raw_data)\n        except Exception as e:\n            self.logger.error(f"Failed to load {target_lang} segments: {e}", exc_info=True)\n            return False\n        \n        # Generate SRT file\n        if generate_srt_from_segments(segments, output_srt):\n            self.logger.info(f"{target_lang.upper()} subtitles generated: {output_srt}")\n            return True\n        else:\n            self.logger.error(f"{target_lang} subtitle generation failed", exc_info=True)\n            return False\n    \n    def _stage_subtitle_generation_target(self) -&gt; bool:\n        """Stage 3b (subtitle workflow): Generate SRT subtitle file in target language"""\n        self.logger.info("Generating target language subtitles...")\n        \n        segments_file = self.job_dir / "transcripts" / "segments_translated.json"\n        target_lang = self._get_target_language()\n        \n        # Generate filename\n        title = self.job_config.get("title", "output")\n        output_srt = self.job_dir / "subtitles" / f"{title}.{target_lang}.srt"\n        \n        # Load translated segments\n        try:\n            with open(segments_file, \'r\', encoding=\'utf-8\') as f:\n                raw_data = json.load(f)\n                data, segments = normalize_segments_data(raw_data)\n        except Exception as e:\n            self.logger.error(f"Failed to load segments: {e}", exc_info=True)\n            return False\n        \n        # Generate SRT file\n        if generate_srt_from_segments(segments, output_srt):\n            self.logger.info(f"Target subtitles generated: {output_srt}")\n            return True\n        else:\n            self.logger.error("Target subtitle generation failed", exc_info=True)\n            return False\n    \n    def _stage_hinglish_detection(self) -&gt; bool:\n        """Stage 3b (subtitle workflow): Detect and tag word-level languages in Hinglish subtitles"""\n        self.logger.info("Running Hinglish word-level language detection...")\n        \n        source_lang = self.job_config["source_language"]\n        title = self.job_config.get("title", "output")\n        \n        # Source subtitle file\n        source_srt = self.job_dir / "subtitles" / f"{title}.{source_lang}.srt"\n        \n        if not source_srt.exists():\n            self.logger.warning(f"Source subtitle not found: {source_srt}")\n            self.logger.warning("Skipping Hinglish detection")\n            return True  # Not a failure, just skip\n        \n        # Output files\n        tagged_srt = self.job_dir / "subtitles" / f"{title}.{source_lang}.tagged.srt"\n        analysis_json = self.job_dir / "subtitles" / f"{title}.{source_lang}.analysis.json"\n        \n        self.logger.info(f"Analyzing: {source_srt}")\n        \n        # Get Python executable from common environment\n        python_exe = self.env_manager.get_python_executable("common")\n        \n        # Run hinglish detector\n        cmd = [\n            str(python_exe),\n            str(self.scripts_dir / "hinglish_word_detector.py"),\n            str(source_srt),\n            "-o", str(tagged_srt),\n            "-j", str(analysis_json)\n        ]\n        \n        if self.debug:\n            cmd.append("-v")\n        \n        try:\n            result = subprocess.run(\n                cmd,\n                capture_output=True,\n                text=True,\n                check=True\n            )\n            \n            self.logger.info(f"\u2713 Tagged SRT created: {tagged_srt}")\n            self.logger.info(f"\u2713 Analysis JSON created: {analysis_json}")\n            \n            # Parse and log statistics from output\n            if "Hindi words:" in result.stdout:\n                for line in result.stdout.split(\'\\n\'):\n                    if \'words:\' in line.lower() or \'Total\' in line:\n                        self.logger.info(f"  {line.strip()}")\n            \n            return True\n            \n        except subprocess.CalledProcessError as e:\n            self.logger.error(f"Hinglish detection failed: {e.stderr}", exc_info=True)\n            self.logger.warning("Continuing without Hinglish detection...")\n            return True  # Don\'t fail the pipeline, just warn\n    \n    def _stage_mux(self) -&gt; bool:\n        """Stage 9: Mux video with multiple subtitle tracks (up to 5)"""\n        \n        # Get configuration\n        input_media = Path(self.job_config["input_media"])\n        title = self.job_config.get("title", "output")\n        source_lang = self.job_config["source_language"]\n        target_languages = self.job_config.get("target_languages", [])\n        \n        # Get media processing configuration for clipping\n        media_config = self.job_config.get("media_processing", {})\n        processing_mode = media_config.get("mode", "full")\n        start_time = media_config.get("start_time", "")\n        end_time = media_config.get("end_time", "")\n        \n        # Collect all subtitle files (target languages + source)\n        # Try from 08_subtitle_generation first, fallback to subtitles/\n        subtitle_files = []\n        subtitle_langs = []\n        \n        subtitle_dir = self._stage_path("subtitle_generation")\n        fallback_dir = self.job_dir / "subtitles"\n        \n        # Add target language subtitles\n        for target_lang in target_languages:\n            target_srt = subtitle_dir / f"{title}.{target_lang}.srt"\n            if not target_srt.exists():\n                target_srt = fallback_dir / f"{title}.{target_lang}.srt"\n            \n            if not target_srt.exists():\n                self.logger.error(f"Target subtitle not found: {target_lang}")\n                return False\n            subtitle_files.append(target_srt)\n            subtitle_langs.append(target_lang)\n        \n        # Add source language subtitle\n        source_srt = subtitle_dir / f"{title}.{source_lang}.srt"\n        if not source_srt.exists():\n            source_srt = fallback_dir / f"{title}.{source_lang}.srt"\n        \n        if not source_srt.exists():\n            self.logger.error(f"Source subtitle not found: {source_lang}")\n            return False\n        subtitle_files.append(source_srt)\n        subtitle_langs.append(source_lang)\n        \n        # Output directory\n        output_dir = self.job_dir / "10_mux"\n        output_dir.mkdir(parents=True, exist_ok=True)\n        \n        # Log input/output\n        self.logger.info(f"\U0001f4e5 Input video: {input_media.name}")\n        for i, (sub_file, lang) in enumerate(zip(subtitle_files, subtitle_langs), 1):\n            self.logger.info(f"\U0001f4e5 Input subtitle {i}: {sub_file.relative_to(self.job_dir)} ({lang})")\n        \n        self.logger.info(f"Muxing video with {len(subtitle_files)} subtitle tracks: {\', \'.join(subtitle_langs)}")\n        \n        # Detect source file extension and use same format for output\n        source_ext = input_media.suffix.lower()  # e.g., \'.mp4\', \'.mkv\', \'.avi\'\n        \n        # Determine output format\n        if source_ext in [\'.mp4\', \'.m4v\']:\n            output_ext = \'.mp4\'\n            subtitle_codec = \'mov_text\'  # MP4 subtitle format\n        elif source_ext in [\'.mkv\', \'.webm\']:\n            output_ext = \'.mkv\'\n            subtitle_codec = \'srt\'  # MKV subtitle format\n        elif source_ext in [\'.avi\']:\n            output_ext = \'.mkv\'  # AVI doesn\'t support subtitle tracks well, use MKV\n            subtitle_codec = \'srt\'\n            self.logger.info(f"Source is AVI, using MKV for subtitle support")\n        else:\n            # Default to MKV for unknown formats (best subtitle support)\n            output_ext = \'.mkv\'\n            subtitle_codec = \'srt\'\n            self.logger.info(f"Unknown format {source_ext}, using MKV for subtitle support")\n        \n        # Output video file in 09_mux directory\n        output_video = output_dir / f"{title}_subtitled{output_ext}"\n        \n        # Also create copy in media subdirectory for user convenience\n        media_name = input_media.stem\n        media_output_subdir = self.job_dir / "media" / media_name\n        media_output_subdir.mkdir(parents=True, exist_ok=True)\n        media_output_video = media_output_subdir / f"{title}_subtitled{output_ext}"\n        \n        self.logger.info(f"\U0001f4e4 Output: {output_video.relative_to(self.job_dir)}")\n        self.logger.info(f"Output format: {output_ext} (source: {source_ext})")\n        \n        # Build ffmpeg command\n        cmd = ["ffmpeg", "-y"]\n        \n        # Add log level based on debug mode\n        if not self.debug:\n            cmd.extend(["-loglevel", "error"])\n        \n        # Add clipping if configured\n        if processing_mode == "clip" and start_time:\n            cmd.extend(["-ss", start_time])\n        \n        # Add input files - video first, then all subtitles\n        cmd.extend(["-i", str(input_media)])  # Video input (index 0)\n        for sub_file in subtitle_files:\n            cmd.extend(["-i", str(sub_file)])  # Subtitle inputs (indices 1, 2, 3...)\n        \n        # Add end time if clipping\n        if processing_mode == "clip" and end_time:\n            cmd.extend(["-to", end_time])\n        \n        # Map streams: video, audio, all subtitles\n        cmd.extend([\n            "-map", "0:v",  # Video from input 0\n            "-map", "0:a",  # Audio from input 0\n        ])\n        \n        # Map all subtitle files\n        for i in range(len(subtitle_files)):\n            cmd.extend(["-map", str(i + 1)])  # Subtitle from input 1, 2, 3...\n        \n        # Copy codecs (no re-encoding)\n        cmd.extend(["-c", "copy"])\n        \n        # Set subtitle codec based on output format\n        cmd.extend(["-c:s", subtitle_codec])\n        \n        # Add metadata for each subtitle track\n        # Map 2-letter codes to ISO 639-2 (3-letter) for better player compatibility\n        lang_map_iso639_2 = {\n            "hi": "hin",  # Hindi\n            "en": "eng",  # English\n            "gu": "guj",  # Gujarati\n            "ta": "tam",  # Tamil\n            "te": "tel",  # Telugu\n            "bn": "ben",  # Bengali\n            "mr": "mar",  # Marathi\n            "kn": "kan",  # Kannada\n            "ml": "mal",  # Malayalam\n            "pa": "pan",  # Punjabi\n            "ur": "urd",  # Urdu\n            "as": "asm",  # Assamese\n            "or": "ori",  # Odia\n            "ne": "nep",  # Nepali\n            "sd": "snd",  # Sindhi\n            "si": "sin",  # Sinhala\n            "sa": "san",  # Sanskrit\n        }\n        \n        # Map to full language names for display\n        lang_names = {\n            "hin": "Hindi", "hi": "Hindi",\n            "eng": "English", "en": "English",\n            "guj": "Gujarati", "gu": "Gujarati",\n            "tam": "Tamil", "ta": "Tamil",\n            "tel": "Telugu", "te": "Telugu",\n            "ben": "Bengali", "bn": "Bengali",\n            "mar": "Marathi", "mr": "Marathi",\n            "kan": "Kannada", "kn": "Kannada",\n            "mal": "Malayalam", "ml": "Malayalam",\n            "pan": "Punjabi", "pa": "Punjabi",\n            "urd": "Urdu", "ur": "Urdu",\n            "asm": "Assamese", "as": "Assamese",\n            "ori": "Odia", "or": "Odia",\n            "nep": "Nepali", "ne": "Nepali",\n            "snd": "Sindhi", "sd": "Sindhi",\n            "sin": "Sinhala", "si": "Sinhala",\n            "san": "Sanskrit", "sa": "Sanskrit",\n        }\n        \n        for i, lang in enumerate(subtitle_langs):\n            # Convert to ISO 639-2 (3-letter code)\n            lang_iso = lang_map_iso639_2.get(lang, lang)\n            # Get full language name\n            lang_title = lang_names.get(lang, lang.upper())\n            \n            cmd.extend([\n                "-metadata:s:s:" + str(i), f"language={lang_iso}",\n                "-metadata:s:s:" + str(i), f"title={lang_title}",\n            ])\n        \n        # Output file\n        cmd.append(str(output_video))\n        \n        mode_str = f"clipped ({start_time} to {end_time})" if processing_mode == "clip" else "full"\n        self.logger.info(f"Creating {mode_str} video with {len(subtitle_files)} subtitle tracks...")\n        for i, lang in enumerate(subtitle_langs):\n            lang_iso = lang_map_iso639_2.get(lang, lang)\n            lang_title = lang_names.get(lang, lang.upper())\n            self.logger.info(f"  \u2022 Track {i}: {lang_title} ({lang_iso})")\n        \n        try:\n            # Set up environment with debug flag\n            env = os.environ.copy()\n            env[\'DEBUG_MODE\'] = \'true\' if self.debug else \'false\'\n            env[\'LOG_LEVEL\'] = \'DEBUG\' if self.debug else \'INFO\'\n            \n            result = subprocess.run(\n                cmd,\n                capture_output=True,\n                text=True,\n                check=True,\n                env=env\n            )\n            \n            if self.debug and result.stderr:\n                self.logger.debug(f"FFmpeg output: {result.stderr}")\n            \n            if output_video.exists():\n                size_mb = output_video.stat().st_size / (1024 * 1024)\n                self.logger.info(f"\u2713 Video created: {output_video.relative_to(self.job_dir)} ({size_mb:.1f} MB)")\n                self.logger.info(f"\u2713 Video contains {len(subtitle_files)} subtitle tracks: {\', \'.join([l.upper() for l in subtitle_langs])}")\n                \n                # Also copy to media subdirectory for user convenience\n                import shutil\n                shutil.copy2(output_video, media_output_video)\n                self.logger.info(f"\u2713 Copy saved to: {media_output_video.relative_to(self.job_dir)}")\n                \n                return True\n            else:\n                self.logger.error("Video muxing failed - no output file")\n                return False\n                \n        except subprocess.CalledProcessError as e:\n            self.logger.error(f"FFmpeg muxing error: {e.stderr}", exc_info=True)\n            return False\n    \n    # ========================================================================\n    # Main Execution\n    # ========================================================================\n    \n    def run(self) -&gt; bool:\n        """Execute pipeline based on workflow"""\n        self.logger.info(f"Starting pipeline: {self.workflow}")\n        self.logger.info(f"Job ID: {self.job_config[\'job_id\']}")\n        self.logger.info(f"Job directory: {self.job_dir}")\n        \n        self.manifest["status"] = "running"\n        self._save_manifest()\n        \n        if self.workflow == "transcribe":\n            success = self.run_transcribe_workflow()\n        elif self.workflow == "translate":\n            success = self.run_translate_workflow()\n        elif self.workflow == "subtitle":\n            success = self.run_subtitle_workflow()\n        else:\n            self.logger.error(f"Unknown workflow: {self.workflow}")\n            success = False\n        \n        if success:\n            self.manifest["status"] = "completed"\n            self.logger.info("=" * 80)\n            self.logger.info("PIPELINE COMPLETED SUCCESSFULLY")\n            self.logger.info("=" * 80)\n        else:\n            self.manifest["status"] = "failed"\n            self.logger.error("=" * 80)\n            self.logger.error("PIPELINE FAILED")\n            self.logger.error("=" * 80)\n        \n        self._save_manifest()\n        return success\n    \n    def _stage_hallucination_removal(self) -&gt; bool:\n        """\n        Stage: Remove hallucinations from ASR transcript\n        \n        Detects and removes looping/repetition hallucinations from WhisperX output.\n        Runs after ASR, before alignment.\n        \n        Follows developer standards:\n        - Uses Config class for configuration\n        - Proper error handling with graceful degradation\n        - Logging with PipelineLogger\n        - Respects opt-out (HALLUCINATION_REMOVAL_ENABLED=false)\n        \n        Returns:\n            bool: True if successful or disabled, False on error\n        """\n        self.logger.info("Running hallucination removal...")\n        \n        # Check if enabled (default: true, opt-out)\n        enabled = self.env_config.get(\'HALLUCINATION_REMOVAL_ENABLED\', \'true\').lower() == \'true\'\n        if not enabled:\n            self.logger.info("Hallucination removal is disabled (HALLUCINATION_REMOVAL_ENABLED=false)")\n            self.logger.info("Skipping stage - segments will be used as-is")\n            return True\n        \n        # Configuration with defaults (following developer standards)\n        loop_threshold = int(self.env_config.get(\'HALLUCINATION_LOOP_THRESHOLD\', \'3\'))\n        max_repeats = int(self.env_config.get(\'HALLUCINATION_MAX_REPEATS\', \'2\'))\n        \n        self.logger.info(f"Configuration:")\n        self.logger.info(f"  Loop threshold: {loop_threshold} (min consecutive repeats to consider hallucination)")\n        self.logger.info(f"  Max repeats: {max_repeats} (max occurrences to keep)")\n        \n        # Input/output paths\n        segments_file = self.job_dir / "transcripts" / "segments.json"\n        if not segments_file.exists():\n            self.logger.error(f"Segments file not found: {segments_file}")\n            self.logger.error("Run ASR stage first!")\n            return False\n        \n        try:\n            # Load segments\n            with open(segments_file, \'r\', encoding=\'utf-8\') as f:\n                data = json.load(f)\n            \n            # Extract segments and metadata\n            if isinstance(data, dict):\n                segments = data.get(\'segments\', [])\n                metadata = {k: v for k, v in data.items() if k != \'segments\'}\n            else:\n                segments = data\n                metadata = {}\n            \n            if not segments:\n                self.logger.warning("No segments found - nothing to clean")\n                return True\n            \n            original_count = len(segments)\n            self.logger.info(f"Processing {original_count} segments...")\n            \n            # Import hallucination remover (late import to avoid issues)\n            sys.path.insert(0, str(SCRIPT_DIR))\n            from hallucination_removal import HallucinationRemover\n            \n            # Create remover instance\n            remover = HallucinationRemover(\n                loop_threshold=loop_threshold,\n                max_repeats=max_repeats,\n                logger=None  # Use pipeline logger instead\n            )\n            \n            # Detect loops manually (for logging)\n            loops = remover.detect_looping_hallucinations(segments)\n            \n            if loops:\n                self.logger.warning(f"Detected {len(loops)} hallucination loop(s):")\n                for start_idx, end_idx, text in loops:\n                    count = end_idx - start_idx + 1\n                    self.logger.warning(f"  \u2022 \'{text}\' repeated {count} times (segments {start_idx}-{end_idx})")\n            else:\n                self.logger.info("No hallucination loops detected - segments are clean")\n            \n            # Remove loops\n            cleaned_segments = remover.remove_looping_hallucinations(segments, loops)\n            cleaned_count = len(cleaned_segments)\n            removed_count = original_count - cleaned_count\n            \n            # Log results\n            if removed_count &gt; 0:\n                self.logger.info(f"Removed {removed_count} hallucinated segments")\n                self.logger.info(f"Kept {cleaned_count}/{original_count} segments ({cleaned_count/original_count*100:.1f}%)")\n                \n                # Calculate repetition improvement\n                before_texts = [seg.get(\'text\', \'\').strip() for seg in segments if seg.get(\'text\', \'\').strip()]\n                after_texts = [seg.get(\'text\', \'\').strip() for seg in cleaned_segments if seg.get(\'text\', \'\').strip()]\n                \n                before_unique = len(set(before_texts))\n                after_unique = len(set(after_texts))\n                \n                before_rep_rate = 1.0 - (before_unique / len(before_texts)) if before_texts else 0.0\n                after_rep_rate = 1.0 - (after_unique / len(after_texts)) if after_texts else 0.0\n                \n                if before_rep_rate &gt; 0:\n                    improvement = ((before_rep_rate - after_rep_rate) / before_rep_rate) * 100\n                    self.logger.info(f"Repetition rate improved: {before_rep_rate:.1%} \u2192 {after_rep_rate:.1%} ({improvement:.0f}% better)")\n            else:\n                self.logger.info("No hallucinations found - transcript is clean")\n            \n            # Save cleaned segments (backup original first)\n            backup_file = segments_file.with_suffix(\'.json.pre-hallucination-removal\')\n            if not backup_file.exists():\n                import shutil\n                shutil.copy2(segments_file, backup_file)\n                self.logger.info(f"Backed up original segments: {backup_file.name}")\n            \n            # Write cleaned segments\n            output_data = metadata.copy()\n            output_data[\'segments\'] = cleaned_segments\n            output_data[\'hallucination_removal\'] = {\n                \'enabled\': True,\n                \'original_count\': original_count,\n                \'cleaned_count\': cleaned_count,\n                \'removed_count\': removed_count,\n                \'loops_detected\': len(loops),\n                \'loop_threshold\': loop_threshold,\n                \'max_repeats\': max_repeats\n            }\n            \n            with open(segments_file, \'w\', encoding=\'utf-8\') as f:\n                json.dump(output_data, f, indent=2, ensure_ascii=False)\n            \n            self.logger.info(f"Cleaned segments saved: {segments_file}")\n            self.logger.info("\u2705 Hallucination removal completed successfully")\n            return True\n            \n        except Exception as e:\n            self.logger.error(f"Error in hallucination removal: {e}", exc_info=True)\n            if self.main_config.debug_mode:\n                import traceback\n                self.logger.error(f"Traceback: {traceback.format_exc()}", exc_info=True)\n            \n            # Graceful degradation - continue with original segments\n            self.logger.warning("Continuing with original segments (graceful degradation)")\n            return True  # Don\'t fail pipeline, just skip cleaning\n\n\ndef main() -&gt; Any:\n    """Main."""\n    parser = argparse.ArgumentParser(\n        description="IndicTrans2 Pipeline Orchestrator"\n    )\n    \n    parser.add_argument(\n        "--job-dir",\n        type=Path,\n        required=True,\n        help="Job directory"\n    )\n    \n    parser.add_argument(\n        "--resume",\n        action="store_true",\n        help="Resume from last completed stage"\n    )\n    \n    args = parser.parse_args()\n    \n    if not args.job_dir.exists():\n        logger.info(f"\u274c Error: Job directory not found: {args.job_dir}")\n        return 1\n    \n    # Create and run pipeline\n    pipeline = IndicTrans2Pipeline(args.job_dir, args.resume)\n    success = pipeline.run()\n    \n    return 0 if success else 1\n\n\nif __name__ == "__main__":\n    sys.exit(main())\n'</skipped></testcase><testcase classname="tests.integration.test_workflow_integration.TestTranscribeWorkflowE2E" name="test_transcribe_sample1_english_technical" time="0.000"><skipped type="pytest.skip" message="Phase 3 - Requires full pipeline with models">/Users/rpatel/Projects/Active/cp-whisperx-app/tests/integration/test_workflow_integration.py:359: Phase 3 - Requires full pipeline with models</skipped></testcase><testcase classname="tests.integration.test_workflow_integration.TestTranscribeWorkflowE2E" name="test_transcribe_sample2_hinglish" time="0.000"><skipped type="pytest.skip" message="Phase 3 - Requires full pipeline with models">/Users/rpatel/Projects/Active/cp-whisperx-app/tests/integration/test_workflow_integration.py:375: Phase 3 - Requires full pipeline with models</skipped></testcase><testcase classname="tests.integration.test_workflow_integration.TestTranslateWorkflowE2E" name="test_translate_english_to_hindi" time="0.000"><skipped type="pytest.skip" message="Phase 3 - Requires full pipeline with models">/Users/rpatel/Projects/Active/cp-whisperx-app/tests/integration/test_workflow_integration.py:399: Phase 3 - Requires full pipeline with models</skipped></testcase><testcase classname="tests.integration.test_workflow_integration.TestTranslateWorkflowE2E" name="test_translate_hindi_to_english" time="0.000"><skipped type="pytest.skip" message="Phase 3 - Requires full pipeline with models">/Users/rpatel/Projects/Active/cp-whisperx-app/tests/integration/test_workflow_integration.py:415: Phase 3 - Requires full pipeline with models</skipped></testcase><testcase classname="tests.integration.test_workflow_integration.TestSubtitleWorkflowE2E" name="test_subtitle_full_pipeline_sample2" time="0.000"><skipped type="pytest.skip" message="Phase 3 - Requires full pipeline with models">/Users/rpatel/Projects/Active/cp-whisperx-app/tests/integration/test_workflow_integration.py:439: Phase 3 - Requires full pipeline with models</skipped></testcase><testcase classname="tests.integration.test_quality_baselines.TestSample1QualityBaselines" name="test_sample1_asr_wer_meets_target" time="0.000"><skipped type="pytest.skip" message="Phase 3 - Requires actual pipeline execution and models">/Users/rpatel/Projects/Active/cp-whisperx-app/tests/integration/test_quality_baselines.py:88: Phase 3 - Requires actual pipeline execution and models</skipped></testcase><testcase classname="tests.integration.test_quality_baselines.TestSample1QualityBaselines" name="test_sample1_translation_bleu_meets_target" time="0.000"><skipped type="pytest.skip" message="Phase 3 - Requires actual pipeline execution and models">/Users/rpatel/Projects/Active/cp-whisperx-app/tests/integration/test_quality_baselines.py:114: Phase 3 - Requires actual pipeline execution and models</skipped></testcase><testcase classname="tests.integration.test_quality_baselines.TestSample1QualityBaselines" name="test_sample1_technical_term_preservation" time="0.000"><skipped type="pytest.skip" message="Phase 3 - Requires actual pipeline execution and models">/Users/rpatel/Projects/Active/cp-whisperx-app/tests/integration/test_quality_baselines.py:140: Phase 3 - Requires actual pipeline execution and models</skipped></testcase><testcase classname="tests.integration.test_quality_baselines.TestSample2QualityBaselines" name="test_sample2_asr_wer_meets_target" time="0.000"><skipped type="pytest.skip" message="Phase 3 - Requires actual pipeline execution and models">/Users/rpatel/Projects/Active/cp-whisperx-app/tests/integration/test_quality_baselines.py:176: Phase 3 - Requires actual pipeline execution and models</skipped></testcase><testcase classname="tests.integration.test_quality_baselines.TestSample2QualityBaselines" name="test_sample2_subtitle_quality_meets_target" time="0.000"><skipped type="pytest.skip" message="Phase 3 - Requires actual pipeline execution and models">/Users/rpatel/Projects/Active/cp-whisperx-app/tests/integration/test_quality_baselines.py:203: Phase 3 - Requires actual pipeline execution and models</skipped></testcase><testcase classname="tests.integration.test_quality_baselines.TestSample2QualityBaselines" name="test_sample2_timing_accuracy_meets_target" time="0.000"><skipped type="pytest.skip" message="Phase 3 - Requires actual pipeline execution and models">/Users/rpatel/Projects/Active/cp-whisperx-app/tests/integration/test_quality_baselines.py:232: Phase 3 - Requires actual pipeline execution and models</skipped></testcase><testcase classname="tests.integration.test_quality_baselines.TestSample2QualityBaselines" name="test_sample2_glossary_application_meets_target" time="0.000"><skipped type="pytest.skip" message="Phase 3 - Requires actual pipeline execution and models">/Users/rpatel/Projects/Active/cp-whisperx-app/tests/integration/test_quality_baselines.py:258: Phase 3 - Requires actual pipeline execution and models</skipped></testcase><testcase classname="tests.integration.test_quality_baselines.TestSample2QualityBaselines" name="test_sample2_context_awareness_meets_target" time="0.000"><skipped type="pytest.skip" message="Phase 3 - Requires actual pipeline execution and models">/Users/rpatel/Projects/Active/cp-whisperx-app/tests/integration/test_quality_baselines.py:286: Phase 3 - Requires actual pipeline execution and models</skipped></testcase><testcase classname="tests.integration.test_quality_baselines.TestBaselineInfrastructure" name="test_quality_baselines_document_exists" time="0.000" /><testcase classname="tests.integration.test_quality_baselines.TestBaselineInfrastructure" name="test_quality_baselines_fixture_loads" time="0.000" /><testcase classname="tests.integration.test_quality_baselines.TestBaselineInfrastructure" name="test_sample1_baselines_complete" time="0.000" /><testcase classname="tests.integration.test_quality_baselines.TestBaselineInfrastructure" name="test_sample2_baselines_complete" time="0.000" /><testcase classname="tests.integration.test_quality_baselines.TestBaselineInfrastructure" name="test_baseline_values_reasonable" time="0.000" /></testsuite></testsuites>