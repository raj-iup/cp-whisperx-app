#!/usr/bin/env python3
"""cp-whisperx-app full pipeline orchestration.

Orchestrates the complete workflow:
1. Filename parsing & era detection
2. TMDB enrichment
3. Prompt assembly
4. Rolling windowed bias injection
5. ASR+Translation (WhisperX)
6. Diarization (pyannote)
7. Two-pass merge
8. NER extraction & canonicalization
9. Mux QC (MP4/MKV)
10. Packaging & manifest
"""
from __future__ import annotations

import argparse
import json
import sys
import subprocess
from datetime import datetime
from pathlib import Path

ROOT = Path(__file__).resolve().parent
sys.path.insert(0, str(ROOT))

from scripts.config_loader import Config
from scripts.logger import create_logger, create_timestamped_log_dir
from scripts.filename_parser import parse_filename
from scripts.tmdb_enrichment import enrich_from_tmdb
from scripts.prompt_assembly import assemble_prompt
from scripts.manifest import ManifestBuilder


def main():
    parser = argparse.ArgumentParser(description="cp-whisperx-app pipeline")
    parser.add_argument("-i", "--input", help="Input file path")
    parser.add_argument("--infer-tmdb-from-filename", action="store_true", help="Infer TMDB from filename")
    parser.add_argument("--two-pass-merge", action="store_true", help="Enable two-pass merge")
    parser.add_argument("--prep-prompt", action="store_true", help="Prepare prompt only")
    args = parser.parse_args()

    # Load configuration
    config = Config(project_root=ROOT)
    
    # Setup logging
    log_root = ROOT / config.get("LOG_ROOT", "logs")
    log_dir = create_timestamped_log_dir(log_root)
    logger = create_logger("pipeline", log_dir)
    
    logger.info("Pipeline started")
    
    # Get input file
    input_file = args.input if args.input else config.input_file
    if not input_file:
        logger.error("No input file specified")
        print("Error: No input file specified. Use -i or set INPUT_FILE in config/.env")
        sys.exit(1)
    
    input_path = Path(input_file)
    if not input_path.is_absolute():
        input_path = ROOT / input_path
        
    if not input_path.exists():
        logger.error(f"Input file not found: {input_path}")
        print(f"Error: Input file not found: {input_path}")
        sys.exit(1)
    
    logger.info(f"Processing: {input_path}")
    
    # 1. Parse filename
    parsed = parse_filename(str(input_path))
    logger.info(f"Filename parsed: {parsed.title} ({parsed.year})")
    
    # 2. Era detection and lexicon
    era_lexicon = None
    if parsed.year:
        from scripts.era_lexicon import ERA_LEXICONS
        for era_name, lexicon in ERA_LEXICONS.items():
            if lexicon.start_year <= parsed.year <= lexicon.end_year:
                era_lexicon = lexicon
                logger.info(f"Era detected: {era_name}")
                break
    
    # 3. TMDB enrichment (if enabled)
    tmdb_metadata = None
    if config.get("INFER_TMDB", True) or args.infer_tmdb_from_filename:
        try:
            tmdb_api_key = config.get_secret("tmdb_api_key")
            logger.info(f"Enriching from TMDB: {parsed.title} ({parsed.year})")
            tmdb_metadata = enrich_from_tmdb(
                title=parsed.title,
                year=parsed.year,
                api_key=tmdb_api_key
            )
            if tmdb_metadata and tmdb_metadata.found:
                logger.info(f"TMDB enrichment complete: {tmdb_metadata.title} ({tmdb_metadata.year})")
            else:
                logger.warning("TMDB enrichment returned no results")
        except Exception as e:
            logger.error(f"TMDB enrichment failed: {e}")
    
    # 4. Assemble prompt
    output_root = ROOT / config.get("OUTPUT_ROOT", "out")
    # Include year in directory name to match container pipeline behavior
    if parsed.year:
        movie_name = f"{parsed.sanitized_title}_{parsed.year}"
    else:
        movie_name = parsed.sanitized_title
    movie_dir = output_root / movie_name
    movie_dir.mkdir(parents=True, exist_ok=True)
    
    logger.info(f"Output directory: {movie_dir}")
    
    assembled_prompt = assemble_prompt(
        parsed_filename=parsed,
        era_lexicon=era_lexicon,
        tmdb_metadata=tmdb_metadata,
        user_prompt=""
    )
    
    from scripts.prompt_assembly import write_prompt_files
    write_prompt_files(
        output_dir=movie_dir,
        basename=parsed.sanitized_title,
        assembled_prompt=assembled_prompt
    )
    logger.info(f"Prompt files written to {movie_dir}")
    
    # Collect prompt file paths
    prompt_files = [
        movie_dir / f"{parsed.sanitized_title}.initial_prompt.txt",
        movie_dir / f"{parsed.sanitized_title}.combined.initial_prompt.txt",
        movie_dir / f"{parsed.sanitized_title}.combined.initial_prompt.md"
    ]
    
    if args.prep_prompt or config.get("PREP_PROMPT", False):
        logger.info("Prep prompt mode - stopping here")
        manifest = ManifestBuilder()
        manifest.set_input(str(input_path), parsed.title, parsed.year, None)
        manifest.set_output_dir(str(movie_dir))
        manifest.set_era(era_lexicon.era_name if era_lexicon else None)
        if tmdb_metadata and tmdb_metadata.found:
            manifest.set_tmdb_status(
                True,
                tmdb_metadata.tmdb_id,
                len(tmdb_metadata.cast),
                len(tmdb_metadata.crew)
            )
        else:
            manifest.set_tmdb_status(False, None, 0, 0)
        manifest.set_pipeline_step("prompt_assembly", True, completed=True)
        manifest.finalize()
        manifest.save(log_dir / "manifest.json")
        print(f"Prompt prepared in: {movie_dir}")
        print(f"Manifest: {log_dir / 'manifest.json'}")
        return
    
    # 5. Prepare audio/video clipping if enabled
    working_file = input_path
    if config.get("CLIP_VIDEO", False):
        clip_minutes = config.get("CLIP_MINUTES", 5)
        logger.info(f"Clipping video to first {clip_minutes} minutes for testing")
        import subprocess
        clip_path = movie_dir / f"{parsed.sanitized_title}_clip_{clip_minutes}min.mp4"
        clip_seconds = clip_minutes * 60
        try:
            subprocess.run([
                "ffmpeg", "-y", "-i", str(input_path),
                "-t", str(clip_seconds),
                "-c", "copy",
                str(clip_path)
            ], check=True, capture_output=True)
            working_file = clip_path
            logger.info(f"Clipped file created: {clip_path}")
        except Exception as e:
            logger.warning(f"Failed to clip video: {e}. Using original file.")
    
    # Get video duration
    try:
        import subprocess
        result = subprocess.run([
            "ffprobe", "-v", "error",
            "-show_entries", "format=duration",
            "-of", "default=noprint_wrappers=1:nokey=1",
            str(working_file)
        ], capture_output=True, text=True, check=True)
        duration_seconds = float(result.stdout.strip())
        logger.info(f"Video duration: {duration_seconds:.2f} seconds")
    except Exception as e:
        logger.warning(f"Could not determine video duration: {e}")
        duration_seconds = None
    
    # 6. Create bias windows
    from scripts.bias_injection import create_bias_windows
    bias_terms = []
    if era_lexicon:
        bias_terms.extend(era_lexicon.names[:10])
        bias_terms.extend(era_lexicon.places[:5])
    if tmdb_metadata and tmdb_metadata.found:
        bias_terms.extend(tmdb_metadata.cast[:10])
        bias_terms.extend(tmdb_metadata.crew[:5])
    
    bias_windows = None
    if duration_seconds and bias_terms:
        window_seconds = config.get("WINDOW_SECONDS", 45)
        stride_seconds = config.get("STRIDE_SECONDS", 15)
        topk = config.get("BIAS_TOPK", 10)
        bias_windows = create_bias_windows(
            duration_seconds=duration_seconds,
            window_seconds=window_seconds,
            stride_seconds=stride_seconds,
            base_terms=bias_terms,
            topk=topk
        )
        logger.info(f"Created {len(bias_windows)} bias windows")
        
        # Save bias windows
        bias_dir = movie_dir / "bias"
        bias_dir.mkdir(exist_ok=True)
        for window in bias_windows:
            window_file = bias_dir / f"bias.window.{window.window_id:04d}.json"
            with open(window_file, 'w') as f:
                json.dump({
                    "window_id": window.window_id,
                    "start_time": window.start_time,
                    "end_time": window.end_time,
                    "bias_terms": window.bias_terms,
                    "bias_prompt": window.bias_prompt
                }, f, indent=2)
    
    # 7. WhisperX ASR + Translation (in Docker container)
    logger.info("Starting WhisperX ASR + Translation (Docker)")
    
    # Copy working file to movie directory so it's accessible in container
    container_audio_file = movie_dir / working_file.name
    if working_file != container_audio_file:
        import shutil
        shutil.copy2(working_file, container_audio_file)
    
    # Prepare arguments for ASR container
    src_lang = config.get("SRC_LANG", "hi")
    tgt_lang = config.get("TGT_LANG", "en")
    
    # Create a task file for the ASR container
    asr_task = {
        "task": "asr",
        "audio_file": str(container_audio_file.name),
        "source_lang": src_lang,
        "target_lang": tgt_lang,
        "model_name": "medium",
        "device": "cpu",
        "compute_type": "int8",
        "output_dir": "asr",
        "basename": parsed.sanitized_title,
        "initial_prompt": assembled_prompt.combined_text,
        "has_bias_windows": bias_windows is not None
    }
    
    asr_task_file = movie_dir / "asr_task.json"
    with open(asr_task_file, 'w') as f:
        json.dump(asr_task, f, indent=2)
    
    logger.info("Running ASR in Docker container...")
    
    # Create inline Python script for ASR
    asr_script = f"""
import sys
import json
from pathlib import Path

sys.path.insert(0, '/app')

# Use whisperx with Silero VAD (avoids pyannote dependency issues)
import whisperx
import whisperx.audio

from scripts.logger import create_logger
from scripts.bias_injection import BiasWindow

# Work in the mounted movie directory
work_dir = Path('/app/out/{parsed.sanitized_title}')

# Load task
with open(work_dir / 'asr_task.json') as f:
    task = json.load(f)

logger = create_logger('asr', Path('/app/logs'))
logger.info("ASR task started in container")

# Load HF token
with open('/app/config/secrets.json') as f:
    secrets = json.load(f)

# Load model with Silero VAD (not pyannote)
logger.info(f"Loading WhisperX model: {{task['model_name']}}")
logger.info(f"  Device: {{task['device']}}")
logger.info(f"  Compute type: {{task['compute_type']}}")
logger.info(f"  VAD method: silero (avoiding pyannote)")

# Prepare ASR options with initial_prompt and hotwords for context-aware transcription
asr_options = {{}}
if task.get('initial_prompt'):
    asr_options['initial_prompt'] = task['initial_prompt']
    logger.info(f"  Using initial prompt: {{task['initial_prompt'][:100]}}...")

# Add hotwords from bias terms if available
if task.get('has_bias_windows'):
    bias_dir = work_dir / 'bias'
    if bias_dir.exists():
        # Collect unique bias terms from all windows
        all_bias_terms = set()
        for window_file in sorted(bias_dir.glob('bias.window.*.json')):
            with open(window_file) as f:
                w = json.load(f)
                all_bias_terms.update(w.get('bias_terms', []))
        if all_bias_terms:
            # Create hotwords string (comma-separated)
            hotwords = ', '.join(sorted(all_bias_terms)[:50])  # Limit to top 50
            asr_options['hotwords'] = hotwords
            logger.info(f"  Using {{len(all_bias_terms)}} hotwords for better recognition")

try:
    model = whisperx.load_model(
        task['model_name'],
        device=task['device'],
        compute_type=task['compute_type'],
        vad_method='silero',  # Use Silero VAD instead of pyannote
        download_root=None,
        asr_options=asr_options if asr_options else None
    )
    logger.info("  Model loaded successfully with Silero VAD and context options")
except Exception as e:
    logger.error(f"  Failed to load model: {{e}}")
    raise

# Load and transcribe audio
audio_path = str(work_dir / task['audio_file'])
logger.info(f"Loading audio: {{audio_path}}")

audio = whisperx.audio.load_audio(audio_path)
logger.info("Audio loaded successfully")

# Transcribe with context-aware settings (initial_prompt and hotwords now applied via asr_options)
logger.info("Starting transcription with context-aware settings...")
result = model.transcribe(
    audio,
    batch_size=16,
    language=task['source_lang'],
    task='translate' if task['target_lang'] != task['source_lang'] else 'transcribe'
)

logger.info(f"Transcription complete: {{len(result.get('segments', []))}} segments")

# Save results
output_dir = work_dir / task['output_dir']
output_dir.mkdir(parents=True, exist_ok=True)
output_file = output_dir / f"{{task['basename']}}.asr.json"

# Convert to JSON-serializable format
result_dict = {{
    'segments': result.get('segments', []),
    'language': result.get('language', task['source_lang'])
}}

with open(output_file, 'w') as f:
    json.dump(result_dict, f, indent=2)

logger.info(f"ASR complete, saved to {{output_file}}")

# Align if possible (optional, depends on alignment model availability)
try:
    logger.info(f"Loading alignment model for language: {{task['target_lang']}}")
    align_model, align_metadata = whisperx.load_align_model(
        language_code=task['target_lang'],
        device=task['device']
    )
    logger.info("Alignment model loaded")
    
    result_aligned = whisperx.align(
        result['segments'],
        align_model,
        align_metadata,
        audio,
        task['device'],
        return_char_alignments=False
    )
    
    result_dict['segments'] = result_aligned.get('segments', result['segments'])
    result_dict['word_segments'] = result_aligned.get('word_segments', [])
    
    with open(output_file, 'w') as f:
        json.dump(result_dict, f, indent=2)
    logger.info("Alignment complete")
except Exception as e:
    logger.warning(f"Alignment failed or not available: {{e}}")

logger.info("ASR container task finished")
"""
    
    # Save script to file
    asr_script_file = movie_dir / "asr_script.py"
    with open(asr_script_file, 'w') as f:
        f.write(asr_script)
    
    # Run ASR container
    try:
        result_proc = subprocess.run([
            "docker", "compose", "run", "--rm",
            "asr",
            "python", f"/app/out/{parsed.sanitized_title}/asr_script.py"
        ], check=True, capture_output=True, text=True, cwd=ROOT)
        
        if result_proc.stdout:
            logger.info(f"ASR output: {result_proc.stdout}")
        if result_proc.stderr:
            logger.warning(f"ASR stderr: {result_proc.stderr}")
            
    except subprocess.CalledProcessError as e:
        logger.error(f"ASR container failed: {e}")
        logger.error(f"stdout: {e.stdout}")
        logger.error(f"stderr: {e.stderr}")
        raise
    
    logger.info("ASR container execution complete")
    
    # Load ASR results
    asr_dir = movie_dir / "asr"
    asr_json = asr_dir / f"{parsed.sanitized_title}.asr.json"
    if not asr_json.exists():
        raise FileNotFoundError(f"ASR results not found: {asr_json}")
    
    with open(asr_json) as f:
        result = json.load(f)
    logger.info(f"Loaded ASR results from {asr_json}")
    
    # 8. Diarization (in Docker container)
    diarization_enabled = config.get("DEVICE_DIARIZATION") is not None and config.get("DEVICE_DIARIZATION") != ""
    if diarization_enabled and duration_seconds:
        logger.info("Starting speaker diarization (Docker)")
        
        # Create diarization script
        diarization_script = f"""
import sys
import json
from pathlib import Path

sys.path.insert(0, '/app')

from scripts.diarization import DiarizationProcessor
from scripts.logger import create_logger

work_dir = Path('/app/out/{parsed.sanitized_title}')

logger = create_logger('diarization', Path('/app/logs'))
logger.info("Diarization task started in container")

# Load secrets
with open('/app/config/secrets.json') as f:
    secrets = json.load(f)

# Load ASR results
asr_file = work_dir / 'asr' / '{parsed.sanitized_title}.asr.json'
with open(asr_file) as f:
    asr_result = json.load(f)

processor = DiarizationProcessor(
    hf_token=secrets['pyannote_token'],
    device='cpu',
    logger=logger
)

processor.load_model()

# Run diarization
audio_file = str(work_dir / '{container_audio_file.name}')
logger.info(f"Diarizing: {{audio_file}}")

diarization_result = processor.diarize_audio(audio_file)

# Load speaker map if available (not implemented yet)
speaker_map = None

# Assign speakers
segments = processor.assign_speakers_to_segments(
    asr_result['segments'],
    diarization_result,
    speaker_map
)

# Update ASR result with speakers
asr_result['segments'] = segments
with open(asr_file, 'w') as f:
    json.dump(asr_result, f, indent=2)

# Save diarization results
processor.save_results(
    diarization_result,
    segments,
    work_dir,
    '{parsed.sanitized_title}'
)

logger.info("Diarization complete")
"""
        
        diarization_script_file = movie_dir / "diarization_script.py"
        with open(diarization_script_file, 'w') as f:
            f.write(diarization_script)
        
        logger.info("Running diarization in Docker container...")
        
        try:
            result_proc = subprocess.run([
                "docker", "compose", "run", "--rm",
                "diarization",
                "python", f"/app/out/{parsed.sanitized_title}/diarization_script.py"
            ], check=True, capture_output=True, text=True, cwd=ROOT)
            
            if result_proc.stdout:
                logger.info(f"Diarization output: {result_proc.stdout}")
            if result_proc.stderr:
                logger.warning(f"Diarization stderr: {result_proc.stderr}")
                
        except subprocess.CalledProcessError as e:
            logger.error(f"Diarization container failed: {e}")
            logger.error(f"stdout: {e.stdout}")
            logger.error(f"stderr: {e.stderr}")
            raise
        
        logger.info("Diarization container execution complete")
        
        # Reload ASR results with speaker info
        with open(asr_json) as f:
            result = json.load(f)
    
    # 9. Two-pass merge (translation refinement)
    if config.get("SECOND_PASS_ENABLED", False):
        logger.info("Starting two-pass translation refinement")
        from scripts.translation_refine import run_translation_refine_pipeline
        
        result_segments = result.get("segments", [])
        refined_segments = run_translation_refine_pipeline(
            segments=result_segments,
            output_dir=movie_dir,
            basename=parsed.sanitized_title,
            backend=config.get("SECOND_PASS_BACKEND", "opus-mt"),
            source_lang=src_lang,
            target_lang=tgt_lang,
            device=config.get("DEVICE_SECOND_PASS", "cpu"),
            logger=logger
        )
        result["segments"] = refined_segments
        logger.info("Translation refinement complete")
    
    # 10. NER extraction and canonicalization (in Docker container)
    if config.get("NER_ENABLED", False):
        logger.info("Starting NER extraction (Docker)")
        
        # Create NER script
        ner_script = f"""
import sys
import json
from pathlib import Path

sys.path.insert(0, '/app')

from scripts.ner_extraction import NERProcessor
from scripts.canonicalization import CanonicalProcessor
from scripts.logger import create_logger

work_dir = Path('/app/out/{parsed.sanitized_title}')

logger = create_logger('ner', Path('/app/logs'))
logger.info("NER task started in container")

# Load ASR results
asr_file = work_dir / 'asr' / '{parsed.sanitized_title}.asr.json'
with open(asr_file) as f:
    asr_result = json.load(f)

# NER processing
ner_proc = NERProcessor(
    model_name='{config.get("NER_PRESET", "en_core_web_trf")}',
    device='cpu',
    logger=logger
)

ner_proc.load_model()

# Annotate segments with entities
segments = ner_proc.annotate_segments(asr_result['segments'])

# Extract canonical candidates
canonical_candidates = ner_proc.extract_canonical_candidates(segments)

# Save NER results
ner_proc.save_results(
    segments,
    canonical_candidates,
    work_dir,
    '{parsed.sanitized_title}'
)

logger.info("NER extraction complete")

# Canonicalization
canon_map_path = '{config.get("CANON_MAP", "")}'
canonicalizer = CanonicalProcessor(
    canon_map_file=canon_map_path if canon_map_path else None,
    logger=logger
)

if canon_map_path and Path(canon_map_path).exists():
    canonicalizer.load_canon_map()

# Apply canonical replacements
segments = canonicalizer.apply_canonical_replacements(segments)

# Polish punctuation and case
segments = canonicalizer.polish_punctuation_and_case(segments)

logger.info("Canonicalization complete")

# Update ASR result
asr_result['segments'] = segments
with open(asr_file, 'w') as f:
    json.dump(asr_result, f, indent=2)

logger.info("NER container task finished")
"""
        
        ner_script_file = movie_dir / "ner_script.py"
        with open(ner_script_file, 'w') as f:
            f.write(ner_script)
        
        logger.info("Running NER in Docker container...")
        
        try:
            result_proc = subprocess.run([
                "docker", "compose", "run", "--rm",
                "ner",
                "python", f"/app/out/{parsed.sanitized_title}/ner_script.py"
            ], check=True, capture_output=True, text=True, cwd=ROOT)
            
            if result_proc.stdout:
                logger.info(f"NER output: {result_proc.stdout}")
            if result_proc.stderr:
                logger.warning(f"NER stderr: {result_proc.stderr}")
                
        except subprocess.CalledProcessError as e:
            logger.error(f"NER container failed: {e}")
            logger.error(f"stdout: {e.stdout}")
            logger.error(f"stderr: {e.stderr}")
            raise
        
        logger.info("NER container execution complete")
        
        # Reload ASR results with NER annotations
        with open(asr_json) as f:
            result = json.load(f)
    
    # 11. Generate final SRT
    logger.info("Generating final SRT file")
    
    srt_dir = movie_dir / "en_merged"
    srt_dir.mkdir(exist_ok=True)
    srt_file = srt_dir / f"{parsed.sanitized_title}.merged.srt"
    
    # Use canonicalizer's SRT generation if NER was enabled, otherwise use pysubs2
    if config.get("NER_ENABLED", False):
        from scripts.canonicalization import CanonicalProcessor
        canon_map_file = config.get("CANON_MAP")
        canonicalizer = CanonicalProcessor(
            canon_map_file=canon_map_file if canon_map_file and Path(canon_map_file).exists() else None,
            logger=logger
        )
        canonicalizer.generate_final_srt(result["segments"], srt_file)
    else:
        import pysubs2
        subs = pysubs2.SSAFile()
        for seg in result["segments"]:
            event = pysubs2.SSAEvent(
                start=int(seg["start"] * 1000),
                end=int(seg["end"] * 1000),
                text=seg.get("text", "")
            )
            subs.append(event)
        subs.save(str(srt_file))
    
    logger.info(f"SRT file saved to {srt_file}")
    
    # 12. Mux subtitles to video (MP4 with mov_text)
    logger.info("Muxing subtitles to video")
    output_video = movie_dir / f"{parsed.sanitized_title}.subs.mp4"
    try:
        subprocess.run([
            "ffmpeg", "-y",
            "-i", str(working_file),
            "-i", str(srt_file),
            "-c", "copy",
            "-c:s", "mov_text",
            "-metadata:s:s:0", f"language={tgt_lang}",
            str(output_video)
        ], check=True, capture_output=True)
        logger.info(f"Output video with subtitles: {output_video}")
    except Exception as e:
        logger.warning(f"Failed to mux with MP4/mov_text: {e}. Trying MKV fallback...")
        output_video = movie_dir / f"{parsed.sanitized_title}.subs.mkv"
        try:
            subprocess.run([
                "mkvmerge", "-o", str(output_video),
                str(working_file),
                "--language", f"0:{tgt_lang}",
                str(srt_file)
            ], check=True, capture_output=True)
            logger.info(f"Output video with subtitles (MKV): {output_video}")
        except Exception as e2:
            logger.error(f"Failed to mux subtitles: {e2}")
    
    # Write final manifest
    manifest = ManifestBuilder()
    manifest.set_input(str(input_path), parsed.title, parsed.year, duration_seconds)
    manifest.set_output_dir(str(movie_dir))
    manifest.set_era(era_lexicon.era_name if era_lexicon else None)
    if tmdb_metadata and tmdb_metadata.found:
        manifest.set_tmdb_status(
            True,
            tmdb_metadata.tmdb_id,
            len(tmdb_metadata.cast),
            len(tmdb_metadata.crew)
        )
    else:
        manifest.set_tmdb_status(False, None, 0, 0)
    
    if bias_windows:
        manifest.set_bias_windows(len(bias_windows), window_seconds, stride_seconds)
    
    manifest.set_device("whisperx", config.get("DEVICE_WHISPERX", "cpu"), "cpu")
    manifest.set_pipeline_step("prompt_assembly", True, completed=True)
    manifest.set_pipeline_step("whisperx_asr", True, completed=True)
    manifest.set_pipeline_step("diarization", config.get("DEVICE_DIARIZATION") is not None, completed=True)
    manifest.set_pipeline_step("translation_refine", config.get("SECOND_PASS_ENABLED", False), completed=True)
    manifest.set_pipeline_step("ner", config.get("NER_ENABLED", False), completed=True)
    manifest.add_output_file("srt", str(srt_file))
    manifest.add_output_file("video", str(output_video))
    manifest.finalize()
    manifest.save(log_dir / "manifest.json")
    
    logger.info("=" * 60)
    logger.info("Pipeline complete!")
    logger.info(f"Output directory: {movie_dir}")
    logger.info(f"SRT file: {srt_file}")
    logger.info(f"Video with subs: {output_video}")
    logger.info(f"Manifest: {log_dir / 'manifest.json'}")
    logger.info("=" * 60)
    
    print(f"\nPipeline complete!")
    print(f"Output: {output_video}")
    print(f"Manifest: {log_dir / 'manifest.json'}")


if __name__ == "__main__":
    main()
