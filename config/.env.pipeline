# ============================================================================
# CP-WhisperX-App Pipeline Configuration
# ============================================================================
# Version: 3.0.0
# Date: 2025-11-25
# 
# This file defines configuration for all pipeline stages.
# Parameters are organized by stage for clarity.
# Only actively used parameters are included.
#
# DEVELOPER GUIDELINES:
# 1. Parameters must be documented with purpose and valid values
# 2. Unused parameters should be removed immediately
# 3. New parameters must include usage examples
# 4. Group parameters by pipeline stage
# 5. Use consistent naming: STAGE_PARAM_NAME format
# ============================================================================

# ============================================================================
# GLOBAL CONFIGURATION
# ============================================================================

# ------------------------------------------------------------
# Job Identification (auto-generated by prepare-job.sh)
# ------------------------------------------------------------
# JOB_ID: Unique identifier in format job-YYYYMMDD-user-NNNN
# USER_ID: User identifier (default: 1)
# WORKFLOW_MODE: Pipeline workflow type
#   Values: transcribe | translate | subtitle
JOB_ID=
USER_ID=1
WORKFLOW_MODE=subtitle

# ------------------------------------------------------------
# Media Metadata (auto-populated by prepare-job.sh)
# ------------------------------------------------------------
# TITLE: Movie/media title
# YEAR: Release year (for TMDB lookup)
TITLE=
YEAR=

# ------------------------------------------------------------
# Directory Paths (auto-configured by prepare-job.sh)
# ------------------------------------------------------------
# IN_ROOT: Input media directory
# OUTPUT_ROOT: Job output directory (out/YYYY/MM/DD/user/N/)
# LOG_ROOT: Log files directory
IN_ROOT=
OUTPUT_ROOT=
LOG_ROOT=

# ------------------------------------------------------------
# Logging Configuration
# ------------------------------------------------------------
# LOG_LEVEL: Logging verbosity
#   Values: DEBUG | INFO | WARN | ERROR | CRITICAL
#   Default: INFO
# LOG_TO_CONSOLE: Enable console output (true/false)
# LOG_TO_FILE: Enable file logging (true/false)
LOG_LEVEL=INFO
LOG_TO_CONSOLE=true
LOG_TO_FILE=true

# ------------------------------------------------------------
# External Services (used in various stages)
# ------------------------------------------------------------
# SECRETS_PATH: Path to secrets.json (API keys for TMDB, LLM, etc.)
#   Default: ./config/secrets.json
#   Format: {"tmdb_api_key": "...", "anthropic_api_key": "...", etc.}
SECRETS_PATH=./config/secrets.json

# ------------------------------------------------------------
# Device Configuration (applies to all ML stages)
# ------------------------------------------------------------
# DEVICE: Primary compute device
#   Values: cpu | mps (Apple Silicon) | cuda (NVIDIA GPU)
#   Default: Auto-detected by prepare-job.sh
#   Note: Stages auto-fallback to CPU if MPS/CUDA fails
DEVICE=mps

# MPS Environment Variables (Apple Silicon optimization)
# PYTORCH_MPS_HIGH_WATERMARK_RATIO: Memory management
#   Values: 0.0 (default) - 1.0
# PYTORCH_ENABLE_MPS_FALLBACK: Auto-fallback to CPU
#   Values: 0 (disabled) | 1 (enabled)
# MPS_ALLOC_MAX_SIZE_MB: Max memory allocation
#   Values: Integer (MB), default: 4096
PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0
PYTORCH_ENABLE_MPS_FALLBACK=0
MPS_ALLOC_MAX_SIZE_MB=4096

# ------------------------------------------------------------
# Docker Configuration (for containerized deployments)
# ------------------------------------------------------------
# DOCKER_REGISTRY: Docker registry for pipeline images
# DOCKER_TAG: Image version tag
# DOCKER_MEMORY_LIMIT: Container memory limit
# DOCKER_CPU_LIMIT: Container CPU limit
DOCKER_REGISTRY=rajiup
DOCKER_TAG=latest
DOCKER_MEMORY_LIMIT=10g
DOCKER_CPU_LIMIT=4

# ------------------------------------------------------------
# Advanced: Large File Processing
# ------------------------------------------------------------
# ENABLE_CHUNKING: Split large files into chunks
#   Values: true | false
#   When: Auto-enabled for files > 30 minutes
# CHUNK_DURATION_MINUTES: Chunk length
#   Values: Integer (minutes), default: 30
ENABLE_CHUNKING=false
CHUNK_DURATION_MINUTES=30

# ============================================================================
# PIPELINE STAGE CONTROL
# ============================================================================
# Enable/disable individual stages (true/false)
# prepare-job.sh sets these based on workflow mode
# ============================================================================

# Legacy stage control (backward compatibility)
STEP_DEMUX=true                    # Stage 1: Extract audio from video
STEP_TMDB_METADATA=true            # Stage 2: Fetch movie metadata
STEP_VAD_SILERO=true               # Stage 5: Fast voice activity detection
STEP_VAD_PYANNOTE=true             # Stage 6: Precise voice activity detection
STEP_WHISPERX=true                 # Stage 7: Speech recognition & translation
STEP_DIARIZATION=true              # Stage 8: Speaker identification
STEP_SUBTITLE_GEN=true             # Stage 12: Generate SRT subtitles
STEP_MUX=true                      # Stage 13: Embed subtitles in video

# ============================================================================
# MODULAR PIPELINE STAGE CONTROL (Phase 4)
# ============================================================================
# New standardized stage enable/disable controls
# Format: STAGE_XX_NAME_ENABLED=true|false
# ============================================================================

# Core Processing Stages
STAGE_01_DEMUX_ENABLED=true          # Audio extraction from video
STAGE_02_TMDB_ENABLED=true           # TMDB metadata enrichment
STAGE_03_GLOSSARY_ENABLED=true       # Glossary loading and preparation
STAGE_04_ASR_ENABLED=true            # Automatic speech recognition (WhisperX)
STAGE_05_NER_ENABLED=true            # Named entity recognition
STAGE_06_LYRICS_ENABLED=true         # Lyrics detection and marking
STAGE_07_HALLUCINATION_ENABLED=true  # Hallucination removal
STAGE_08_TRANSLATION_ENABLED=true    # Translation (IndicTrans2/NLLB)
STAGE_09_SUBTITLE_ENABLED=true       # Subtitle generation (SRT/VTT)
STAGE_10_MUX_ENABLED=true            # Video muxing (embed subtitles)

# ============================================================================
# STAGE 1: DEMUX - Audio Extraction
# ============================================================================
# Purpose: Extract audio track from video using FFmpeg
# Input: Video file (MP4, MKV, AVI, etc.)
# Output: Audio WAV file
# Device: CPU only
# ============================================================================

# Audio output format settings
# AUDIO_SAMPLE_RATE: Sampling rate in Hz
#   Values: 8000 | 16000 | 22050 | 44100 | 48000
#   Default: 16000 (required for Whisper)
# AUDIO_CHANNELS: Channel count
#   Values: 1 (mono) | 2 (stereo)
#   Default: 1 (mono required for VAD)
# AUDIO_FORMAT: Output format
#   Values: wav | flac | mp3
#   Default: wav (lossless, required)
# AUDIO_CODEC: Audio codec
#   Values: pcm_s16le | pcm_s24le | pcm_s32le
#   Default: pcm_s16le (16-bit PCM)
AUDIO_SAMPLE_RATE=16000
AUDIO_CHANNELS=1
AUDIO_FORMAT=wav
AUDIO_CODEC=pcm_s16le

# ============================================================================
# STAGE 1.5: SOURCE SEPARATION - Vocal Extraction
# ============================================================================
# Purpose: Extract vocals, remove background music using Demucs
# Input: Audio WAV file
# Output: Vocals-only WAV file
# Device: CPU (Demucs model)
# Impact: Significantly improves ASR accuracy for songs
# ============================================================================

# SOURCE_SEPARATION_ENABLED: Enable vocal extraction
#   Values: true | false
#   Default: true (recommended for Bollywood content)
# SOURCE_SEPARATION_QUALITY: Processing quality
#   Values: fast | balanced | quality
#   Default: quality
#   Impact: fast (2x speed) | balanced (1x) | quality (0.5x, best results)
SOURCE_SEPARATION_ENABLED=true
SOURCE_SEPARATION_QUALITY=quality

# ============================================================================
# STAGE 2: TMDB - Movie Metadata
# ============================================================================
# Purpose: Fetch cast, crew, synopsis from TMDB API
# Input: Movie title and year
# Output: tmdb/metadata.json
# Device: CPU (API calls)
# Requires: TMDB API key in secrets.json
# ============================================================================

# TMDB_ENABLED: Enable metadata fetching
#   Values: true | false
#   Default: true (improves NER accuracy)
# TMDB_LANGUAGE: Metadata language
#   Values: ISO 639-1 codes (en-US, hi-IN, etc.)
#   Default: en-US
TMDB_ENABLED=true
TMDB_LANGUAGE=en-US

# ============================================================================
# STAGE 4: BIAS - ASR Prompt Enhancement
# ============================================================================
# Purpose: Create contextual prompts with names/terms for ASR
# Input: NER entities + TMDB metadata
# Output: Time-windowed bias terms
# Device: CPU
# ============================================================================

# BIAS_ENABLED: Enable bias term generation
#   Values: true | false
#   Default: true (improves name recognition by 60-85%)
# BIAS_WINDOW_SECONDS: Time window duration
#   Values: Integer (seconds), default: 45
#   Note: Shorter windows = more contextual, longer = more efficient
# BIAS_STRIDE_SECONDS: Window overlap
#   Values: Integer (seconds), default: 15
# BIAS_TOPK: Max terms per window
#   Values: Integer, default: 10
# BIAS_MIN_CONFIDENCE: Minimum NER confidence
#   Values: 0.0 - 1.0, default: 0.6
BIAS_ENABLED=true
BIAS_WINDOW_SECONDS=30
BIAS_STRIDE_SECONDS=10
BIAS_TOPK=15
# BIAS_STRATEGY: Bias prompting strategy for name recognition
#   Values: global | hybrid | chunked_windows | chunked
#   Default: global (fast, good accuracy)
#   hybrid: Best balance of speed and accuracy
#   chunked_windows: Highest accuracy, slower
BIAS_STRATEGY=hybrid
BIAS_MIN_CONFIDENCE=0.6

# ============================================================================
# STAGE 5: SILERO VAD - Voice Activity Detection (Fast)
# ============================================================================
# Purpose: Fast, coarse voice activity detection
# Input: Audio WAV file
# Output: Speech segments with timestamps
# Device: CPU/MPS/CUDA (auto-fallback to CPU)
# ============================================================================

# SILERO_THRESHOLD: Voice detection sensitivity
#   Values: 0.0 - 1.0, default: 0.5
#   Lower = more sensitive (may include noise)
#   Higher = stricter (may miss quiet speech)
# SILERO_MIN_SPEECH_DURATION_MS: Minimum speech segment
#   Values: Integer (ms), default: 250
# SILERO_MIN_SILENCE_DURATION_MS: Minimum silence gap
#   Values: Integer (ms), default: 100
# SILERO_MERGE_GAP_SEC: Merge nearby segments
#   Values: Float (seconds), default: 0.35
# SILERO_DEVICE: Compute device
#   Values: cpu | mps | cuda
#   Default: Inherited from global DEVICE
SILERO_THRESHOLD=0.5
SILERO_MIN_SPEECH_DURATION_MS=250
SILERO_MIN_SILENCE_DURATION_MS=100
SILERO_MERGE_GAP_SEC=0.35
SILERO_DEVICE=mps

# ============================================================================
# STAGE 6: PYANNOTE VAD - Voice Activity Detection (Precise)
# ============================================================================
# Purpose: Precise VAD refinement for better boundaries
# Input: Audio + Silero segments
# Output: Refined speech segments
# Device: CPU/MPS/CUDA (auto-fallback to CPU)
# ============================================================================

# PYANNOTE_ONSET: Speech start threshold
#   Values: 0.0 - 1.0, default: 0.5
# PYANNOTE_OFFSET: Speech end threshold
#   Values: 0.0 - 1.0, default: 0.5
# PYANNOTE_MIN_DURATION_ON: Minimum speech duration
#   Values: Float (seconds), default: 0.0
# PYANNOTE_MIN_DURATION_OFF: Minimum silence duration
#   Values: Float (seconds), default: 0.0
# PYANNOTE_DEVICE: Compute device
#   Values: cpu | mps | cuda
PYANNOTE_ONSET=0.5
PYANNOTE_OFFSET=0.5
PYANNOTE_MIN_DURATION_ON=0.0
PYANNOTE_MIN_DURATION_OFF=0.0
PYANNOTE_DEVICE=mps

# ============================================================================
# STAGE 7: WHISPERX ASR - Speech Recognition
# ============================================================================
# Purpose: Transcribe speech to text with word-level timestamps
# Input: Audio + VAD segments
# Output: Transcriptions with timestamps
# Device: CPU/MPS/CUDA (auto-fallback to CPU)
# ============================================================================

# Core Whisper Settings
# WHISPER_MODEL: Model size/version
#   Values: tiny | base | small | medium | large | large-v2 | large-v3
#   Default: large-v3 (best accuracy)
#   Size: tiny (39M) → large-v3 (1550M)
# WHISPER_COMPUTE_TYPE: Precision
#   Values: int8 | float16 | float32
#   Default: float16 (best speed/accuracy balance)
# BATCH_SIZE: Parallel processing batch size
#   Values: Integer, default: 2
#   Note: Larger = faster but more memory
WHISPER_MODEL=large-v3
WHISPER_COMPUTE_TYPE=float16
BATCH_SIZE=2

# Language Settings
# WHISPER_LANGUAGE: Source audio language
#   Values: ISO 639-1 codes (hi, en, ta, te, etc.)
#   Default: hi (Hindi)
# TARGET_LANGUAGE: Translation target
#   Values: ISO 639-1 codes
#   Default: en (English)
# WHISPER_TASK: ASR task type
#   Values: transcribe | translate
#   Default: translate
#   Note: In two-step mode, this is overridden to 'transcribe'
# TWO_STEP_TRANSCRIPTION: Enable two-step workflow
#   Values: true | false
#   Default: false (read from job config)
#   Note: Phase 2 Task 1 - Transcribe in source language, then translate
#   Impact: +5-8% accuracy on Hindi transcription
WHISPER_LANGUAGE=hi
TARGET_LANGUAGE=en
WHISPER_TASK=translate
TWO_STEP_TRANSCRIPTION=false

# Advanced Whisper Parameters
# ==============================================================================
# PHASE 1 OPTIMIZATION: Quick Wins for Subtitle Accuracy
# ==============================================================================
# Changes made 2025-11-28 for subtitle accuracy improvements:
# - Reduced temperature values (6 → 3) for 20-30% speed improvement
# - Optimized thresholds for hallucination prevention
# - Maintained quality while improving performance
# ==============================================================================
# WHISPER_TEMPERATURE: Sampling temperature
#   Phase 1 Optimization: Reduced to 3 values (0.0,0.2,0.4) from 6 values
#   Values: Comma-separated floats, default: 0.0,0.2,0.4,0.6,0.8,1.0
#   Note: 0.0 = deterministic, higher = more creative
#   Impact: 20-30% faster, minimal accuracy loss
WHISPER_TEMPERATURE=0.0,0.2,0.4
# WHISPER_BEAM_SIZE: Beam search width
#   Values: Integer, default: 5
#   Current: 5 (good balance of speed/accuracy)
WHISPER_BEAM_SIZE=5
# WHISPER_BEST_OF: Number of candidates
#   Values: Integer, default: 5
#   Current: 5 (balanced)
WHISPER_BEST_OF=5
# WHISPER_PATIENCE: Beam search patience
#   Values: Float, default: 1.0
WHISPER_PATIENCE=1.0
# WHISPER_LENGTH_PENALTY: Length preference
#   Values: Float, default: 1.0 (neutral)
WHISPER_LENGTH_PENALTY=1.0
# WHISPER_NO_SPEECH_THRESHOLD: Silence detection
#   Phase 1 Optimization: Increased to 0.65 from 0.6 for stricter silence detection
#   Values: 0.0 - 1.0, default: 0.6
#   Impact: Reduces false speech detection
WHISPER_NO_SPEECH_THRESHOLD=0.65
# WHISPER_LOGPROB_THRESHOLD: Confidence threshold
#   Phase 1 Optimization: Set to -0.7 (stricter than default -1.0)
#   Values: Float, default: -1.0 (no limit)
#   Impact: Filters low-confidence segments
WHISPER_LOGPROB_THRESHOLD=-0.7
# WHISPER_COMPRESSION_RATIO_THRESHOLD: Repetition detection
#   Phase 1 Optimization: Set to 2.2 (stricter) to detect repetitions earlier
#   Values: Float, default: 2.4
#   Impact: Catches repetitive hallucinations sooner
WHISPER_COMPRESSION_RATIO_THRESHOLD=2.2
# WHISPER_CONDITION_ON_PREVIOUS_TEXT: Context awareness
#   CRITICAL: Must be false to prevent context-dependent hallucinations
#   Values: true | false, default: true
WHISPER_CONDITION_ON_PREVIOUS_TEXT=false
# WHISPER_MIN_DURATION: Minimum segment duration (Phase 1 addition)
#   Values: Float (seconds), default: 0.1
#   Impact: Filters zero-duration and micro-segments
WHISPER_MIN_DURATION=0.1

# Multi-Pass Refinement (Phase 3, Task 1)
# MULTIPASS_ENABLED: Enable multi-pass refinement of low-confidence segments
#   Values: true | false
#   Default: false
#   Note: Re-processes difficult segments with enhanced parameters
#   Impact: +3-5% accuracy on low-confidence segments, +30-40% processing time
# MULTIPASS_CONFIDENCE_THRESHOLD: Confidence threshold for refinement
#   Values: 0.0 - 1.0
#   Default: 0.6
#   Note: Segments below this threshold are re-transcribed
# MULTIPASS_MAX_ITERATIONS: Maximum refinement passes
#   Values: Integer (1-5)
#   Default: 3
#   Note: More passes = better quality but slower
# MULTIPASS_BEAM_SIZE_INCREMENT: Beam size increase per pass
#   Values: Integer
#   Default: 5
#   Note: Pass 1: beam=10, Pass 2: beam=15, Pass 3: beam=20
# MULTIPASS_MIN_SEGMENT_DURATION: Minimum segment length for refinement
#   Values: Float (seconds)
#   Default: 1.0
#   Note: Very short segments are skipped
MULTIPASS_ENABLED=false
MULTIPASS_CONFIDENCE_THRESHOLD=0.6
MULTIPASS_MAX_ITERATIONS=3
MULTIPASS_BEAM_SIZE_INCREMENT=5
MULTIPASS_MIN_SEGMENT_DURATION=1.0

# Speaker-Aware Bias (Phase 3, Task 2)
# SPEAKER_AWARE_BIAS_ENABLED: Use speaker info for context-aware glossary
#   Values: true | false
#   Default: false
#   Note: Requires diarization to run before ASR
#   Impact: +5-8% accuracy on character names
# SPEAKER_BIAS_BOOST_FACTOR: Weight multiplier for speaker-associated terms
#   Values: Float (1.0 - 3.0)
#   Default: 1.5
#   Note: Higher = stronger boost for character's own name when speaking
# SPEAKER_CONTEXT_WINDOW: Time window for speaker context (seconds)
#   Values: Float
#   Default: 10.0
#   Note: How far to look for speaker transitions
# SPEAKER_MIN_COOCCURRENCE: Minimum co-occurrences to associate speaker with term
#   Values: Integer
#   Default: 2
#   Note: How many times term must appear in speaker's segments
SPEAKER_AWARE_BIAS_ENABLED=false
SPEAKER_BIAS_BOOST_FACTOR=1.5
SPEAKER_CONTEXT_WINDOW=10.0
SPEAKER_MIN_COOCCURRENCE=2

# Lyrics Detection and Optimization (Phase 3, Task 3)
# LYRICS_DETECTION_ENABLED: Detect and optimize for song/music segments
#   Values: true | false
#   Default: false
#   Note: Applies different ASR parameters for lyrics vs dialogue
#   Impact: +2-3% accuracy on overall content
# LYRICS_MUSIC_THRESHOLD: Confidence threshold for music classification
#   Values: 0.0 - 1.0
#   Default: 0.7
#   Note: Higher = stricter classification
# LYRICS_BEAM_SIZE: Beam size for lyrics transcription
#   Values: Integer
#   Default: 12
#   Note: Slightly larger than dialogue (allows more creativity)
# LYRICS_TEMPERATURE: Sampling temperature for lyrics
#   Values: Float (0.0 - 1.0)
#   Default: 0.1
#   Note: Slightly higher than dialogue (songs have variations)
# LYRICS_ALLOW_REPETITION: Allow repetition in lyrics
#   Values: true | false
#   Default: true
#   Note: Songs naturally repeat (na na, chorus, etc.)
# LYRICS_COMPRESSION_RATIO_THRESHOLD: Compression threshold for lyrics
#   Values: Float
#   Default: 3.0
#   Note: Higher than dialogue (songs compress more)
LYRICS_DETECTION_ENABLED=false
LYRICS_MUSIC_THRESHOLD=0.7
LYRICS_BEAM_SIZE=12
LYRICS_TEMPERATURE=0.1
LYRICS_ALLOW_REPETITION=true
LYRICS_COMPRESSION_RATIO_THRESHOLD=3.0

# WhisperX Alignment Settings
# WHISPERX_ALIGN_EXTEND: Extend segment boundaries
#   Values: Float (seconds), default: 2.0
# WHISPERX_ALIGN_FROM_PREV: Use previous alignment
#   Values: true | false, default: true
WHISPERX_ALIGN_EXTEND=2.0
WHISPERX_ALIGN_FROM_PREV=true

# Backend Selection
# WHISPER_BACKEND: ASR backend engine
#   Values: whisperx | mlx | auto
#   Default: auto (selects optimal backend for detected hardware)
#   Details:
#     - mlx: Apple MLX framework (requires venv/mlx environment and MPS device)
#            Provides 2-4x speedup on Apple Silicon (M1/M2/M3)
#     - whisperx: WhisperX with CTranslate2 (works on CPU/CUDA/MPS)
#                 Supports bias parameters for character names
#     - auto: Automatically selects:
#             * mlx for Apple Silicon (MPS)
#             * whisperx for CUDA/CPU
#   Note: System will gracefully fall back to whisperx if mlx unavailable
#   Note: Device/backend compatibility is validated automatically
WHISPER_BACKEND=auto

# Device Assignment
# WHISPERX_DEVICE: Compute device for WhisperX
#   Values: cpu | mps | cuda
WHISPERX_DEVICE=mps

# Initial Prompt (optional, for bias injection)
# WHISPER_INITIAL_PROMPT: Context prompt for ASR
#   Values: Text string with names/terms
#   Default: Empty (filled by bias injection if enabled)
#   Example: "Amitabh Bachchan, Shahrukh Khan, Mumbai"
WHISPER_INITIAL_PROMPT=

# ============================================================================
# STAGE 7.5: HALLUCINATION REMOVAL
# ============================================================================
# Purpose: Remove repetitive/looping text errors from ASR
# Input: ASR transcripts
# Output: Cleaned transcripts
# Device: CPU (text processing)
# Impact: Reduces repetition rate by ~78%
# ============================================================================

# HALLUCINATION_REMOVAL_ENABLED: Enable cleaning
#   Values: true | false
#   Default: true (recommended)
# HALLUCINATION_LOOP_THRESHOLD: Min repeats to detect
#   Values: Integer, default: 3
#   Example: 3 = detect "बलल बलल बलल"
# HALLUCINATION_MAX_REPEATS: Max repeats to keep
#   Values: Integer, default: 2
#   Example: 2 = keep first 2 occurrences, remove rest
HALLUCINATION_REMOVAL_ENABLED=true
HALLUCINATION_LOOP_THRESHOLD=3
HALLUCINATION_MAX_REPEATS=2

# ============================================================================
# STAGE 8: DIARIZATION - Speaker Identification
# ============================================================================
# Purpose: Identify and label different speakers
# Input: Audio + transcripts
# Output: Speaker-labeled segments
# Device: CPU/MPS/CUDA (auto-fallback to CPU)
# ============================================================================

# DIARIZATION_MIN_SPEAKERS: Minimum expected speakers
#   Values: Integer, default: 1
# DIARIZATION_MAX_SPEAKERS: Maximum expected speakers
#   Values: Integer, default: 15
# DIARIZATION_MODEL: PyAnnote diarization model
#   Values: pyannote/speaker-diarization-3.1 (recommended)
# DIARIZATION_METHOD: Diarization algorithm
#   Values: pyannote (only supported method)
# DIARIZATION_DEVICE: Compute device
#   Values: cpu | mps | cuda
# SPEAKER_MAP: Optional speaker name mapping
#   Values: JSON string {"SPEAKER_00": "Amitabh", "SPEAKER_01": "Shahrukh"}
#   Default: Empty (generic labels)
DIARIZATION_MIN_SPEAKERS=1
DIARIZATION_MAX_SPEAKERS=15
DIARIZATION_MODEL=pyannote/speaker-diarization-3.1
DIARIZATION_METHOD=pyannote
DIARIZATION_DEVICE=mps
SPEAKER_MAP=

# ============================================================================
# STAGE 9: TRANSLATION - IndicTrans2/NLLB
# ============================================================================
# Purpose: High-quality translation for Indic languages
# Input: ASR transcripts
# Output: Translated text
# Device: CPU/MPS/CUDA
# ============================================================================

# IndicTrans2 Settings (for 22 Indic languages)
# INDICTRANS2_DEVICE: Compute device
#   Values: auto | cpu | mps | cuda
#   Default: auto (uses global DEVICE)
# INDICTRANS2_NUM_BEAMS: Beam search width
#   Values: Integer 1-10, default: 4
#   Higher = better quality, slower
# INDICTRANS2_MAX_NEW_TOKENS: Max translation length
#   Values: Integer, default: 128
INDICTRANS2_DEVICE=auto
INDICTRANS2_NUM_BEAMS=4
INDICTRANS2_MAX_NEW_TOKENS=128

# ------------------------------------------------------------
# Beam Search Optimization (NEW)
# ------------------------------------------------------------
# Automatically test beam sizes 4-10 and select optimal for quality

# INDICTRANS2_OPTIMIZE_BEAMS: Enable automatic beam size optimization
#   Values: true | false
#   Default: false (use fixed beam size from INDICTRANS2_NUM_BEAMS)
#   Impact: ~2x slower but 8-15% better quality
# INDICTRANS2_BEAM_MIN: Minimum beam size to test
#   Values: Integer, default: 4
# INDICTRANS2_BEAM_MAX: Maximum beam size to test
#   Values: Integer, default: 10
# INDICTRANS2_OPTIMIZATION_SAMPLE_SIZE: Number of segments to test
#   Values: Integer, default: 20
#   Note: Larger sample = more accurate but slower
INDICTRANS2_OPTIMIZE_BEAMS=false
INDICTRANS2_BEAM_MIN=4
INDICTRANS2_BEAM_MAX=10
INDICTRANS2_OPTIMIZATION_SAMPLE_SIZE=20

# NLLB Settings (fallback for non-Indic languages)
# SECOND_PASS_ENABLED: Enable NLLB translation
#   Values: true | false
#   Default: true
# SECOND_PASS_BACKEND: Translation backend
#   Values: nllb | indictrans2
#   Default: nllb
SECOND_PASS_ENABLED=true
SECOND_PASS_BACKEND=nllb

# Hybrid Translation (LLM for songs/poetry)
# USE_HYBRID_TRANSLATION: Enable LLM for creative content
#   Values: true | false
#   Default: true
#   Requires: Anthropic or OpenAI API key
# LLM_PROVIDER: LLM service
#   Values: anthropic | openai
#   Default: anthropic
# USE_LLM_FOR_SONGS: Apply LLM to song segments
#   Values: true | false
#   Default: true
# CONFIDENCE_THRESHOLD: Minimum confidence to accept translation
#   Values: 0.0 - 1.0, default: 0.7
#   Lower = fewer fallbacks (faster, cheaper)
#   Higher = more fallbacks (better quality)
# ENABLE_CONFIDENCE_FALLBACK: Enable confidence-based fallback
#   Values: true | false
#   Default: true
#   When enabled: Low confidence translations retry with alternative method
#   Impact: Improves quality by 15-30%, automatic LLM API failure recovery
USE_HYBRID_TRANSLATION=true
LLM_PROVIDER=anthropic
USE_LLM_FOR_SONGS=true
CONFIDENCE_THRESHOLD=0.7
ENABLE_CONFIDENCE_FALLBACK=true

# ------------------------------------------------------------
# Phase 4: Advanced Translation Features (NEW - 2025-11-28)
# ------------------------------------------------------------
# Purpose: Glossary protection and translation validation
# Protects proper nouns during translation and validates output quality

# TRANSLATION_GLOSSARY_PROTECTION: Enable glossary term protection
#   Values: true | false
#   Default: true
#   Impact: Prevents mistranslation of character names, places, titles
#   Example: "Jai" stays "Jai" instead of becoming "Victory"
TRANSLATION_GLOSSARY_PROTECTION=true

# TRANSLATION_VALIDATION_ENABLED: Enable post-translation validation
#   Values: true | false
#   Default: true
#   Impact: Validates glossary compliance and translation quality
TRANSLATION_VALIDATION_ENABLED=true

# TRANSLATION_MAX_LENGTH_RATIO: Maximum target/source length ratio
#   Values: Float, default: 3.0
#   Note: Translations longer than this ratio may indicate hallucinations
TRANSLATION_MAX_LENGTH_RATIO=3.0

# TRANSLATION_MIN_LENGTH_RATIO: Minimum target/source length ratio
#   Values: Float, default: 0.3
#   Note: Translations shorter than this may indicate dropped content
TRANSLATION_MIN_LENGTH_RATIO=0.3

# ============================================================================
# STAGE 10: LYRICS DETECTION
# ============================================================================
# Purpose: Identify song/musical segments
# Input: Audio features + transcripts
# Output: Song segment timestamps
# Device: CPU
# ============================================================================

# LYRICS_DETECTION_ENABLED: Enable song detection
#   Values: true | false
#   Default: true
# LYRICS_DETECTION_THRESHOLD: Confidence threshold for song detection
#   Values: 0.0 - 1.0, default: 0.5
#   Lower = more sensitive (catches more songs, may include dialogue with music)
#   Higher = more strict (only high-confidence songs)
#   Impact: Works with CONFIDENCE_THRESHOLD to optimize LLM usage
#   Example: 0.5 = balanced, 0.7 = strict (fewer false positives)
# LYRICS_MIN_DURATION: Minimum song length
#   Values: Float (seconds), default: 30.0
LYRICS_DETECTION_ENABLED=true
LYRICS_DETECTION_THRESHOLD=0.5
LYRICS_MIN_DURATION=30.0

# Lyrics Styling
# LYRIC_DETECT_ENABLED: Enable special lyrics formatting
#   Values: true | false
# LYRIC_THRESHOLD: Detection sensitivity
#   Values: 0.0 - 1.0
# LYRIC_STYLE: Subtitle style for songs
#   Values: lyric | italic | bold | normal
# LYRIC_MIN_DURATION: Min duration to apply style
#   Values: Float (seconds)
LYRIC_DETECT_ENABLED=true
LYRIC_THRESHOLD=0.5
LYRIC_STYLE=lyric
LYRIC_MIN_DURATION=30.0

# Song Bias Enhancement
# SONG_BIAS_ENABLED: Apply song-specific corrections
#   Values: true | false
#   Default: true (improves song title/artist accuracy)
# SONG_BIAS_FUZZY_THRESHOLD: Matching threshold
#   Values: 0.0 - 1.0, default: 0.80
#   Note: Lower than dialogue (songs transcribe poorly)
SONG_BIAS_ENABLED=true
SONG_BIAS_FUZZY_THRESHOLD=0.80

# ============================================================================
# STAGE 11: NER - Named Entity Recognition
# ============================================================================
# Purpose: Extract and correct named entities
# Input: Transcripts + TMDB metadata
# Output: Corrected entity names
# Device: CPU
# ============================================================================

# PRE-NER (before ASR, for bias generation)
# PRE_NER_MODEL: spaCy NER model
#   Values: en_core_web_sm | en_core_web_md | en_core_web_trf
#   Default: en_core_web_trf (transformer-based, most accurate)
# PRE_NER_ENTITY_TYPES: Entity types to extract
#   Values: Comma-separated (PERSON, ORG, GPE, LOC, FAC)
#   Default: PERSON,ORG,GPE,LOC,FAC
PRE_NER_MODEL=en_core_web_trf
PRE_NER_ENTITY_TYPES=PERSON,ORG,GPE,LOC,FAC

# POST-NER (after ASR, for correction)
# POST_NER_MODEL: spaCy NER model
# POST_NER_ENTITY_CORRECTION: Enable entity correction
#   Values: true | false
# POST_NER_TMDB_MATCHING: Match against TMDB data
#   Values: true | false
# POST_NER_CONFIDENCE_THRESHOLD: Min confidence for correction
#   Values: 0.0 - 1.0, default: 0.8
POST_NER_MODEL=en_core_web_trf
POST_NER_ENTITY_CORRECTION=true
POST_NER_TMDB_MATCHING=true
POST_NER_CONFIDENCE_THRESHOLD=0.8

# ============================================================================
# STAGE 12: SUBTITLE GENERATION
# ============================================================================
# Purpose: Generate properly formatted SRT subtitle files
# Input: Aligned transcripts with timestamps
# Output: SRT files
# Device: CPU
# ============================================================================

# Format Settings
# SUBTITLE_FORMAT: Output format
#   Values: srt | vtt | ass
#   Default: srt (most compatible)
# SUBTITLE_MAX_LINE_LENGTH: Max characters per line
#   Values: Integer, default: 42 (industry standard)
# SUBTITLE_MAX_LINES: Max lines per subtitle
#   Values: Integer, default: 2
# SUBTITLE_WORD_LEVEL_TIMESTAMPS: Use word-level timing
#   Values: true | false
#   Default: false (sentence-level is more readable)
SUBTITLE_FORMAT=srt
SUBTITLE_MAX_LINE_LENGTH=42
SUBTITLE_MAX_LINES=2
SUBTITLE_WORD_LEVEL_TIMESTAMPS=false

# Timing Settings
# SUBTITLE_MAX_DURATION: Max subtitle display time
#   Values: Float (seconds), default: 7.0
# SUBTITLE_MIN_DURATION: Min subtitle display time
#   Values: Float (seconds), default: 1.0
# SUBTITLE_MERGE_SHORT: Merge very short subtitles
#   Values: true | false
#   Default: true
SUBTITLE_MAX_DURATION=7.0
SUBTITLE_MIN_DURATION=1.0
SUBTITLE_MERGE_SHORT=true

# CPS (Characters Per Second) Settings
# CPS_ENFORCEMENT: Enforce reading speed limits
#   Values: true | false
#   Default: true (improves readability)
# CPS_TARGET: Target reading speed
#   Values: Float (chars/sec), default: 15.0
#   Industry standard: 13-17 CPS
# CPS_HARD_CAP: Maximum reading speed
#   Values: Float (chars/sec), default: 17.0
# CPS_MAX_GAP: Max gap before splitting subtitle
#   Values: Float (seconds), default: 3.0
CPS_ENFORCEMENT=true
CPS_TARGET=15.0
CPS_HARD_CAP=17.0
CPS_MAX_GAP=3.0

# ------------------------------------------------------------
# Phase 3: Intelligent Segment Merging (NEW - 2025-11-28)
# ------------------------------------------------------------
# Purpose: Merge short subtitle segments for optimal readability
# Reduces segment count by 40-60% while improving reading speed

# SEGMENT_MERGING_ENABLED: Enable/disable segment merging
#   Values: true | false
#   Default: true
#   Note: Phase 3 enhancement for better subtitle readability
SEGMENT_MERGING_ENABLED=true

# SEGMENT_MERGE_MAX_GAP: Maximum gap between segments to merge
#   Values: Float (seconds), default: 1.5
#   Note: Segments with gaps > this won't be merged
SEGMENT_MERGE_MAX_GAP=1.5

# SEGMENT_MERGE_MAX_CHARS: Maximum characters in merged subtitle
#   Values: Integer, default: 84 (2 lines × 42 chars)
#   Note: Prevents creating overly long subtitles
SEGMENT_MERGE_MAX_CHARS=84

# SEGMENT_MERGE_MAX_DURATION: Maximum duration of merged subtitle
#   Values: Float (seconds), default: 7.0
#   Note: Prevents subtitles from staying too long
SEGMENT_MERGE_MAX_DURATION=7.0

# SEGMENT_MERGE_MIN_CHARS_PER_SECOND: Minimum reading speed
#   Values: Float (chars/sec), default: 17.0
#   Note: Comfortable reading speed (17-20 CPS)
SEGMENT_MERGE_MIN_CPS=17.0

# SEGMENT_MERGE_MAX_CHARS_PER_SECOND: Maximum reading speed
#   Values: Float (chars/sec), default: 20.0
#   Note: Upper limit for comfortable reading
SEGMENT_MERGE_MAX_CPS=20.0

# Speaker Labels
# SUBTITLE_INCLUDE_SPEAKER_LABELS: Show speaker names
#   Values: true | false
#   Default: false (cleaner for general viewing)
# SUBTITLE_SPEAKER_FORMAT: Label format
#   Values: Template string with {speaker} placeholder
#   Default: [{speaker}]
#   Example: "{speaker}:" or "- {speaker} -"
SUBTITLE_INCLUDE_SPEAKER_LABELS=false
SUBTITLE_SPEAKER_FORMAT=[{speaker}]

# ------------------------------------------------------------
# Unified Glossary System (Phase 1) - UPDATED
# ------------------------------------------------------------
# Comprehensive glossary manager with TMDB integration, caching, and learning

# GLOSSARY_ENABLE: Enable/disable glossary building
#   Values: true | false
#   Default: true
#   Note: Master switch for glossary-builder stage
GLOSSARY_ENABLE=true

# GLOSSARY_SEED_SOURCES: Data sources for glossary building
#   Values: Comma-separated list: asr,tmdb,master,film
#   Default: asr,tmdb
#   - asr: Extract terms from ASR transcript
#   - tmdb: Extract cast/crew from TMDB enrichment
#   - master: Load from hinglish_master.tsv
#   - film: Load film-specific overrides
GLOSSARY_SEED_SOURCES=asr,tmdb,master

# GLOSSARY_MIN_CONF: Minimum confidence threshold for term inclusion
#   Values: 0.0 - 1.0
#   Default: 0.55
#   Note: Terms with confidence < threshold are excluded
GLOSSARY_MIN_CONF=0.55

# GLOSSARY_MASTER: Path to master Hinglish glossary
#   Values: Path to TSV file
#   Default: glossary/hinglish_master.tsv
#   Format: source<tab>preferred_english<tab>notes<tab>context
GLOSSARY_MASTER=glossary/hinglish_master.tsv

# GLOSSARY_PROMPTS_DIR: Directory for film-specific prompts
#   Values: Directory path
#   Default: glossary/prompts
#   Note: Contains custom terms for specific films
GLOSSARY_PROMPTS_DIR=glossary/prompts

# GLOSSARY_CACHE_DIR: Directory for glossary cache
#   Values: Directory path
#   Default: glossary/cache
#   Note: Stores TMDB glossaries for reuse
GLOSSARY_CACHE_DIR=glossary/cache

# GLOSSARY_CACHE_ENABLED: Enable TMDB glossary caching
#   Values: true | false
#   Default: true (caches TMDB glossaries per-film)
GLOSSARY_CACHE_ENABLED=true

# GLOSSARY_CACHE_TTL_DAYS: Cache expiry in days
#   Values: Integer, default: 30
GLOSSARY_CACHE_TTL_DAYS=30

# GLOSSARY_LEARNING_ENABLED: Enable term frequency learning
#   Values: true | false
#   Default: false (Phase 2 feature)
GLOSSARY_LEARNING_ENABLED=false

# GLOSSARY_AUTO_LEARN: Auto-detect names from transcripts
#   Values: true | false
#   Default: true (Phase 2: Task 3)
#   Note: Extracts character names from ASR output
GLOSSARY_AUTO_LEARN=true

# GLOSSARY_MIN_OCCURRENCES: Minimum name occurrences
#   Values: Integer, default: 2
#   Note: Names must appear N times to be considered
GLOSSARY_MIN_OCCURRENCES=2

# GLOSSARY_CONFIDENCE_THRESHOLD: Confidence for inclusion
#   Values: Integer, default: 3
#   Note: Names appearing N+ times are high-confidence
GLOSSARY_CONFIDENCE_THRESHOLD=3

# Legacy Glossary Variables (for backwards compatibility)
# Note: These are used by subtitle-gen stage, not glossary-builder
GLOSSARY_ENABLED=true
GLOSSARY_PATH=glossary/hinglish_master.tsv
GLOSSARY_STRATEGY=adaptive
FILM_PROMPT_PATH=
FREQUENCY_DATA_PATH=glossary/glossary_learned/term_frequency.json

# ============================================================================
# STAGE 13: MUX - Subtitle Embedding
# ============================================================================
# Purpose: Embed SRT subtitles into video file
# Input: Original video + SRT files
# Output: Video with embedded subtitles
# Device: CPU (FFmpeg)
# ============================================================================

# MUX_SUBTITLE_CODEC: Subtitle codec
#   Values: mov_text (MP4) | srt | ass | webvtt
#   Default: mov_text (MP4 compatible)
# MUX_SUBTITLE_LANGUAGE: Subtitle track language code
#   Values: ISO 639-2 codes (eng, hin, tam, etc.)
#   Default: eng
# MUX_SUBTITLE_TITLE: Track title/label
#   Values: String, default: English
# MUX_COPY_VIDEO: Copy video stream without re-encoding
#   Values: true | false
#   Default: true (fast, lossless)
# MUX_COPY_AUDIO: Copy audio stream without re-encoding
#   Values: true | false
#   Default: true (fast, lossless)
# MUX_CONTAINER_FORMAT: Output container format
#   Values: mp4 | mkv | mov
#   Default: mp4 (most compatible)
MUX_SUBTITLE_CODEC=mov_text
MUX_SUBTITLE_LANGUAGE=eng
MUX_SUBTITLE_TITLE=English
MUX_COPY_VIDEO=true
MUX_COPY_AUDIO=true
MUX_CONTAINER_FORMAT=mp4

# ============================================================================
# PHASE 3: ADVANCED FEATURES (Tier 3)
# ============================================================================
# Purpose: Advanced ASR optimization features
# Expected Improvement: +10% accuracy (90-93% → 93-96%)
# ============================================================================

# ------------------------------------------------------------
# Multi-Pass Refinement
# ------------------------------------------------------------
# MULTIPASS_ENABLED: Enable multi-pass refinement for low-confidence segments
#   Values: true | false
#   Default: false (opt-in for quality improvement)
#   Impact: 3-5% accuracy improvement, +30-40% processing time
# MULTIPASS_CONFIDENCE_THRESHOLD: Confidence threshold for refinement
#   Values: Float (0.0-1.0), default: 0.6
#   Note: Segments below this confidence are re-transcribed
# MULTIPASS_MAX_ITERATIONS: Maximum refinement passes
#   Values: Integer (1-5), default: 3
# MULTIPASS_BEAM_SIZE_INCREMENT: Beam size increase per pass
#   Values: Integer, default: 5
#   Note: Pass 2 uses beam_size + 5, Pass 3 uses beam_size + 10, etc.
# MULTIPASS_MIN_SEGMENT_DURATION: Minimum segment length to refine
#   Values: Float (seconds), default: 1.0
#   Note: Skip very short segments
MULTIPASS_ENABLED=false
MULTIPASS_CONFIDENCE_THRESHOLD=0.6
MULTIPASS_MAX_ITERATIONS=3
MULTIPASS_BEAM_SIZE_INCREMENT=5
MULTIPASS_MIN_SEGMENT_DURATION=1.0

# ------------------------------------------------------------
# Speaker-Aware Bias
# ------------------------------------------------------------
# SPEAKER_AWARE_BIAS_ENABLED: Enable speaker-specific bias term application
#   Values: true | false
#   Default: false (requires diarization)
#   Impact: 5-8% character name accuracy improvement
# SPEAKER_BIAS_BOOST_FACTOR: Weight boost for speaker-specific terms
#   Values: Float (1.0-3.0), default: 1.5
#   Note: 1.5 = 50% boost for terms associated with current speaker
# SPEAKER_CONTEXT_WINDOW: Context window for speaker changes
#   Values: Float (seconds), default: 10.0
#   Note: Apply speaker context within N seconds of speaker change
SPEAKER_AWARE_BIAS_ENABLED=false
SPEAKER_BIAS_BOOST_FACTOR=1.5
SPEAKER_CONTEXT_WINDOW=10.0

# ------------------------------------------------------------
# Music Detection & Handling
# ------------------------------------------------------------
# MUSIC_DETECTION_ENABLED: Pre-ASR music detection for optimized handling
#   Values: true | false
#   Default: true (recommended for movies with songs)
#   Impact: 60-80% reduction in music hallucinations, 20-30% faster
# MUSIC_CONFIDENCE_THRESHOLD: Classification threshold for music
#   Values: Float (0.0-1.0), default: 0.7
#   Note: Segments above this confidence classified as music
# MIN_MUSIC_DURATION: Minimum duration to classify as music
#   Values: Float (seconds), default: 10.0
#   Note: Short musical segments treated as dialogue
# SKIP_HIGH_CONFIDENCE_MUSIC: Skip transcription for high-confidence music
#   Values: true | false
#   Default: true (skip if confidence > 0.9)
# MUSIC_TRANSCRIPTION_TEMPERATURE: Temperature for singing/lyrics
#   Values: Float, default: 0.2
#   Note: Slightly higher for creative lyrics transcription
# MUSIC_TAG_IN_SUBTITLES: Add [MUSIC] tags for music segments
#   Values: true | false
#   Default: true (mark music in subtitle output)
MUSIC_DETECTION_ENABLED=true
MUSIC_CONFIDENCE_THRESHOLD=0.7
MIN_MUSIC_DURATION=10.0
SKIP_HIGH_CONFIDENCE_MUSIC=true
MUSIC_TRANSCRIPTION_TEMPERATURE=0.2
MUSIC_TAG_IN_SUBTITLES=true

# ------------------------------------------------------------
# Quality Metrics
# ------------------------------------------------------------
# QUALITY_METRICS_ENABLED: Enable quality metrics collection
#   Values: true | false
#   Default: true (recommended for tracking improvements)
#   Impact: Minimal performance overhead (<5%)
# GENERATE_QUALITY_REPORT: Generate quality report after pipeline
#   Values: true | false
#   Default: true (creates quality_report.json)
# TRACK_PERFORMANCE_METRICS: Track timing and resource usage
#   Values: true | false
#   Default: true (creates performance_report.json)
# QUALITY_REPORT_FORMAT: Output format for reports
#   Values: json | text | both
#   Default: json (structured data)
QUALITY_METRICS_ENABLED=true
GENERATE_QUALITY_REPORT=true
TRACK_PERFORMANCE_METRICS=true
QUALITY_REPORT_FORMAT=json

# ============================================================================
# END OF CONFIGURATION
# ============================================================================

