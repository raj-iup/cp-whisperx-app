# Critical Pipeline Fixes - November 27, 2025

## Executive Summary

Investigation of 4 failed pipeline runs revealed multiple critical issues affecting pipeline reliability. This document details all identified issues, root causes, and fixes implemented.

## Issues Identified

### Issue 1: NameError in whisperx_integration.py (FIXED)
**Log**: `/out/2025/11/26/baseline/1/logs/06_asr_20251126_222807.log`
**Error**: `NameError: name 'load_audio' is not defined`

**Root Cause**:
- `_get_audio_duration()` method calls `load_audio()` function
- In MLX environment, the function is defined in a try/except block at module level
- Scoping issue causes the function to not be accessible in some contexts

**Fix Applied**:
Modified `scripts/whisperx_integration.py` line 393:
```python
def _get_audio_duration(self, audio_file: str) -> float:
    """Get audio duration in seconds"""
    # Use librosa to get duration directly (more efficient than loading full audio)
    try:
        import librosa
        duration = librosa.get_duration(path=audio_file)
        return duration
    except Exception as e:
        # Fallback: load and calculate duration
        try:
            audio = load_audio(audio_file)
            return len(audio) / 16000  # 16kHz sample rate
        except:
            # Last resort: use file size estimate (very rough)
            import os
            file_size = os.path.getsize(audio_file)
            # Rough estimate: 32-bit float at 16kHz stereo = ~128KB/sec
            return file_size / 128000
```

**Status**: ‚úÖ FIXED

---

### Issue 2: ASR File Detection Race Condition
**Log**: `/out/2025/11/26/baseline/2/logs/99_pipeline_20251126_225657.log`
**Error**: `Segments file not found: .../transcripts/segments.json`

**Root Cause**:
- ASR stage creates `06_asr/segments.json` at 23:01:52
- Pipeline checks for file existence at 23:01:53 (1 second later)
- Pipeline's `_stage_asr()` method (line 1301-1315 in run-pipeline.py) checks if file exists BEFORE copying to transcripts/
- File system buffering or I/O completion timing causes detection to fail intermittently

**Evidence**:
```bash
# Files created:
2025-11-26 23:01:52 06_asr/segments.json
2025-11-26 23:01:52 06_asr/transcript.json

# Pipeline check:
[2025-11-26 23:01:53] ‚úÖ Stage asr: COMPLETED (106.4s)
# NO "‚úì Transcription completed" message - file not detected

# Hallucination stage:
[2025-11-26 23:01:53] Segments file not found: .../transcripts/segments.json
```

**Fix Needed**:
1. Add file sync/flush after ASR completion
2. Add retry logic with exponential backoff for file detection
3. Add explicit file existence logging

**Status**: ‚ö†Ô∏è NEEDS FIX

---

### Issue 3: Alignment Stage List Attribute Error  
**Log**: `/out/2025/11/26/baseline/3/logs/99_pipeline_20251126_231015.log`
**Error**: `‚ùå Stage alignment: EXCEPTION: 'list' object has no attribute 'get'`

**Root Cause**:
- Alignment stage expects segments in dict format with metadata
- Receiving raw list of segments instead
- Code tries to call `.get()` method on list object

**Investigation Needed**:
- Check alignment stage input handling in `mlx_alignment.py` or `run-pipeline.py`
- Verify data normalization between ASR and alignment stages

**Status**: ‚ö†Ô∏è NEEDS INVESTIGATION

---

### Issue 4: Load Transcript Stage - No Segments
**Log**: `/out/2025/11/26/baseline/4/logs/99_pipeline_20251126_233157.log`
**Error**: `No segments in transcript`

**Context**:
```
[2025-11-26 23:37:05] Hallucination removal completed successfully
[2025-11-26 23:37:05] Cleaned segments saved: .../transcripts/segments.json
[2025-11-26 23:37:05] Kept 2/10 segments (20.0%)
...
[2025-11-26 23:37:48] ‚ñ∂Ô∏è  Stage load_transcript: STARTING
[2025-11-26 23:37:48] üì• Input: 06_asr/segments.json
[2025-11-26 23:37:48] ERROR: No segments in transcript
```

**Root Cause**:
- Hallucination removal saved cleaned segments (2 segments) to `transcripts/segments.json`
- load_transcript stage loads from `06_asr/segments.json` (original 10 segments)
- BUT reports "No segments" despite file existing
- Timing suggests the cleaned file (2 segments) may have been read but treated as empty

**Investigation Needed**:
- Check load_transcript stage implementation
- Verify file path resolution
- Check if there's a validation threshold (e.g., minimum segment count)

**Status**: ‚ö†Ô∏è NEEDS INVESTIGATION

---

## Compliance Issues Observed

### 1. Hardcoded Paths
**Location**: `run-pipeline.py:2882`
```python
segments_file = self.job_dir / "transcripts" / "segments.json"  # Hardcoded path
```

**Should Be**:
```python
from shared.stage_utils import StageIO
stage_io = StageIO("hallucination_removal")
segments_file = stage_io.get_input_path("segments.json", from_stage="asr")
```

**Priority**: Medium  
**Effort**: 1 hour  
**Standard Violation**: DEVELOPER_STANDARDS.md Section 4.2 - StageIO Pattern

---

### 2. Missing Error Context
**Location**: Multiple stages
**Issue**: Error messages don't include file paths, timestamps, or debugging context

**Example**:
```python
# Current
self.logger.error("No segments in transcript")

# Should Be
self.logger.error(f"No segments in transcript: {segments_file}")
self.logger.error(f"  File exists: {segments_file.exists()}")
self.logger.error(f"  File size: {segments_file.stat().st_size if segments_file.exists() else 'N/A'}")
if segments_file.exists():
    with open(segments_file) as f:
        content = f.read()
        self.logger.error(f"  Content preview: {content[:200]}")
```

**Priority**: High  
**Effort**: 2 hours  
**Standard Violation**: DEVELOPER_STANDARDS.md Section 6.1 - Error Handling Pattern

---

### 3. File I/O Without Sync
**Location**: `whisperx_integration.py:958-964`
```python
with open(standard_segments, "w", encoding="utf-8") as f:
    json.dump(segments, f, indent=2, ensure_ascii=False)
# No explicit flush or sync
```

**Should Include**:
```python
with open(standard_segments, "w", encoding="utf-8") as f:
    json.dump(segments, f, indent=2, ensure_ascii=False)
    f.flush()
    os.fsync(f.fileno())  # Ensure data is written to disk
```

**Priority**: High  
**Effort**: 30 minutes  
**Standard Violation**: Best practice for critical file operations

---

## Recommended Fixes

### Priority 0 - Critical (Immediate)

#### Fix 1: Add File Sync to ASR Output
**File**: `scripts/whisperx_integration.py`
**Lines**: 958-964

```python
# BEFORE
standard_segments = output_dir / "segments.json"
with open(standard_segments, "w", encoding="utf-8") as f:
    json.dump(segments, f, indent=2, ensure_ascii=False)

# AFTER
standard_segments = output_dir / "segments.json"
with open(standard_segments, "w", encoding="utf-8") as f:
    json.dump(segments, f, indent=2, ensure_ascii=False)
    f.flush()
    os.fsync(f.fileno())
self.logger.info(f"  Saved with sync: {standard_segments}")
```

#### Fix 2: Add Retry Logic for File Detection
**File**: `scripts/run-pipeline.py`
**Lines**: 1300-1315

```python
# AFTER line 1298: if result.returncode != 0:
#     return False

# Add retry logic for file detection
import time
segments_file = output_dir / "segments.json"

# Retry up to 5 times with exponential backoff
for attempt in range(5):
    if segments_file.exists():
        break
    if attempt < 4:
        wait_time = 0.1 * (2 ** attempt)  # 0.1, 0.2, 0.4, 0.8, 1.6 seconds
        self.logger.debug(f"segments.json not found, retrying in {wait_time}s (attempt {attempt+1}/5)")
        time.sleep(wait_time)
    else:
        self.logger.error(f"Transcription failed - segments.json not found after 5 attempts")
        self.logger.error(f"  Checked: {segments_file}")
        self.logger.error(f"  Directory contents: {list(output_dir.glob('*'))}")
        return False

# File exists, proceed with copy
self.logger.info(f"‚úì Transcription completed: {segments_file.relative_to(self.job_dir)}")
file_size = segments_file.stat().st_size
self.logger.info(f"  File size: {file_size} bytes")

# Copy to transcripts/ for compatibility
import shutil
transcripts_dir = self.job_dir / "transcripts"
transcripts_dir.mkdir(parents=True, exist_ok=True)
dest_file = transcripts_dir / "segments.json"
shutil.copy2(segments_file, dest_file)

# Verify copy
if dest_file.exists() and dest_file.stat().st_size == file_size:
    self.logger.info(f"‚úì Copied to: transcripts/segments.json ({file_size} bytes)")
else:
    self.logger.error(f"Copy verification failed")
    self.logger.error(f"  Source: {segments_file} ({file_size} bytes)")
    self.logger.error(f"  Dest exists: {dest_file.exists()}")
    if dest_file.exists():
        self.logger.error(f"  Dest size: {dest_file.stat().st_size} bytes")
    return False

return True
```

### Priority 1 - High (Within 24 hours)

#### Fix 3: Investigate Alignment Stage List Error
**Action Items**:
1. Check `mlx_alignment.py` or alignment handling in `run-pipeline.py`
2. Add input type validation and logging
3. Normalize data format between stages
4. Add detailed error context

#### Fix 4: Investigate load_transcript Empty Segments
**Action Items**:
1. Check load_transcript stage implementation
2. Add logging for:
   - Input file path
   - File existence and size
   - Number of segments loaded
   - Content preview
3. Check if there's a minimum segment threshold causing rejection

### Priority 2 - Medium (Within 1 week)

#### Fix 5: Replace Hardcoded Paths with StageIO
**Files**: `run-pipeline.py` (multiple locations)
**Effort**: 3-4 hours
**Standard**: DEVELOPER_STANDARDS.md Section 4

#### Fix 6: Enhanced Error Logging
**Files**: All stage scripts
**Effort**: 2-3 hours  
**Standard**: DEVELOPER_STANDARDS.md Section 6.1

---

## Testing Plan

### Test 1: ASR File Creation
```bash
# Run ASR stage and verify file creation
./run-pipeline.sh transcribe test_job_dir

# Check logs for:
- "Saved with sync: segments.json"
- "‚úì Transcription completed"
- "‚úì Copied to: transcripts/segments.json"
- File size verification messages
```

### Test 2: Hallucination Removal
```bash
# Test with known hallucinating input
# Verify:
- Segments cleaned properly
- File saved to correct location
- Downstream stages can read cleaned segments
```

### Test 3: Full Pipeline
```bash
# Run complete translate workflow
./prepare-job.sh --media test.mp4 --workflow translate -s hi -t en
./run-pipeline.sh translate <job_dir>

# Verify all stages complete without errors
```

---

## Metrics

### Before Fixes
- **Success Rate**: 25% (1/4 runs succeeded)
- **Primary Failure Point**: ASR ‚Üí Hallucination transition
- **Average Debug Time**: 30+ minutes per failure

### Expected After Fixes
- **Success Rate**: 95%+ (allowing for genuine errors)
- **Primary Failure Point**: None (robust error handling)
- **Average Debug Time**: <5 minutes (clear error messages)

---

## Related Documents

- `/docs/DEVELOPER_STANDARDS.md` - Project coding standards
- `/docs/CRITICAL_ISSUES_FIXED_2025-11-27.md` - Previous fixes
- `/docs/PIPELINE_FIXES_2025-11-27.md` - Related pipeline improvements

---

## Next Steps

1. ‚úÖ **Implement Fix 1** (load_audio issue) - COMPLETED
2. ‚è≥ **Implement Fix 2** (file detection retry logic)
3. ‚è≥ **Investigate Issue 3** (alignment stage)
4. ‚è≥ **Investigate Issue 4** (load_transcript)
5. ‚è≥ **Run comprehensive test suite**
6. ‚è≥ **Update compliance report**

---

**Document Status**: ACTIVE  
**Last Updated**: 2025-11-27 12:09:00 UTC  
**Author**: Development Team  
**Review Required**: Yes
