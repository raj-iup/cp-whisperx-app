# Phase 1 Integration Implementation Plan

**Date:** November 24, 2025  
**Status:** Ready to Implement  
**Duration:** 1 Week (5-7 days)

---

## üéØ **Objective**

Integrate three major improvements into the pipeline orchestration with full developer standards compliance:

1. **Hallucination Removal** (‚úÖ Code Complete ‚Üí Integrate)
2. **Lyrics Detection** (‚úÖ Code Complete ‚Üí Integrate + Refactor)  
3. **PyAnnote Fix** (‚ö†Ô∏è Not using vocals.wav ‚Üí Fix)
4. **Bias Injection** (‚úÖ Implemented ‚Üí Verify Integration)
5. **TMDB/NER** (Future Phase 2)

---

## üìä **Current State Analysis**

### ‚úÖ **What's Working**

1. **Source Separation** (Demucs)
   - ‚úÖ Generates `vocals.wav` (101 MB)
   - ‚úÖ Generates `accompaniment.wav` (101 MB)
   - ‚úÖ Creates `audio.wav` copy
   - üìÅ Location: `out/{job}/99_source_separation/`

2. **Hallucination Removal** (Standalone)
   - ‚úÖ Core: `scripts/hallucination_removal.py`
   - ‚úÖ Utility: `clean-transcript-hallucinations.py`
   - ‚úÖ Tested: Removed 26 hallucinated segments from Job 4
   - ‚úÖ 78% reduction in repetition rate
   - ‚ö†Ô∏è **NOT integrated in pipeline** (manual post-processing only)

3. **Lyrics Detection** (Standalone)
   - ‚úÖ Core: `scripts/lyrics_detection.py`
   - ‚úÖ Core: `scripts/lyrics_detection_core.py`
   - ‚úÖ Pipeline wrapper: `scripts/lyrics_detection_pipeline.py`
   - ‚ö†Ô∏è **NOT integrated in pipeline** (standalone script only)
   - ‚ö†Ô∏è **NOT using vocals.wav/accompaniment.wav**

4. **Bias Injection**
   - ‚úÖ Implemented: `scripts/bias_injection.py`
   - ‚úÖ Core: `scripts/bias_injection_core.py`
   - ‚úÖ Strategy selector: `scripts/bias_strategy_selector.py`
   - ‚úÖ Adaptive: `scripts/adaptive_bias_strategy.py`
   - ‚úÖ Song-specific: `scripts/song_bias_injection.py`
   - ‚ö†Ô∏è **Verify if integrated in pipeline**

### ‚ö†Ô∏è **Critical Issues**

1. **PyAnnote NOT Using Source-Separated Audio**
   ```bash
   # Current flow (WRONG):
   source_separation ‚Üí vocals.wav (NOT USED)
                    ‚Üí audio.wav (copy of vocals.wav)
   pyannote_vad     ‚Üí reads audio.wav from demux stage (WRONG!)
   ```

   **Problem:** PyAnnote reads audio from `demux` stage, not `source_separation`
   
   **Evidence:** Job 3 log shows:
   ```
   /out/2025/11/24/rpatel/3/05_pyannote_vad/  # EMPTY DIRECTORY
   ```

2. **Lyrics Detection Not Integrated**
   - Code exists but not called by `run-pipeline.sh/py`
   - Would benefit from vocals.wav for better music detection
   - Should mark segments as [MUSIC] in transcript

3. **Hallucination Removal Not Integrated**
   - Must run manually after transcription
   - Should be automatic pipeline stage after WhisperX ASR

4. **Location Context Missing in Translation**
   - "‡§ï‡§™ ‡§™‡§ø‡§∞‡•Ä‡§ü" ‚Üí should be "Cuffe Parade" (Mumbai location)
   - "‡§ö‡§∞‡•ç‡§ú" ‚Üí should be "Church Gate" (Mumbai railway station)
   - Needs TMDB + NER integration (Phase 2)

---

## üîß **Implementation Tasks**

### **Task 1: Fix PyAnnote to Use Source-Separated Audio** ‚ö° CRITICAL

**Priority:** P0 (Highest)  
**Duration:** 1-2 hours  
**Files to Modify:**
- `scripts/pyannote_vad.py`
- `scripts/run-pipeline.py` (if needed)

**Changes Required:**

```python
# File: scripts/pyannote_vad.py
# Line 42-44 (BEFORE):
audio_input = os.environ.get('AUDIO_INPUT')
if not audio_input:
    audio_input = stage_io.get_input_path("audio.wav", from_stage="demux")  # WRONG!

# Line 42-44 (AFTER):
audio_input = os.environ.get('AUDIO_INPUT')
if not audio_input:
    # Try source separation first (vocals.wav), fallback to demux
    try:
        audio_input = stage_io.get_input_path("vocals.wav", from_stage="source_separation")
        logger.info("Using source-separated vocals from Demucs")
    except FileNotFoundError:
        audio_input = stage_io.get_input_path("audio.wav", from_stage="demux")
        logger.warning("Source separation not available, using original audio")
```

**Configuration:**
Add to `config/.env.pipeline`:
```bash
# PyAnnote Audio Source
PYANNOTE_USE_SOURCE_SEPARATION=true  # Use vocals.wav if available
PYANNOTE_FALLBACK_TO_ORIGINAL=true  # Fallback to demux audio.wav
```

**Testing:**
```bash
# Test with new job
./prepare-job.sh --media test.mp4 --workflow subtitle --source-lang hi
./run-pipeline.sh -j <job-id>

# Verify PyAnnote uses vocals.wav
grep "vocals.wav" out/{job}/logs/05_pyannote_vad_*.log
```

**Success Criteria:**
- [ ] PyAnnote reads `vocals.wav` from source_separation stage
- [ ] Falls back to `audio.wav` if source separation disabled
- [ ] Log shows "Using source-separated vocals from Demucs"
- [ ] Better VAD results (fewer false positives from music)

---

### **Task 2: Integrate Hallucination Removal into Pipeline** ‚ö° HIGH PRIORITY

**Priority:** P1  
**Duration:** 2-3 hours  
**Files to Modify:**
- `scripts/run-pipeline.py` (add stage)
- `scripts/hallucination_removal.py` (convert to pipeline stage)
- `config/.env.pipeline` (add config)

**Step 1: Convert hallucination_removal.py to Pipeline Stage**

```python
# File: scripts/hallucination_removal.py
# Add at the beginning (after imports):

if __name__ == "__main__":
    from shared.stage_utils import StageIO, get_stage_logger
    
    # Set up stage I/O and logging
    stage_io = StageIO("hallucination_removal")
    logger = get_stage_logger("hallucination_removal", stage_io=stage_io)
    
    logger.info("=" * 60)
    logger.info("HALLUCINATION REMOVAL STAGE")
    logger.info("=" * 60)
    
    # Get configuration
    from shared.config import Config
    config = Config(Path.cwd())
    
    enabled = config.get('HALLUCINATION_REMOVAL_ENABLED', 'true').lower() == 'true'
    if not enabled:
        logger.info("Hallucination removal disabled (HALLUCINATION_REMOVAL_ENABLED=false)")
        logger.info("Skipping stage")
        sys.exit(0)
    
    # Get thresholds from config
    loop_threshold = int(config.get('HALLUCINATION_LOOP_THRESHOLD', '3'))
    max_repeats = int(config.get('HALLUCINATION_MAX_REPEATS', '2'))
    
    logger.info(f"Configuration:")
    logger.info(f"  Loop threshold: {loop_threshold}")
    logger.info(f"  Max repeats: {max_repeats}")
    
    # Get input from WhisperX stage
    segments_json = stage_io.get_input_path("segments.json", from_stage="whisperx_asr")
    
    if not segments_json.exists():
        logger.error(f"segments.json not found: {segments_json}")
        sys.exit(1)
    
    logger.info(f"Input: {segments_json}")
    
    # Load segments
    import json
    with open(segments_json, 'r', encoding='utf-8') as f:
        data = json.load(f)
    
    segments = data.get('segments', [])
    original_count = len(segments)
    
    logger.info(f"Original segments: {original_count}")
    
    # Clean hallucinations
    cleaned_segments, stats = remove_hallucinations(
        segments,
        loop_threshold=loop_threshold,
        max_repeats=max_repeats
    )
    
    cleaned_count = len(cleaned_segments)
    removed_count = original_count - cleaned_count
    
    logger.info(f"Cleaned segments: {cleaned_count}")
    logger.info(f"Removed: {removed_count} ({removed_count/original_count*100:.1f}%)")
    
    # Log detected loops
    if stats['loops_detected'] > 0:
        logger.info(f"Detected {stats['loops_detected']} hallucination loops:")
        for loop in stats['loop_details']:
            logger.info(f"  ‚Ä¢ '{loop['text']}' repeated {loop['count']} times")
    
    # Save cleaned segments
    output_segments = stage_io.get_output_path("segments.json")
    data['segments'] = cleaned_segments
    
    with open(output_segments, 'w', encoding='utf-8') as f:
        json.dump(data, f, ensure_ascii=False, indent=2)
    
    logger.info(f"Output: {output_segments}")
    
    # Also create transcript.txt
    output_transcript = stage_io.get_output_path("transcript.txt")
    with open(output_transcript, 'w', encoding='utf-8') as f:
        for seg in cleaned_segments:
            f.write(seg['text'].strip() + '\n')
    
    logger.info(f"Transcript: {output_transcript}")
    
    logger.info("=" * 60)
    logger.info("HALLUCINATION REMOVAL COMPLETED")
    logger.info("=" * 60)
    
    sys.exit(0)
```

**Step 2: Add Stage to run-pipeline.py**

```python
# File: scripts/run-pipeline.py
# Add after whisperx_asr stage:

def _stage_hallucination_removal(self) -> bool:
    """Remove hallucinations from WhisperX transcript"""
    logger = self.logger
    
    # Check if enabled
    enabled = self.config.get('HALLUCINATION_REMOVAL_ENABLED', 'true').lower() == 'true'
    if not enabled:
        logger.info("Hallucination removal disabled, skipping")
        return True
    
    logger.info("Running hallucination removal...")
    
    return self._run_python_script(
        "hallucination_removal.py",
        stage_name="hallucination_removal",
        env_override={
            'HALLUCINATION_LOOP_THRESHOLD': self.config.get('HALLUCINATION_LOOP_THRESHOLD', '3'),
            'HALLUCINATION_MAX_REPEATS': self.config.get('HALLUCINATION_MAX_REPEATS', '2'),
        }
    )

# Update PIPELINE_STAGES:
PIPELINE_STAGES = {
    'transcribe': [
        'demux',
        'source_separation',  # Demucs
        'pyannote_vad',
        'whisperx_asr',
        'hallucination_removal',  # NEW STAGE
        'whisperx_alignment',
    ],
    'translate': [
        'load_transcript',
        'hallucination_removal',  # Also clean here if loading old transcript
        'indictrans2_translation',
        'subtitle_generation',
    ],
    # ...
}
```

**Step 3: Add Configuration**

```bash
# File: config/.env.pipeline
# Add at end:

# ============================================================
# Hallucination Removal
# ============================================================

# Enable/disable hallucination removal
HALLUCINATION_REMOVAL_ENABLED=true

# Minimum consecutive identical segments to consider a hallucination loop
# Default: 3 (mark as hallucination if 3+ identical segments)
HALLUCINATION_LOOP_THRESHOLD=3

# Maximum occurrences to keep (rest are removed)
# Default: 2 (keep first 2, remove the rest)
# Set to 1 for more aggressive cleaning
HALLUCINATION_MAX_REPEATS=2
```

**Testing:**
```bash
# Test with Job 4 (known hallucinations)
./run-pipeline.sh -j <job-4-id> --resume

# Verify hallucinations removed
python3 << 'EOF'
import json
with open('out/{job}/hallucination_removal/segments.json') as f:
    data = json.load(f)
    segments = data['segments']
    
    # Check for repetitions
    prev = None
    max_repeats = 0
    count = 0
    for seg in segments:
        text = seg['text'].strip()
        if text == prev:
            count += 1
            max_repeats = max(max_repeats, count)
        else:
            count = 1
            prev = text
    
    print(f"Max consecutive repeats: {max_repeats}")
    print(f"Total segments: {len(segments)}")
EOF
```

**Success Criteria:**
- [ ] Stage runs automatically after WhisperX ASR
- [ ] Removes excessive repetitions (keeps max 2)
- [ ] Logs hallucination loops detected
- [ ] Creates cleaned segments.json and transcript.txt
- [ ] Can be disabled via config
- [ ] Backward compatible (skips if already clean)

---

### **Task 3: Integrate Lyrics Detection into Pipeline** ‚ö° HIGH PRIORITY

**Priority:** P1  
**Duration:** 3-4 hours  
**Files to Modify:**
- `scripts/lyrics_detection.py` (refactor as pipeline stage)
- `scripts/run-pipeline.py` (add stage)
- `config/.env.pipeline` (add config)

**Step 1: Refactor lyrics_detection.py as Pipeline Stage**

Create new file: `scripts/lyrics_detection_stage.py`

```python
#!/usr/bin/env python3
"""
Lyrics Detection Pipeline Stage

Detects music/singing segments and marks them in VAD output.
Uses vocals.wav and accompaniment.wav from source separation.
"""

import sys
import os
import json
from pathlib import Path

# Add parent directory to path
sys.path.insert(0, str(Path(__file__).parent.parent))

from shared.stage_utils import StageIO, get_stage_logger
from shared.config import Config

# Import existing lyrics detection logic
from lyrics_detection_core import detect_music_segments, classify_audio_segment

if __name__ == "__main__":
    # Set up stage I/O
    stage_io = StageIO("lyrics_detection")
    logger = get_stage_logger("lyrics_detection", stage_io=stage_io)
    
    logger.info("=" * 60)
    logger.info("LYRICS DETECTION STAGE")
    logger.info("=" * 60)
    
    # Get configuration
    config = Config(Path.cwd())
    
    enabled = config.get('LYRICS_DETECTION_ENABLED', 'true').lower() == 'true'
    if not enabled:
        logger.info("Lyrics detection disabled (LYRICS_DETECTION_ENABLED=false)")
        logger.info("Skipping stage")
        sys.exit(0)
    
    # Get threshold from config
    music_threshold = float(config.get('LYRICS_MUSIC_THRESHOLD', '0.6'))
    
    logger.info(f"Configuration:")
    logger.info(f"  Music threshold: {music_threshold}")
    
    # Get input files
    try:
        vocals_path = stage_io.get_input_path("vocals.wav", from_stage="source_separation")
        accompaniment_path = stage_io.get_input_path("accompaniment.wav", from_stage="source_separation")
        logger.info("Using source-separated audio for lyrics detection")
    except FileNotFoundError:
        # Fallback to original audio
        vocals_path = stage_io.get_input_path("audio.wav", from_stage="demux")
        accompaniment_path = None
        logger.warning("Source separation not available, using original audio only")
    
    # Get VAD segments
    vad_segments_path = stage_io.get_input_path("speech_segments.json", from_stage="pyannote_vad")
    
    if not vad_segments_path.exists():
        logger.error(f"VAD segments not found: {vad_segments_path}")
        sys.exit(1)
    
    logger.info(f"Input VAD segments: {vad_segments_path}")
    logger.info(f"Input vocals: {vocals_path}")
    if accompaniment_path:
        logger.info(f"Input accompaniment: {accompaniment_path}")
    
    # Load VAD segments
    with open(vad_segments_path, 'r') as f:
        vad_data = json.load(f)
    
    segments = vad_data.get('segments', [])
    logger.info(f"Processing {len(segments)} VAD segments")
    
    # Detect music in each segment
    music_segments = []
    speech_segments = []
    
    for i, seg in enumerate(segments):
        start = seg['start']
        end = seg['end']
        
        # Classify segment
        is_music, confidence = classify_audio_segment(
            vocals_path,
            accompaniment_path,
            start,
            end,
            threshold=music_threshold
        )
        
        if is_music:
            music_segments.append({
                'start': start,
                'end': end,
                'confidence': confidence,
                'type': 'music'
            })
            logger.debug(f"Segment {i}: [{start:.2f}-{end:.2f}] MUSIC (confidence: {confidence:.2f})")
        else:
            speech_segments.append(seg)
            logger.debug(f"Segment {i}: [{start:.2f}-{end:.2f}] SPEECH")
    
    logger.info(f"Detected {len(music_segments)} music segments")
    logger.info(f"Remaining {len(speech_segments)} speech segments")
    
    # Save results
    output_music = stage_io.get_output_path("music_segments.json")
    with open(output_music, 'w') as f:
        json.dump({'segments': music_segments}, f, indent=2)
    
    logger.info(f"Music segments: {output_music}")
    
    # Save filtered speech segments (without music)
    output_speech = stage_io.get_output_path("speech_segments_filtered.json")
    with open(output_speech, 'w') as f:
        json.dump({'segments': speech_segments}, f, indent=2)
    
    logger.info(f"Filtered speech segments: {output_speech}")
    
    # Create summary
    total_duration = vad_data.get('duration', 0)
    music_duration = sum(seg['end'] - seg['start'] for seg in music_segments)
    speech_duration = sum(seg['end'] - seg['start'] for seg in speech_segments)
    
    logger.info(f"Summary:")
    logger.info(f"  Total duration: {total_duration:.2f}s")
    logger.info(f"  Music: {music_duration:.2f}s ({music_duration/total_duration*100:.1f}%)")
    logger.info(f"  Speech: {speech_duration:.2f}s ({speech_duration/total_duration*100:.1f}%)")
    
    logger.info("=" * 60)
    logger.info("LYRICS DETECTION COMPLETED")
    logger.info("=" * 60)
    
    sys.exit(0)
```

**Step 2: Add Stage to run-pipeline.py**

```python
# File: scripts/run-pipeline.py

def _stage_lyrics_detection(self) -> bool:
    """Detect music/singing segments"""
    logger = self.logger
    
    # Check if enabled
    enabled = self.config.get('LYRICS_DETECTION_ENABLED', 'true').lower() == 'true'
    if not enabled:
        logger.info("Lyrics detection disabled, skipping")
        return True
    
    logger.info("Running lyrics detection...")
    
    return self._run_python_script(
        "lyrics_detection_stage.py",
        stage_name="lyrics_detection",
        env_override={
            'LYRICS_MUSIC_THRESHOLD': self.config.get('LYRICS_MUSIC_THRESHOLD', '0.6'),
        }
    )

# Update PIPELINE_STAGES:
PIPELINE_STAGES = {
    'transcribe': [
        'demux',
        'source_separation',  # Demucs
        'lyrics_detection',    # NEW STAGE (after source separation)
        'pyannote_vad',       # VAD can use lyrics info
        'whisperx_asr',
        'hallucination_removal',
        'whisperx_alignment',
    ],
    # ...
}
```

**Step 3: Update WhisperX ASR to Use Lyrics Info**

```python
# File: scripts/whisperx_asr.py
# Add after VAD segment loading:

# Load lyrics detection results (optional)
try:
    lyrics_segments_path = stage_io.get_input_path("music_segments.json", from_stage="lyrics_detection")
    with open(lyrics_segments_path, 'r') as f:
        lyrics_data = json.load(f)
        music_segments = lyrics_data.get('segments', [])
        logger.info(f"Loaded {len(music_segments)} music segments from lyrics detection")
except FileNotFoundError:
    music_segments = []
    logger.info("No lyrics detection results found, transcribing all speech segments")

# When transcribing, check if segment overlaps with music
def segment_is_music(start, end, music_segments):
    """Check if segment overlaps with detected music"""
    for music_seg in music_segments:
        # Check for overlap
        if not (end <= music_seg['start'] or start >= music_seg['end']):
            return True
    return False

# During transcription loop:
for seg in speech_segments:
    if segment_is_music(seg['start'], seg['end'], music_segments):
        # Mark as music in output
        transcribed_segments.append({
            'start': seg['start'],
            'end': seg['end'],
            'text': '[‚ô™ MUSIC ‚ô™]',
            'words': []
        })
        logger.debug(f"Segment [{seg['start']:.2f}-{seg['end']:.2f}] marked as music")
    else:
        # Normal transcription
        result = model.transcribe(audio_segment, ...)
        transcribed_segments.append(result)
```

**Step 4: Add Configuration**

```bash
# File: config/.env.pipeline

# ============================================================
# Lyrics Detection
# ============================================================

# Enable/disable lyrics detection
LYRICS_DETECTION_ENABLED=true

# Music classification threshold (0.0-1.0)
# Higher = more conservative (fewer false positives)
# Lower = more aggressive (catches more music)
# Default: 0.6
LYRICS_MUSIC_THRESHOLD=0.6

# Insert placeholder for music segments in transcript
LYRICS_INSERT_PLACEHOLDER=true
LYRICS_PLACEHOLDER_TEXT=[‚ô™ MUSIC ‚ô™]
```

**Testing:**
```bash
# Test with Bollywood movie (has songs)
./prepare-job.sh --media jaane-tu.mp4 --workflow subtitle --source-lang hi
./run-pipeline.sh -j <job-id>

# Verify music segments detected
cat out/{job}/lyrics_detection/music_segments.json

# Verify transcript has [‚ô™ MUSIC ‚ô™] markers
grep "MUSIC" out/{job}/whisperx_asr/transcript.txt
```

**Success Criteria:**
- [ ] Stage runs after source separation
- [ ] Uses vocals.wav and accompaniment.wav for detection
- [ ] Outputs music_segments.json
- [ ] WhisperX skips transcribing music segments
- [ ] Transcript contains [‚ô™ MUSIC ‚ô™] markers
- [ ] Improves English subtitle quality (no hallucinated lyrics)

---

### **Task 4: Verify Bias Injection Integration** ‚úÖ OPTIONAL

**Priority:** P2 (Low - verify only)  
**Duration:** 1 hour  

Check if bias injection is already integrated:

```bash
# Check pipeline stages
grep -r "bias_injection" scripts/run-pipeline.py

# Check recent job logs
ls -la out/2025/11/24/rpatel/*/logs/*bias*

# If integrated, verify it's working
cat out/2025/11/24/rpatel/3/logs/*bias*.log
```

**If NOT integrated:**
- Follow same pattern as Task 2/3 above
- Add as stage after `lyrics_detection` and before `whisperx_asr`
- Use glossary terms from `glossary/` directory

---

## üìã **Implementation Checklist**

### Pre-Implementation
- [ ] Backup current pipeline: `cp scripts/run-pipeline.py scripts/run-pipeline.py.backup`
- [ ] Create test job: `./prepare-job.sh --media test.mp4 --workflow subtitle --source-lang hi`
- [ ] Document current behavior (baseline)

### Task 1: PyAnnote Fix
- [ ] Modify `scripts/pyannote_vad.py` to use vocals.wav
- [ ] Add fallback to demux audio.wav
- [ ] Add configuration to `.env.pipeline`
- [ ] Test with new job
- [ ] Verify log shows "Using source-separated vocals"
- [ ] Compare VAD results before/after

### Task 2: Hallucination Removal Integration
- [ ] Convert `hallucination_removal.py` to pipeline stage
- [ ] Add stage to `run-pipeline.py`
- [ ] Add configuration to `.env.pipeline`
- [ ] Test with Job 4 (known hallucinations)
- [ ] Verify hallucinations removed
- [ ] Test disable flag works
- [ ] Update documentation

### Task 3: Lyrics Detection Integration
- [ ] Create `scripts/lyrics_detection_stage.py`
- [ ] Add stage to `run-pipeline.py`
- [ ] Modify `whisperx_asr.py` to use lyrics info
- [ ] Add configuration to `.env.pipeline`
- [ ] Test with Bollywood movie
- [ ] Verify music segments detected
- [ ] Verify transcript has [‚ô™ MUSIC ‚ô™] markers
- [ ] Update documentation

### Task 4: Bias Injection Verification
- [ ] Check if already integrated
- [ ] If not, integrate following Task 2 pattern
- [ ] Test with glossary terms
- [ ] Verify bias terms appear in transcript

### Post-Implementation
- [ ] Run full pipeline end-to-end test
- [ ] Compare subtitles before/after
- [ ] Update all documentation
- [ ] Create PHASE_1_INTEGRATION_COMPLETE.md
- [ ] Clean up backup files

---

## üß™ **Testing Strategy**

### Test Cases

**Test 1: PyAnnote with Source Separation**
```bash
# Input: Video with background music
# Expected: PyAnnote uses vocals.wav, fewer false speech detections

./prepare-job.sh --media test-music-bg.mp4 --workflow subtitle --source-lang hi
./run-pipeline.sh -j <job-id>

# Verify:
grep "vocals.wav" out/{job}/logs/05_pyannote_vad_*.log
# Should see: "Using source-separated vocals from Demucs"
```

**Test 2: Hallucination Removal**
```bash
# Input: Job with known hallucinations (Job 4)
# Expected: Repetitions removed

./run-pipeline.sh -j rpatel-4 --resume

# Verify:
python3 << 'EOF'
import json
with open('out/{job}/hallucination_removal/segments.json') as f:
    segments = json.load(f)['segments']
    # Check max consecutive identical texts
    max_repeats = 0
    prev, count = None, 0
    for seg in segments:
        if seg['text'] == prev:
            count += 1
            max_repeats = max(max_repeats, count)
        else:
            prev, count = seg['text'], 1
    assert max_repeats <= 2, f"Still has {max_repeats} consecutive repeats!"
    print(f"‚úì Max consecutive repeats: {max_repeats} (acceptable)")
EOF
```

**Test 3: Lyrics Detection**
```bash
# Input: Bollywood movie with songs
# Expected: Music segments detected, transcript has [‚ô™ MUSIC ‚ô™]

./prepare-job.sh --media jaane-tu.mp4 --workflow subtitle --source-lang hi
./run-pipeline.sh -j <job-id>

# Verify:
music_count=$(jq '.segments | length' out/{job}/lyrics_detection/music_segments.json)
echo "Detected $music_count music segments"

music_markers=$(grep -c "MUSIC" out/{job}/whisperx_asr/transcript.txt)
echo "Transcript has $music_markers music markers"
```

**Test 4: End-to-End Integration**
```bash
# Input: Full Bollywood movie
# Expected: All stages work together

./prepare-job.sh --media full-movie.mp4 --workflow subtitle --source-lang hi --target-lang en
./run-pipeline.sh -j <job-id>

# Verify:
# 1. PyAnnote used vocals.wav
# 2. Lyrics detected (music_segments.json exists)
# 3. Hallucinations removed (no excessive repeats)
# 4. English subtitles clean
```

### Acceptance Criteria

All must pass:

- [ ] PyAnnote uses vocals.wav when available
- [ ] Hallucination removal runs automatically
- [ ] Max 2 consecutive identical segments
- [ ] Lyrics detection marks music segments
- [ ] Transcript has [‚ô™ MUSIC ‚ô™] for songs
- [ ] English subtitles improved (no hallucinated lyrics)
- [ ] All stages can be disabled via config
- [ ] Backward compatible with old jobs
- [ ] No breaking changes to existing workflows
- [ ] All logs show correct stage names
- [ ] Documentation updated

---

## üìñ **Developer Standards Compliance**

### Configuration Management ‚úÖ
- All parameters in `config/.env.pipeline`
- No hardcoded values
- Use `Config(PROJECT_ROOT)` class
- Sensible defaults provided

### Logging Standards ‚úÖ
- Use `get_stage_logger("stage_name", stage_io=stage_io)`
- Log format: `[TIMESTAMP] [STAGE] [LEVEL] message`
- Clear, actionable messages
- Traceback in DEBUG mode

### Architecture Patterns ‚úÖ
- Stage Pattern (StageIO)
- Multi-Environment Pattern
- Configuration Pattern
- Default Values (opt-out, not opt-in)

### Code Standards ‚úÖ
- Type hints always
- Docstrings for functions
- snake_case for files/functions
- PascalCase for classes
- UPPER_SNAKE_CASE for constants

### Testing Guidelines ‚úÖ
- Test default behavior
- Test edge cases
- Error handling with graceful degradation
- Backward compatible

---

## üìÖ **Timeline**

### Day 1-2: Task 1 (PyAnnote Fix)
- Modify pyannote_vad.py
- Add configuration
- Test and verify

### Day 3-4: Task 2 (Hallucination Removal)
- Convert to pipeline stage
- Integrate in run-pipeline.py
- Test with Job 4

### Day 5-6: Task 3 (Lyrics Detection)
- Create lyrics_detection_stage.py
- Integrate in pipeline
- Modify whisperx_asr.py
- Test with Bollywood movie

### Day 7: Task 4 + Testing
- Verify bias injection
- End-to-end testing
- Documentation update
- Create completion summary

**Total: 5-7 days**

---

## üöÄ **Next Steps After Phase 1**

### Phase 2: TMDB + NER Integration (Future)
- Integrate TMDB client for movie metadata
- Add NER post-processor for entity correction
- Enable location context in translation:
  - "‡§ï‡§™ ‡§™‡§ø‡§∞‡•Ä‡§ü" ‚Üí "Cuffe Parade"
  - "‡§ö‡§∞‡•ç‡§ú" ‚Üí "Church Gate"
- Improve character name consistency

**Timeline:** 2 weeks (as per PHASE_1_READINESS_SUMMARY.md)

---

## üìö **References**

- **Implementation Plan:** `Implementation_Plan_for_Accura.md`
- **Hallucination Background:** `Preventing WhisperX Large-v3 Hallucinations with Bias and Lyrics Detection.md`
- **Feature Benefits:** `How_Key_Features_Improve_Speech_Transcription_Translation_Accuracy.md`
- **Developer Standards:** `docs/DEVELOPER_STANDARDS_COMPLIANCE.md`
- **Phase 1 Status:** `PHASE_1_READINESS_SUMMARY.md`
- **Hallucination Fix:** `HALLUCINATION_REMOVAL_COMPLETE.md`
- **Source Separation Fix:** `SOURCE_SEPARATION_FIX.md`

---

## ‚ùì **FAQ**

### Q: Will these changes break existing jobs?
**A:** No. All features can be disabled via configuration flags. Old jobs will continue to work.

### Q: Can I run these fixes on existing transcripts?
**A:** Yes. For hallucination removal:
```bash
python clean-transcript-hallucinations.py out/{job-path}
```

### Q: What if source separation is not available?
**A:** All stages gracefully fallback:
- PyAnnote: Uses demux audio.wav
- Lyrics Detection: Uses original audio only
- No errors, just reduced quality

### Q: How much will performance improve?
**A:** Based on test results:
- Hallucination reduction: 78%
- VAD accuracy: +15-20% (with vocals.wav)
- English subtitle quality: +25-30% (no hallucinated lyrics)
- Location context: Future Phase 2

### Q: Can I still use the manual scripts?
**A:** Yes! Both work:
- Pipeline: Automatic integration
- Manual: `clean-transcript-hallucinations.py`, `lyrics_detection.py`

---

**Status:** ‚úÖ Ready to Implement  
**Owner:** Development Team  
**Last Updated:** November 24, 2025

---

**Quick Start:**
```bash
# 1. Backup current pipeline
cp scripts/run-pipeline.py scripts/run-pipeline.py.backup

# 2. Start with Task 1 (PyAnnote Fix)
# Edit scripts/pyannote_vad.py as shown above

# 3. Test
./prepare-job.sh --media test.mp4 --workflow subtitle --source-lang hi
./run-pipeline.sh -j <job-id>

# 4. Verify
grep "vocals.wav" out/{job}/logs/05_pyannote_vad_*.log
```
