# ============================================================================
# CP-WhisperX-App Docker Compose Configuration
# Phase 2 Enhancement: Native Execution Mode Support
# ============================================================================
#
# EXECUTION MODES:
# 
# 1. NATIVE MODE (Recommended - Default)
#    - PyTorch from .bollyenv (created by bootstrap)
#    - Slim Docker images (no PyTorch)
#    - 60% smaller images, 50% faster builds
#    - Set: EXECUTION_MODE=native
#    - Mount: .bollyenv volume
#
# 2. DOCKER MODE (Advanced - Distributed setups)
#    - PyTorch in Docker images
#    - Larger images (~8 GB extra)
#    - Set: EXECUTION_MODE=docker
#    - Rebuild images with full PyTorch
#
# PHASE 2 DEFAULT: Native mode (GPU stages use .bollyenv for PyTorch)
# ============================================================================

services:
  # CPU-Only Stages - use :cpu tag
  demux:
    image: "${DOCKERHUB_USER:-rajiup}/cp-whisperx-app-demux:cpu"
    container_name: cp_whisperx_demux
    volumes:
      - ./in:/app/in:ro
      - ./out:/app/out
      - ./config:/app/config:ro
      - ./shared:/app/shared:ro
    environment:
      - CONFIG_PATH=${CONFIG_PATH:-/app/config/.env}
      - PYTHONPATH=/app

  tmdb:
    image: "${DOCKERHUB_USER:-rajiup}/cp-whisperx-app-tmdb:cpu"
    container_name: cp_whisperx_tmdb
    volumes:
      - ./out:/app/out
      - ./config:/app/config:ro
      - ./shared:/app/shared:ro
    environment:
      - CONFIG_PATH=${CONFIG_PATH:-/app/config/.env}
      - PYTHONPATH=/app
    depends_on:
      - demux

  pre-ner:
    image: "${DOCKERHUB_USER:-rajiup}/cp-whisperx-app-pre-ner:cpu"
    container_name: cp_whisperx_pre_ner
    volumes:
      - ./out:/app/out
      - ./config:/app/config:ro
      - ./scripts:/app/scripts:ro
      - ./shared:/app/shared:ro
      - ./shared-model-and-cache:/shared-model-and-cache
    environment:
      - CONFIG_PATH=${CONFIG_PATH:-/app/config/.env}
      - PYTHONPATH=/app
      - SPACY_DATA=/shared-model-and-cache/spacy
    depends_on:
      - tmdb

  post-ner:
    image: "${DOCKERHUB_USER:-rajiup}/cp-whisperx-app-post-ner:cpu"
    container_name: cp_whisperx_post_ner
    volumes:
      - ./out:/app/out
      - ./config:/app/config:ro
      - ./scripts:/app/scripts:ro
      - ./shared:/app/shared:ro
      - ./shared-model-and-cache:/shared-model-and-cache
    environment:
      - CONFIG_PATH=${CONFIG_PATH:-/app/config/.env}
      - PYTHONPATH=/app
      - SPACY_DATA=/shared-model-and-cache/spacy
    depends_on:
      - asr

  subtitle-gen:
    image: "${DOCKERHUB_USER:-rajiup}/cp-whisperx-app-subtitle-gen:cpu"
    container_name: cp_whisperx_subtitle_gen
    volumes:
      - ./out:/app/out
      - ./config:/app/config:ro
      - ./shared:/app/shared:ro
    environment:
      - CONFIG_PATH=${CONFIG_PATH:-/app/config/.env}
      - PYTHONPATH=/app
    depends_on:
      - post-ner

  mux:
    image: "${DOCKERHUB_USER:-rajiup}/cp-whisperx-app-mux:cpu"
    container_name: cp_whisperx_mux
    volumes:
      - ./in:/app/in:ro
      - ./out:/app/out
      - ./config:/app/config:ro
      - ./shared:/app/shared:ro
    environment:
      - CONFIG_PATH=${CONFIG_PATH:-/app/config/.env}
      - PYTHONPATH=/app
    depends_on:
      - subtitle-gen

  # GPU Stages - use :cuda tag
  # Phase 2: GPU stages now support native PyTorch execution via .bollyenv
  silero-vad:
    image: "${DOCKERHUB_USER:-rajiup}/cp-whisperx-app-silero-vad:cuda"
    container_name: cp_whisperx_silero_vad
    volumes:
      - ./.bollyenv:/app/.bollyenv:ro  # Phase 2: Native PyTorch environment
      - ./out:/app/out
      - ./config:/app/config:ro
      - ./shared:/app/shared:ro
    environment:
      - EXECUTION_MODE=native  # Phase 2: Enable native execution
      - CONFIG_PATH=${CONFIG_PATH:-/app/config/.env}
      - PYTHONPATH=/app
      - PATH=/app/.bollyenv/bin:/usr/local/bin:/usr/bin:/bin  # Phase 2: Add .bollyenv to PATH
    depends_on:
      - pre-ner

  pyannote-vad:
    image: "${DOCKERHUB_USER:-rajiup}/cp-whisperx-app-pyannote-vad:cuda"
    container_name: cp_whisperx_pyannote_vad
    volumes:
      - ./.bollyenv:/app/.bollyenv:ro  # Phase 2: Native PyTorch environment
      - ./in:/app/in
      - ./out:/app/out
      - ./config:/app/config:ro
      - ./shared:/app/shared:ro
      - ./shared-model-and-cache:/shared-model-and-cache
    environment:
      - EXECUTION_MODE=native  # Phase 2: Enable native execution
      - CONFIG_PATH=${CONFIG_PATH:-/app/config/.env}
      - PYTHONPATH=/app
      - PATH=/app/.bollyenv/bin:/usr/local/bin:/usr/bin:/bin  # Phase 2: Add .bollyenv to PATH
      - HF_HOME=/shared-model-and-cache/huggingface
      - TRANSFORMERS_CACHE=/shared-model-and-cache/transformers
    depends_on:
      - silero-vad

  diarization:
    image: "${DOCKERHUB_USER:-rajiup}/cp-whisperx-app-diarization:cuda"
    container_name: cp_whisperx_diarization
    volumes:
      - ./.bollyenv:/app/.bollyenv:ro  # Phase 2: Native PyTorch environment
      - ./in:/app/in
      - ./out:/app/out
      - ./config:/app/config:ro
      - ./scripts:/app/scripts:ro
      - ./shared:/app/shared:ro
      - ./shared-model-and-cache:/shared-model-and-cache
    environment:
      - EXECUTION_MODE=native  # Phase 2: Enable native execution
      - CONFIG_PATH=${CONFIG_PATH:-/app/config/.env}
      - PYTHONPATH=/app
      - PATH=/app/.bollyenv/bin:/usr/local/bin:/usr/bin:/bin  # Phase 2: Add .bollyenv to PATH
      - HF_HOME=/shared-model-and-cache/huggingface
      - TRANSFORMERS_CACHE=/shared-model-and-cache/transformers
      - PYANNOTE_CACHE=/shared-model-and-cache/pyannote
    depends_on:
      - pyannote-vad

  asr:
    image: "${DOCKERHUB_USER:-rajiup}/cp-whisperx-app-asr:cuda"
    container_name: cp_whisperx_asr
    volumes:
      - ./.bollyenv:/app/.bollyenv:ro  # Phase 2: Native PyTorch environment
      - ./in:/app/in
      - ./out:/app/out
      - ./config:/app/config:ro
      - ./scripts:/app/scripts:ro
      - ./shared:/app/shared:ro
      - ./shared-model-and-cache:/shared-model-and-cache
    environment:
      - EXECUTION_MODE=native  # Phase 2: Enable native execution
      - CONFIG_PATH=${CONFIG_PATH:-/app/config/.env}
      - PYTHONPATH=/app
      - PATH=/app/.bollyenv/bin:/usr/local/bin:/usr/bin:/bin  # Phase 2: Add .bollyenv to PATH
      - HF_HOME=/shared-model-and-cache/huggingface
      - TRANSFORMERS_CACHE=/shared-model-and-cache/transformers
      - WHISPER_CACHE=/shared-model-and-cache/whisper
    depends_on:
      - diarization
      - TRANSFORMERS_CACHE=/shared-model-and-cache/transformers
      - WHISPER_CACHE=/shared-model-and-cache/whisper
    mem_limit: 16g
    memswap_limit: 16g
    depends_on:
      - diarization

# Usage notes:
# 1) Build all images:
#    scripts\build-all-images.bat (Windows)
#    ./scripts/build-all-images.sh (Linux/Mac)
# 
# 2) Run complete pipeline:
#    python pipeline.py -i in/movie.mp4
#
# 3) Run individual services:
#    docker compose run --rm demux in/movie.mp4
#    docker compose run --rm asr out/Movie_Name
#
# Image Tagging Strategy:
# - CPU-Only Stages (demux, tmdb, pre-ner, post-ner, subtitle-gen, mux): :cpu tag
# - GPU Stages (silero-vad, pyannote-vad, diarization, asr): :cuda tag
# - All CPU stages built from rajiup/cp-whisperx-app-base:cpu
# - All GPU stages built from rajiup/cp-whisperx-app-base:cuda
