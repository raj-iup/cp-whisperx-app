# Docker Compose with GPU fallback support
#
# GPU stages will try CUDA images first, then fallback to CPU if:
# - GPU not available
# - CUDA image not found
# - GPU execution fails
#
# Set GPU_AVAILABLE=false to force CPU execution

services:
  # CPU-Only Stages - use :cpu tag
  demux:
    image: "${DOCKERHUB_USER:-rajiup}/cp-whisperx-app-demux:cpu"
    container_name: cp_whisperx_demux
    volumes:
      - ./in:/app/in:ro
      - ./out:/app/out
      - ./config:/app/config:ro
      - ./shared:/app/shared:ro
    environment:
      - CONFIG_PATH=${CONFIG_PATH:-/app/config/.env}
      - PYTHONPATH=/app

  tmdb:
    image: "${DOCKERHUB_USER:-rajiup}/cp-whisperx-app-tmdb:cpu"
    container_name: cp_whisperx_tmdb
    volumes:
      - ./out:/app/out
      - ./config:/app/config:ro
      - ./shared:/app/shared:ro
    environment:
      - CONFIG_PATH=${CONFIG_PATH:-/app/config/.env}
      - PYTHONPATH=/app
    depends_on:
      - demux

  pre-ner:
    image: "${DOCKERHUB_USER:-rajiup}/cp-whisperx-app-pre-ner:cpu"
    container_name: cp_whisperx_pre_ner
    volumes:
      - ./out:/app/out
      - ./LLM:/app/LLM
      - ./config:/app/config:ro
      - ./scripts:/app/scripts:ro
      - ./shared:/app/shared:ro
    environment:
      - CONFIG_PATH=${CONFIG_PATH:-/app/config/.env}
      - PYTHONPATH=/app
    depends_on:
      - tmdb

  post-ner:
    image: "${DOCKERHUB_USER:-rajiup}/cp-whisperx-app-post-ner:cpu"
    container_name: cp_whisperx_post_ner
    volumes:
      - ./out:/app/out
      - ./config:/app/config:ro
      - ./scripts:/app/scripts:ro
      - ./shared:/app/shared:ro
    environment:
      - CONFIG_PATH=${CONFIG_PATH:-/app/config/.env}
      - PYTHONPATH=/app
    depends_on:
      - asr

  subtitle-gen:
    image: "${DOCKERHUB_USER:-rajiup}/cp-whisperx-app-subtitle-gen:cpu"
    container_name: cp_whisperx_subtitle_gen
    volumes:
      - ./out:/app/out
      - ./config:/app/config:ro
      - ./shared:/app/shared:ro
    environment:
      - CONFIG_PATH=${CONFIG_PATH:-/app/config/.env}
      - PYTHONPATH=/app
    depends_on:
      - post-ner

  mux:
    image: "${DOCKERHUB_USER:-rajiup}/cp-whisperx-app-mux:cpu"
    container_name: cp_whisperx_mux
    volumes:
      - ./in:/app/in:ro
      - ./out:/app/out
      - ./config:/app/config:ro
      - ./shared:/app/shared:ro
    environment:
      - CONFIG_PATH=${CONFIG_PATH:-/app/config/.env}
      - PYTHONPATH=/app
    depends_on:
      - subtitle-gen

  # GPU Stages - use :cuda tag with :cpu fallback
  # These services are defined twice: GPU version and CPU fallback
  
  # Silero VAD - GPU version
  silero-vad-gpu:
    image: "${DOCKERHUB_USER:-rajiup}/cp-whisperx-app-silero-vad:${GPU_TAG:-cuda}"
    container_name: cp_whisperx_silero_vad_gpu
    profiles: ["gpu"]
    volumes:
      - ./out:/app/out
      - ./config:/app/config:ro
      - ./shared:/app/shared:ro
    environment:
      - CONFIG_PATH=${CONFIG_PATH:-/app/config/.env}
      - PYTHONPATH=/app
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    depends_on:
      - pre-ner

  # Silero VAD - CPU fallback
  silero-vad:
    image: "${DOCKERHUB_USER:-rajiup}/cp-whisperx-app-silero-vad:cpu"
    container_name: cp_whisperx_silero_vad
    profiles: ["cpu", "default"]
    volumes:
      - ./out:/app/out
      - ./config:/app/config:ro
      - ./shared:/app/shared:ro
    environment:
      - CONFIG_PATH=${CONFIG_PATH:-/app/config/.env}
      - PYTHONPATH=/app
    depends_on:
      - pre-ner

  # PyAnnote VAD - GPU version
  pyannote-vad-gpu:
    image: "${DOCKERHUB_USER:-rajiup}/cp-whisperx-app-pyannote-vad:${GPU_TAG:-cuda}"
    container_name: cp_whisperx_pyannote_vad_gpu
    profiles: ["gpu"]
    volumes:
      - ./in:/app/in
      - ./out:/app/out
      - ./LLM:/app/LLM
      - ./config:/app/config:ro
      - ./shared:/app/shared:ro
    environment:
      - CONFIG_PATH=${CONFIG_PATH:-/app/config/.env}
      - PYTHONPATH=/app
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    depends_on:
      - silero-vad-gpu

  # PyAnnote VAD - CPU fallback
  pyannote-vad:
    image: "${DOCKERHUB_USER:-rajiup}/cp-whisperx-app-pyannote-vad:cpu"
    container_name: cp_whisperx_pyannote_vad
    profiles: ["cpu", "default"]
    volumes:
      - ./in:/app/in
      - ./out:/app/out
      - ./LLM:/app/LLM
      - ./config:/app/config:ro
      - ./shared:/app/shared:ro
    environment:
      - CONFIG_PATH=${CONFIG_PATH:-/app/config/.env}
      - PYTHONPATH=/app
    depends_on:
      - silero-vad

  # Diarization - GPU version
  diarization-gpu:
    image: "${DOCKERHUB_USER:-rajiup}/cp-whisperx-app-diarization:${GPU_TAG:-cuda}"
    container_name: cp_whisperx_diarization_gpu
    profiles: ["gpu"]
    volumes:
      - ./in:/app/in
      - ./out:/app/out
      - ./LLM:/app/LLM
      - ./config:/app/config:ro
      - ./scripts:/app/scripts:ro
      - ./shared:/app/shared:ro
    environment:
      - CONFIG_PATH=${CONFIG_PATH:-/app/config/.env}
      - PYTHONPATH=/app
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    depends_on:
      - pyannote-vad-gpu

  # Diarization - CPU fallback
  diarization:
    image: "${DOCKERHUB_USER:-rajiup}/cp-whisperx-app-diarization:cpu"
    container_name: cp_whisperx_diarization
    profiles: ["cpu", "default"]
    volumes:
      - ./in:/app/in
      - ./out:/app/out
      - ./LLM:/app/LLM
      - ./config:/app/config:ro
      - ./scripts:/app/scripts:ro
      - ./shared:/app/shared:ro
    environment:
      - CONFIG_PATH=${CONFIG_PATH:-/app/config/.env}
      - PYTHONPATH=/app
    depends_on:
      - pyannote-vad

  # ASR - GPU version
  asr-gpu:
    image: "${DOCKERHUB_USER:-rajiup}/cp-whisperx-app-asr:${GPU_TAG:-cuda}"
    container_name: cp_whisperx_asr_gpu
    profiles: ["gpu"]
    volumes:
      - ./in:/app/in
      - ./out:/app/out
      - ./LLM:/app/LLM
      - ./config:/app/config:ro
      - ./scripts:/app/scripts:ro
      - ./shared:/app/shared:ro
    environment:
      - CONFIG_PATH=${CONFIG_PATH:-/app/config/.env}
      - PYTHONPATH=/app
    mem_limit: 16g
    memswap_limit: 16g
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    depends_on:
      - diarization-gpu

  # ASR - CPU fallback
  asr:
    image: "${DOCKERHUB_USER:-rajiup}/cp-whisperx-app-asr:cpu"
    container_name: cp_whisperx_asr
    profiles: ["cpu", "default"]
    volumes:
      - ./in:/app/in
      - ./out:/app/out
      - ./LLM:/app/LLM
      - ./config:/app/config:ro
      - ./scripts:/app/scripts:ro
      - ./shared:/app/shared:ro
    environment:
      - CONFIG_PATH=${CONFIG_PATH:-/app/config/.env}
      - PYTHONPATH=/app
    mem_limit: 16g
    memswap_limit: 16g
    depends_on:
      - diarization

# Usage examples:
#
# 1) Run with GPU (tries CUDA images first):
#    COMPOSE_PROFILES=gpu docker compose up
#
# 2) Force CPU execution:
#    COMPOSE_PROFILES=cpu docker compose up
#
# 3) Run specific GPU stage:
#    docker compose --profile gpu run --rm asr-gpu out/Movie_Name
#
# 4) Run with CPU fallback (default):
#    docker compose run --rm asr out/Movie_Name
#
# Image Tagging Strategy:
# - CPU-Only Stages: :cpu tag (demux, tmdb, pre-ner, post-ner, subtitle-gen, mux)
# - GPU Stages: :cuda tag (silero-vad, pyannote-vad, diarization, asr)
# - GPU Fallback: :cpu tag (same stages, CPU execution)
